---
output:
  pdf_document: default
  html_document: default
---
# Power Analysis 

```{problem_description}
(Because there are a _lot_ of ways to write this code, we are not going to write a tight testing suite against this question.) 

Understanding whether your experiment design and data collection strategy are able to reject the null hypothesis *when they should* is valuable! And, this is not a theoretical value. If your design and data collection cannot reject the null hypothesis, why even run the experiment in the first place?

The classical formulation of power asks, "Given a test procedure and data, what proportion of the tests I *could conduct* would reject the null hypothesis?" 

Imagine that you and David Reiley are going to revive the sports card experiment from the previous question. However, because it is for a class project, and because you have already spent all your money on a shiny new data science degree :raised_hands: :money_with_wings: , you are not going to be able to afford to recruit as many participants as before. 
```

## Describe your testing procedure
Describe a t-test based testing procedure that you might conduct for this experiment. What is your null hypothesis, and what would it take for you to reject this null hypothesis? (This second statement could either be in terms of p-values, or critical values.)

**Answer:** For this experiment and under a t-test based testing procedure, I would posit the following as our null hypothesis, sticking with the classical convention of using a "two-tailed" test (specifying no preconceived direction of effect): *there is no difference in mean bids between the bidders who receive the treatment auction format and the bidders who receive the control auction format*. We would then need to (A) run the experiment and collect data on card bids between participants in each group (control and treatment), (B) subtract the average (mean) of all bids from participants in the treatment group from the average (mean) of all bids from participants in the control group to get a "difference in means" estimate for the ATE, and then (C) run a t-test to determine if this difference in means is likely (probable) to occur under the assumption of the null hypothesis. Also sticking with convention, I would recommend using an alpha value (or significance level) of 0.05 for the t-test. The t-test would convert our observed difference in means to a t-statistic based upon the assumptions underlying a t-distribution, and then it would generate a p-value that signifies the probability of obtaining a value *as extreme or more extreme* as that t-statistic. If the p-value generated by the test turned out to be <= 0.05 (our chosen significance level), we would conclude two things: (1) the difference in mean bids we observed in our data falls within *either the lower 2.5% quantile or upper 2.5% quantile* of the t-distribution, which means it's an outcome that's improbable to observe based on chance alone under the assumption of the null hypothesis, and (2) we should reject the null hypothesis and instead posit that there *is* a statistically significant difference in mean bids between the bidders who receive the treatment auction format and the bidders who receive the control auction format (this constitutes an "alternative" hypothesis).

## Suppose you only had ten subjects, what would you learn
Suppose that you are only able to recruit 10 people to be a part of your experiment -- 5 in treatment and another 5 in control. Simulate "re-conducting" the sports card experiment once by sampling from the data you previously collected, and conducting the test that you've written down in part 1 above. *Given the results of this 10 person simulation, would your test reject the null hypothesis?*

```{r load cards data ... again}
d <- fread('../data/list_data_2019.csv')
```

```{r ten person sample, include=TRUE}
# this should be a test object

# Create random sample + t-test function for modular and reusable code
rs_t_test <- function(sample_size) {
  
  # Randomly sample `sample_size/2` control group bids (balanced groups)
  c_sample_dt <- d[uniform_price_auction == 0,
                   .SD[sample(.N, sample_size / 2, replace = TRUE)], ]
  
  # Randomly sample `sample_size/2` treatment group bids (balanced groups)
  t_sample_dt <- d[uniform_price_auction == 1,
                   .SD[sample(.N, sample_size / 2, replace = TRUE)], ]
  
  # Combine sampled bids into a single data.table
  sampled_dt <- rbindlist(list(c_sample_dt, t_sample_dt))
  
  # Run t-test on sampled bids to evaluate control/treatment difference in means
  t_test_sampled <- t.test(bid ~ uniform_price_auction, data = sampled_dt)
  
  # Return t-test outcome
  return(t_test_sampled)
  
}

# Run random sample + t-test function for 10 participants
t_test_ten_people <- rs_t_test(10)

# Display result
t_test_ten_people
```

**Answer:** Based on the results of this 10 person simulation *at the time I ran the code*, our t-test would *not* reject the null hypothesis. This test indicates that there is no statistically significant difference between the average bid of the 5 bidders who received the treatment auction format ($M_t$ = `r t_test_ten_people$estimate[2]`) and the average bid of the 5 bidders who received the control auction format ($M_c$ = `r t_test_ten_people$estimate[1]`), as characterized by its test statistic value and a p-value that is greater than our chosen significance level of 0.05 (*`t`*(`r t_test_ten_people$parameter`) = `r t_test_ten_people$statistic`, *`p`* = `r t_test_ten_people$p.value`).

## With only ten subjects, what is your power? 
Repeat this process -- sampling 10 people from your existing data and conducting the appropriate test -- one-thousand times. Each time that you conduct this sample and test, pull the p-value from your t-test and store it in an object for later use. *Consider whether your sampling process should sample with or without replacement.*

```{r many ten person samples, include=TRUE}
# fill this in with the p-values from your power analysis

# Instantiate empty numeric vector of length 1000 to store p-values
t_test_p_values <- numeric(1000)

## you can either write a for loop, use an apply method, or use replicate 
## (which is an easy-of-use wrapper to an apply method)

# Run random sample + t-test function 1000 times, storing p-values along the way
for (i in 1:1000) {
  t_test_trial_result <- rs_t_test(10)
  t_test_p_values[i]  <- t_test_trial_result$p.value
}
```

**Answer:** The vector of p-values from this simulation is notated as `t_test_p_values`. Interestingly, the number of p-values below our chosen significance level of 0.05 is `r sum(t_test_p_values <= 0.05)`, which means only `r (sum(t_test_p_values <= 0.05)/1000)*100`% of all simulated t-tests generate a statistically significant result.

## Visual analysis
Use `ggplot` and either `geom_hist()` or `geom_density()` to produce a distribution of your p-values, and describe what you see. *What impression does this leave you with about the power of your test?* 

```{r histogram of ten person samples, include=TRUE}
# Create density plot to show distribution of p-values from simulated t-tests
ggplot(
  data = NULL,
  aes(
    x = t_test_p_values)) +
  geom_density(
    fill = "lightblue",
    color = "white",
    alpha = 0.7) +
  xlim(
    min_value = 0,
    max_value = 1) +
  geom_vline(
    xintercept = 0.05,
    color = "red",
    linetype = "dashed") +
  labs(
    title = "Most simulated p-values fall to the right of significance level (0.05)",
    subtitle = "Based on 1,000 experiment iterations with a 10 person sample",
    x = "t-test p-value",
    y = "Density"
)
```

**Answer:** Based on the definition of power provided earlier (i.e., "Given a test procedure and data, what proportion of the tests I *could conduct* would reject the null hypothesis?") and based on the distribution of p-values in the chart above, my impression of the power of this 10 person test is that it's quite *low*. In fact, we can calculate it as `r (sum(t_test_p_values <= 0.05) / 1000)`. Only a small proportion of tests within our 1000 test simulation result in statistically significant p-values of <= 0.05 that would reject the null hypothesis. Therefore, this test doesn't seem to have a lot of power.

## Interpret your results, given your power
Suppose that you and David were to actually run this experiment and design -- sample 10 people, conduct a t-test, and draw a conclusion. **And** suppose that when you get the data back, **lo and behold** it happens to reject the null hypothesis. Given the power that your design possesses, does the result seem reliable? Or, does it seem like it might be a false-positive result?

```{r, include = TRUE}
# Answer provided in narrative form below
```

**Answer:** If this scenario *were* to happen, color me surprised! Given the power that our design possesses with just a 10 person sample, this result wouldn't seem reliable to me. Our test's power indicates that we could expect this outcome to arise a mere `r (sum(t_test_p_values <= 0.05) / 1000) * 100`% of the time, which doesn't feel like great odds. If we were to suspend reality for a moment and say that, in this scenario, I hadn't already worked through question 2 on this problem set and calculated an estimate for the ATE from the FULL cards data and established its statistical significance using a t-test/randomization inference test/regression model (!!!), then I *would* consider this outcome to represent a false-positive result. 

## Conduct a power analysis 
Apply the decision rule that you wrote down in part 1 above to each of the simulations you have conducted. What proportion of your simulations have rejected your null hypothesis? This is the power that this design and testing procedure generates. After you write and execute your code, include a narrative sentence or two about what you see.  

```{r ten-person power, include=TRUE}
# Compute power of 1000 simulation test based on number of p-values <= 0.05
t_test_rejects <- sum(t_test_p_values <= 0.05) / 1000

# Display power
t_test_rejects
```

**Answer:** As stated above, the power of this design is `r t_test_rejects`. This means that only `r t_test_rejects * 100`% of our simulations would reject the null hypothesis. This level of power is quite small and makes it difficult to actually justify this design (i.e., using only a 10 person sample) because it leaves a lot of room for error. More specifically, it makes it likely that we'll commit a Type II error and fail to reject the null hypothesis when we actually *should* reject it.

## Moar power! 
Does buying more sample increase the power of your test? Apply the algorithm you have just written onto different sizes of data. Namely, conduct the exact same process that you have for 10 people, but now conduct the process for every 10% of recruitment size of the original data: Conduct a power analysis with a 10%, 20%, 30%, ... 200% sample of the original data. (You could be more granular if you like, perhaps running this task for every 1% of the data). 

```{r, include=TRUE}
# Create vector of uneven sample sizes ranging from 10% - 200% of original data
sampling_trials_uneven <- floor(seq(0.10, 2, by = 0.10) * nrow(d))

# Create function to round value down to nearest whole number and make even
floor_to_even <- function(n) {
  
  # Store `result` value as rounded down number that's divisible by 2
  result <- floor(n)
  if (result %% 2 != 0) {
    result <- result - 1
  }
  
  # Return rounded even value
  return(result)
}

# Instantiate new vector to contain rounded even sampling trials (sizes)
sampling_trials_even <- numeric(0)

# Transform uneven sampling trials to rounded even values and append to vector
for (i in sampling_trials_uneven) {
  sampling_trials_even <- c(sampling_trials_even, floor_to_even(i))
}

# Create power evaluator function to assess sample size impacts on power
percentages_to_sample <- function(samples) {
  
  # Instantiate new vector to hold power estimates/proportion of t-test rejects
  t_test_rejects <- numeric(0)
  
  # Loop through sampling trials (e.g., 10%, 20%, ... , 200% of original data)
  for (j in sampling_trials_even) {
    
    # Instantiate vector of length 1000 to hold p-values
    t_test_p_values <- numeric(1000)
      
    # Run random sample + t-test function 1000 times, storing each p-value
    for (k in 1:1000) {
      t_test_trial_result <- rs_t_test(j)
      t_test_p_values[k]  <- t_test_trial_result$p.value
      }
      
    # Compute power of 1000 simulation test based on number of p-values <= 0.05
    t_test_rejects <- c(t_test_rejects, sum(t_test_p_values <= 0.05) / 1000)
    
  }
    
  # Return vector of power estimates (proportion of t-test rejects)
  return(t_test_rejects)
  
}

# Run power evaluator function for a vector of 20 different sampling schemes,
# ranging from 10% to 200% of original data, with step size 10%
power_estimates <- percentages_to_sample(sampling_trials_even)

# Display estimates of power across all sampling trials
power_estimates
```

```{r adding chart to visualize power/sample size relationship, include=TRUE}
# Create line plot to show relationship between power and sample size
ggplot(
  data = NULL,
  aes(
    x = sampling_trials_even,
    y = power_estimates)
  ) +
  geom_line(
    color = "black",
  ) +
  xlim(
    min_value = 0,
    max_value = 150) +
  labs(
    title = "Estimated power increases non-linearly as a function of sample size",
    subtitle = "Based on 1,000 experiment iterations with various sample sizes",
    x = "Sample Size",
    y = "Power Estimate"
)
```

**Answer:** Yes, buying more sample *does* increase the power of our test --- though not linearly, as both the vector of power estimates (`power_estimates`) and line chart showing power vs. sample size indicate. The relationship we're seeing between power and sample size across the 20 different simulations of the experiment mirrors what Green & Gerber outline in a formal statistical formula for power on p. 93 of *Field Experiments*: our power estimates generally increase at a rate consistent with the square-root of the sample size. This means that increasing sample size is indeed one way to boost power, but it has diminishing returns (e.g., moving from a sample size of 10 to 20 provides much more impact to power than moving from a sample size of 50 to 100). Therefore, it should be considered and evaluated alongside other methods for increasing power, like reducing variance or increasing effect size/treatment dosage.
