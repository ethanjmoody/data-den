# Sports Cards

```{problem_description}
In this experiment, the experimenters invited consumers at a sports card trading show to bid against one other bidder for a pair trading cards.  We abstract from the multi-unit-auction details here, and simply state that the treatment auction format was theoretically predicted to produce lower bids than the control auction format.  We provide you a relevant subset of data from the experiment.

In this question, we are asking you to produce p-values and confidence intervals in three different ways: 

1) Using a `t.test`; 
2) Using a regression; and,
3) Using randomization inference. 
```

```{r load cards data }
d <- fread('../data/list_data_2019.csv')
```

## t-test and confidence interval
Using a `t.test`, compute a 95% confidence interval for the difference between the treatment mean and the control mean. After you conduct your test, write a narrative statement, *using inline code evaluation* that describes what your tests find, and how you interpret these results. (You should be able to look into `str(t_test_cards)` to find the pieces that you want to pull to include in your written results.) 

```{r cards t-test, include=TRUE}
# this should be the t.test object. Extract pieces from this object 
# in-text below the code chunk.

# Run t-test to evaluate difference in means between treatment and control
t_test_cards <- t.test(bid ~ uniform_price_auction, data = d)

# Display components of t-test result
# str(t_test_cards)

# Extract p-value from t-test result
# t_test_cards$p.value

# Extract confidence interval from t-test result
t_test_ci <- t_test_cards$conf.int[1:2]

# Display confidence interval
print(t_test_ci)
```

**Narrative Analysis:** The `r d[, sum(uniform_price_auction == 1)]` bidders who received the treatment auction format had a *significantly lower* average bid ($M_t$ = `r t_test_cards$estimate[2]`) than the `r d[, sum(uniform_price_auction == 0)]` bidders who received the control treatment auction format ($M_c$ = `r t_test_cards$estimate[1]`), based on the results of the t-test (*`t`*(`r t_test_cards$parameter`) = `r t_test_cards$statistic`, *`p`* = `r t_test_cards$p.value`). The 95% confidence interval for the difference between the control mean and treatment mean is [`r t_test_cards$conf.int[1]`, `r t_test_cards$conf.int[2]`]. Note that this confidence interval spans strictly *positive* numbers, because the t-test is subtracting the treatment mean (a lower value) from the control mean (a higher value), rather than the other way around.

## Interpretation of confidence interval
In your own words, what does this confidence interval mean? This can be simple language, but it has to be statistically appropriate language. 

**Answer:** The 95% confidence interval for the difference between the control mean and treatment mean is [`r t_test_cards$conf.int[1]`, `r t_test_cards$conf.int[2]`], which indicates that this interval has a 95% chance of including the actual estimated average treatment effect (ATE). Put another way, if we were to conduct numerous iterations (perhaps thousands!) of this experiment, after which we ran a t-test and computed a confidence interval for the difference in means (just like we did above), we would expect 95% of the confidence intervals we computed to contain this actual estimated ATE. Note that the ATE converted to a t-statistic within our `t_test_cards` object is actually being represented as the control mean minus the treatment mean --- that is, the t-test is comparing the mean bid from the lowest-labeled `uniform_price_auction` group (the control group, with label 0) to the mean bid from the highest-labeled `uniform_price_auction` group (the treatment group, with label 1). This is the reverse of how we would typically think of the ATE --- treatment group mean minus control group mean --- but it yields the same value in *absolute terms* either way.

## Randomization inference, and confidence interval?
Conduct a randomization inference process, with `n_ri_loops = 1000`, using an estimator that you write by hand (i.e. in the same way as earlier questions). On the sharp-null distribution that this process creates, compute the 2.5% quantile and the 97.5% quantile using the function `quantile` with the appropriate vector passed to the `probs` argument. This is the randomization-based uncertainty that is generated by your design. After you conduct your test, write a narrative statement of your test results. 

```{r cards randomization inference, include=TRUE} 
## first, do you work for the randomization inference

# Set number of iterations for randomization inference
n_ri_loops <- 1000

# Create randomization inference function for modular and reusable code
ri <- function(permutations = n_ri_loops) {
  
  # Create vector to store test-statistic (ATE) under each permutation
  cards_ate_vector <- NA
  
  # Simulate `n_ri_loops` random permutations of treatment assignment
  for(perm in 1:permutations) {
    
    # Add computed test-statistics to vector after treatment randomization
    cards_ate_vector[perm] <- d[, .(mean_bids = mean(bid)), 
                  keyby = .(sample(uniform_price_auction))][, diff(mean_bids)]

  }
  
  # Return vector of test-statistics
  return(cards_ate_vector)

}

# Compute mean `bids` across `uniform_price_auction` groups
bids_group_mean <- d[, .(mean_bids = mean(bid)), keyby = uniform_price_auction] 

# Compute ATE
cards_ate <- bids_group_mean[uniform_price_auction == 1, mean_bids] -
             bids_group_mean[uniform_price_auction == 0, mean_bids]

# Run randomization inference function with `n_ri_loops` permutations 
cards_ri_distribution <- ri(n_ri_loops) # numeric vector of length equal 
                                        # to your number of RI permutations

# Compute the 2.5% and 97.5% quantiles
cards_ri_quantiles <- quantile(cards_ri_distribution, probs = c(0.025, 0.975))
# there's a built-in to pull these.

# Compute the p-value associated with obtaining a test-statistic at least as
# extreme as the 2.5% and 97.5% quantile values
cards_ri_p_value <- as.numeric(
  (sum(cards_ri_distribution <= cards_ate) +
   sum(cards_ri_distribution >= (-1 * cards_ate))) / 
   length(cards_ri_distribution)
  )
```

**Narrative:** With `r n_ri_loops` permutations, our randomization inference test reveals the following: (1) like the t-test, we see that the estimated `cards_ate` (`r cards_ate`) --- that is, the difference in mean bids between bidders in the treatment and control groups --- is *statistically significant* with a p-value of `r cards_ri_p_value`, (2) the lowest 2.5% of ATE outcomes from the distribution of randomized/permuted ATEs (under the assumption of the sharp null hypothesis) is defined as all outcomes at or below `r cards_ri_quantiles[[1]]`, and (3) the highest 2.5% of ATE outcomes from this same distribution is defined as all outcomes at or above `r cards_ri_quantiles[[2]]`. The first of these findings (#1) suggests that there is a very slim probability of seeing the actual estimated ATE based on chance alone (i.e., this ATE would fall within the left-tail of the randomization distribution assuming the sharp null hypothesis). The second and third of these findings (#2/#3) confirms that we're seeing a statistically meaningful ATE in the original data, since the value of the estimated ATE (`r cards_ate`) falls *below* the 2.5% quantile value (`r cards_ri_quantiles[[1]]`) of the randomization distribution. It also indicates that the 95% confidence interval generated by the randomization inference test is [`r cards_ri_quantiles[[1]]`, `r cards_ri_quantiles[[2]]`], which means that 95% of the time, we could expect the estimates generated from the sharp null distribution to fall within this range. Additionally, it's interesting to note that the size/width of the 95% confidence interval for our randomization inference test (i.e., the arithmetic difference between the interval lower bound at the 2.5% quantile and upper bound at the 97.5% quantile) is virtually the same as the size/width of the confidence interval for our t-test. For our randomization inference test, this width is equal to `r cards_ri_quantiles[[2]] - cards_ri_quantiles[[1]]`; for our t-test, this width is equal to `r t_test_ci[[2]] - t_test_ci[[1]]`. The *locations* of these intervals are different, however, because our randomization inference interval is based on the assumption of the *sharp null hypothesis* and is therefore centered (virtually perfectly) around 0. We could transform our randomization inference confidence interval to look like our t-test confidence interval by just subtracting the `cards_ate` from the interval lower bound and upper bound (*subtracting*, in this case, again because the t-test confidence interval was generated based on comparing the control group mean bid to the treatment group mean bid instead of the other way around). This finding confirms that, when interpreting confidence intervals, "the *location of the interval varies from one experiment to the next*, while the true ATE remains constant", as Green & Gerber observe in *Field Experiments* (p. 67).  

## Compare regression and randomization inference
Do you learn anything different if you regress the outcome on a binary treatment variable? To answer this question, regress `bid` on a binary variable equal to 0 for the control auction and 1 for the treatment auction and then calculate the 95% confidence interval using *classical standard errors* (in a moment you will calculate with *robust standard errors*). There are two ways to do this -- you can code them by hand; or use a built-in, `confint`. After you conduct your test, write a narrative statement of your test results. 

```{r cards ols regression, include=TRUE}
# this should be a model object, class = 'lm'.

# Create linear model for `bid` regressed on group assignment variable
mod <- lm(bid ~ uniform_price_auction, data = d)

# Compute 95% confidence interval for the model with classical SEs
mod_ci <- confint(mod, level = 0.95)

# Display confidence interval
print(mod_ci)
```

**Narrative:** Our regression model yields very similar results to our t-test and slightly different results than our randomization inference test. The 95% confidence interval generated by our regression model is [`r mod_ci[[2]]`, `r mod_ci[[4]]`], which is virtually the same interval as what we saw for our t-test, but with negative numbers instead of positive numbers due to the order in which the t-test compares group means (i.e., control to treatment). The resulting regression-generated interval has a 95% chance of bracketing the actual estimated ATE, which corroborates our earlier t-test. Meanwhile, this regression-generated confidence interval looks different than the randomization-generated confidence interval (even though both have the same size/width) because it's not based on the sharp null hypothesis. We could transform it to look more like the randomization inference-generated interval by adding the `cards_ate` value (`r cards_ate`) to the lower bound and upper bound of the interval, which would center it around 0.

## Regression with robust confidence interval
Calculate the 95% confidence interval using robust standard errors, using the `sandwich` package. There is a function in `lmtest` called `coefci` that can help with this. It is also possible to do this work by hand. After you conduct your test, write a narrative statement of your test results.

```{r cards robust ci, include=TRUE}
# this should be a numeric vector of length 2

# Generate 95% confidence interval using robust SEs
cards_robust_ci <- c(
  coefci(mod, vcov = vcovHC(mod))[2],
  coefci(mod, vcov = vcovHC(mod))[4]
)

# Display class for `cards_robust_ci` to check for numeric result
# class(cards_robust_ci)

# Display confidence interval with robust SEs
cards_robust_ci
```

**Narrative:** Since robust standard errors (RSEs) are "robust" against outliers and against the assumption of homoscedasticity in the data (i.e., constant/consistent variance in errors across all values of the independent variable), I would generally expect to see *wider*/more conservative 95% confidence intervals with RSEs applied than with OLS/classical SEs applied. Interestingly, applying RSEs to our regression model yields only a slightly wider 95% confidence interval of [`r cards_robust_ci[1]`, `r cards_robust_ci[2]`]. This could mean that the underlying data may meet (or nearly meet) the assumption of homoscedasticity, or that there are few outliers influencing the original confidence interval. Either way, the slightly wider interval here makes sense with RSEs applied. 

## Compare and contrast results
Characterize what you learn from each of these different methods -- are the results contingent on the method of analysis that you choose? 

**Answer:** As expected, all three statistical tests/methods lead to a similar conclusion about the difference in mean bids across control and treatment groups: there is a slim probability this difference in mean bids (the estimated ATE) across auction formats could arise simply due to chance, making it a statistically significant outcome. This main result/conclusion isn't contingent on the method of analysis chosen, even though p-values may differ slightly between tests/methods. However, our choice of method *can* influence the "location" (i.e., lower bound and upper bound points) of a test-generated 95% confidence interval. For example, the confidence interval generated by our randomization inference test is centered around 0 since we're making an assessment of the likelihood of seeing the estimated ATE against the assumption of the sharp null hypothesis; meanwhile, the confidence intervals generated by our t-test and regression model are not centered the same way since we're not invoking the sharp null hypothesis. This means that the language we use to describe the conclusions drawn from these confidence intervals may vary depending on the method of analysis chosen. Yet, we also know that we can transform each of these intervals or re-center them around a consistent value by adding or subtracting the ATE from the interval lower/upper bounds. This indicates that all three tests tell a similar story about the data and the estimated ATE, but do so from slightly different angles.
