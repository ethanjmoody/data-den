{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czxW4C_gf5Eo"
      },
      "source": [
        "# From RAGs to Riches: How Search and Q&A Capabilities with RAG can Accelerate Engineering and Marketing Workflows\n",
        "\n",
        "This is a POC of a RAG system for a hypothetical tech company. The company is looking for new ways to organize their question answering and search capabilities to accelerate both product engineering activity (for an engineering team of ~300 people) and marketing initiatives (for a marketing/support staff department of ~40 people)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Notebook Setup\n",
        "\n",
        "Install and import needed libraries."
      ],
      "metadata": {
        "id": "BC0h4c1b7Vfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIlhfQj-KUlZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install git+https://github.com/huggingface/transformers\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate\n",
        "!pip -q install langchain\n",
        "!pip install einops\n",
        "!pip install faiss-gpu\n",
        "!pip install --upgrade --quiet  langchain-community chromadb bs4 qdrant-client\n",
        "!pip install langchainhub\n",
        "\n",
        "!pip install --upgrade --quiet  wikipedia\n",
        "!pip install --upgrade --quiet  arxiv\n",
        "!pip install --upgrade --quiet  pymupdf\n",
        "\n",
        "!pip install xmltodict\n",
        "\n",
        "!pip install cohere\n",
        "\n",
        "!pip install -U langchain-cohere\n",
        "!pip install evaluate\n",
        "!pip install rouge-score\n",
        "!pip install bert_score\n",
        "!pip install ragas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NjcvYABKieZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import bs4\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import locale\n",
        "\n",
        "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.utils.math import cosine_similarity\n",
        "\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.document_loaders import OnlinePDFLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.document_loaders import PubMedLoader\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import evaluate\n",
        "import spacy\n",
        "from bert_score import BERTScorer\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate as ragas_evaluate\n",
        "from ragas.metrics import (\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        ")\n",
        "import plotly.graph_objects as go\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faZ5fLk_xxAO"
      },
      "outputs": [],
      "source": [
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stlb_ciPxxWA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "TKuUZpD6vqzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', 50)"
      ],
      "metadata": {
        "id": "q_itXcXESSm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll9IqkVMa7qP"
      },
      "outputs": [],
      "source": [
        "COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlSHHPW-f3ZL"
      },
      "source": [
        "##2. Building and Testing the Components of a RAG System\n",
        "\n",
        "Introduce and test the base components of a RAG system using Hugging Face and LangChain libraries. This section starts laying the groundwork for the POC model/system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N3fqR5vKV9b"
      },
      "source": [
        "###2.1 Embedding Model\n",
        "\n",
        "Represent text as vectors using the sentence_transformer architecture.\n",
        "\n",
        "Embedding models tested (from Hugging Face):\n",
        "- 'all-MiniLM-L6-v2'\n",
        "- 'multi-qa-mpnet-base-dot-v1'\n",
        "- 'avsolatorio/GIST-Embedding-v0'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_AqjidjKWif"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgzrqje8PN8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2a9d04-6943-4ebf-97e5-3ab6482256a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension: 768\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text = \"This is a test document.\"\n",
        "query_result = base_embeddings.embed_query(text)\n",
        "print(f'Embedding dimension: {len(query_result)}')\n",
        "\n",
        "doc_result = base_embeddings.embed_documents([text, \"This is not a test document.\"])\n",
        "len(doc_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAp4fF5dPhbM"
      },
      "source": [
        "###2.2. Loading and Chunking Texts\n",
        "\n",
        "Load the documents for the RAG system, starting with just a single example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ak5XlaUP1cW"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZXtDqKcQHlT"
      },
      "source": [
        "Split the  text into chunks (retrieval units).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhcWoTajQnw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7bda4d-ca1a-40f7-eefd-8c4af18b6f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits/chunks:  444\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print('Number of splits/chunks: ', str(len(splits)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du9avBkhP1ll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "651fcd5f-2db9-4461-ba04-96c83894124d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "splits[39].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-RuP_9xRVtG"
      },
      "source": [
        "###2.3 Storing the Embeddings of Chunks in a Vectorstore\n",
        "\n",
        "Save vector representations of the chunks in a vectorstore using Qdrant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T3FVDoJRglN"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(splits,\n",
        "    base_embeddings,\n",
        "    location=\":memory:\",  # Local mode with in-memory storage only\n",
        "    collection_name=\"test\",\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiSqastIP1oT"
      },
      "outputs": [],
      "source": [
        "query = \"What is Chain of Thought doing?\"\n",
        "docs = vectorstore.similarity_search_by_vector(base_embeddings.embed_query(query)) # Ranks the splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7CO0lvSR0MA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3771deec-3032-40ec-bdaa-370e87153c0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '9a97ca9dd4164b52a8f6da12e433ff26', '_collection_name': 'test'}),\n",
              " Document(page_content='[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'd5f0880f43954f1bb117e56ad81fc31c', '_collection_name': 'test'}),\n",
              " Document(page_content='the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '64a22c24de54404a881f2158b8b3b653', '_collection_name': 'test'}),\n",
              " Document(page_content='Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'acd899ae5b954236ab6c97dfcd79127c', '_collection_name': 'test'})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDDom9EbKXCX"
      },
      "source": [
        "###2.4. Large Language Model (LLM)\n",
        "\n",
        "Incorporate an LLM to generate responses to prompts/questions.\n",
        "\n",
        "LLMs tested (from Hugging Face / Cohere):\n",
        "- 'mistralai/Mistral-7B-Instruct-v0.1' (Open Source)\n",
        "- 'ChatCohere' (Proprietary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YooxnCPNOoQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9813d01-473c-4969-d39b-a45f51145c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['llm_int4_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         llm_int4_enable_fp32_cpu_offload=True)\n",
        "\n",
        "\n",
        "llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map='auto',\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvxo5OKwvjNN"
      },
      "outputs": [],
      "source": [
        "mistral_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_mistral_model,\n",
        "    tokenizer=llm_mistral_tokenizer,\n",
        "    max_length=1000,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAUHEvq_TRSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e9a1cc-5582-4394-a1b0-214c56615c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': '[INST]Give me a two-sentence story about an apple![/INST] Once upon a time, there was a red apple that fell from the tree. It rolled down the hill and into the hands of a little girl who delighted in biting into its juicy flesh.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "mistral_pipe(\"[INST]Give me a two-sentence story about an apple![/INST]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh-60AEBTODZ"
      },
      "source": [
        "###2.5 Testing the LLM in a LangChain Chain\n",
        "\n",
        "Create a LangChain chain consisting of a prompt template, LLM, and String Formatter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1LSyyFmTOYP"
      },
      "outputs": [],
      "source": [
        "mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh3R9445K5ct"
      },
      "outputs": [],
      "source": [
        "test_llm_template = \"\"\"[INST] Give me a two-sentence story about an {object}! [/INST]\"\"\"\n",
        "test_llm_prompt_template = PromptTemplate(template=test_llm_template, input_variables=[\"object\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSvSoD7hZb4Z"
      },
      "outputs": [],
      "source": [
        "test_llm_chain_short = (\n",
        "    {\"object\": RunnablePassthrough()}\n",
        "    | test_llm_prompt_template\n",
        "    | mistral_llm_lc\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH8lUWWSZ-9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4ed4824b-6aa3-4248-a3b8-2cd2ef355958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[INST] Give me a two-sentence story about an apple! [/INST] Once upon a time, there was a bright red apple that grew on a lush tree in the heart of an enchanted orchard. One day, a curious little girl stumbled upon the tree and plucked the apple from its branch, not realizing the magic it held within.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "test_llm_chain_short.invoke('apple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JROGul-gZcAU"
      },
      "outputs": [],
      "source": [
        "cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ5bzWTVaZdO"
      },
      "outputs": [],
      "source": [
        "test_cohere_llm_chain_short = (\n",
        "    {\"object\": RunnablePassthrough()}\n",
        "    | test_llm_prompt_template\n",
        "    | cohere_chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZldFGowOciCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cab62a9-f0b7-45c3-d8f4-41afeee0fe40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"The apple, once bitter and unappealing, fell from the tree and transformed into a sweet, juicy treat. Its once sour notes now a distant memory, it became a symbol of nature's surprising delights.\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd702709c-2da1-40d1-aebc-eb7100581ff1'}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd702709c-2da1-40d1-aebc-eb7100581ff1'}, id='run-d2cb3567-f0c3-4ea8-acb8-6388dc141941-0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "test_cohere_llm_chain_short.invoke('apple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rSDhR5AeR7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7d1dd6b6-9c31-454c-b6dd-daa312a712d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The apple, once bitter and unappealing, fell from the tree and transformed into a sweet, juicy treat. Its once sour notes now a distant memory, it became a symbol of nature's surprising delights.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "test_cohere_llm_chain_short_formatted = (\n",
        "    {\"object\": RunnablePassthrough()}\n",
        "    | test_llm_prompt_template\n",
        "    | cohere_chat_model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "test_cohere_llm_chain_short_formatted.invoke('apple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1S5W_ueSWt"
      },
      "source": [
        "###2.6 Setting Up a Simple RAG Chain\n",
        "\n",
        "Simple test of a RAG template that takes a question and a pre-defined context as input, and generates an answer based on the provided context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrSx3gFlncAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "becad27f-a20f-44c8-e4c2-54614b93f0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: [INST] Answer the question based only on the following context:\n",
            "{'context': 'Germany has won the World Cup 4 times.', 'question': 'How many times did Germany win the world cup?'}\n",
            "\n",
            "Question: {'context': 'Germany has won the World Cup 4 times.', 'question': 'How many times did Germany win the world cup?'}\n",
            "[/INST]\n",
            "Answer: Germany won the World Cup 4 times.\n"
          ]
        }
      ],
      "source": [
        "rag_template = \"\"\"[INST] Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_template)\n",
        "\n",
        "base_rag_chain =(\n",
        "    {\"context\": RunnablePassthrough(),\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_template\n",
        "    | mistral_llm_lc\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "predefined_context = \"Germany has won the World Cup 4 times.\"\n",
        "question = \"How many times did Germany win the world cup?\"\n",
        "\n",
        "resp = base_rag_chain.invoke({'context': predefined_context,\n",
        "                           'question': question})\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maq5x1jDhJiX"
      },
      "source": [
        "Create a simple formatting function that can be used in the chain to combine a list of chunks into one string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VUMkGithJtY"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UiGHgRLhKEZ"
      },
      "outputs": [],
      "source": [
        "rag_template = \"\"\"Here is a context:\\n{context} \\n\\nand here is a question: \\n{question}\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVRdhLzwjYk6"
      },
      "outputs": [],
      "source": [
        "output = rag_chain.invoke('What is Chain of Thought?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjlKfsrljYnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae561e3-1fc3-4bb5-e190-de7bd74581d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a context:\n",
            "the model’s thinking process.\n",
            "\n",
            "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
            "\n",
            "the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process\n",
            "\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes \n",
            "\n",
            "and here is a question: \n",
            "What is Chain of Thought?\n"
          ]
        }
      ],
      "source": [
        "print(output.messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh9ZopW3jYqA"
      },
      "outputs": [],
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | mistral_llm_lc\n",
        "\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e_gx7lMjYse",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "17aaadc9-90c2-43a4-f1bd-1b6b9c05576e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: [INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nthe model’s thinking process.\\n\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n\\nthe problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes \\n\\nHere is a question: \\nWhat is Chain of Thought?.[/INST] Based on the provided context, \"Chain of Thought\" refers to a method used in natural language processing that involves breaking down a given problem or task into smaller parts and generating multiple potential solutions for each part through a series of interconnected thought processes. This approach allows for the exploration of multiple reasoning possibilities at each step, resulting in a hierarchical tree-like structure of ideas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "rag_chain.invoke('What is Chain of Thought?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHye-j_MjYvL"
      },
      "outputs": [],
      "source": [
        "cohere_rag_chain = (\n",
        "    {\"context\": retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | cohere_chat_model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnVEnMmNmrgM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3c72cc9d-e079-4ebe-ad13-14fea9c3accb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Chain of Thought (CoT) is a prompting technique used to elicit reasoning and improve the performance of large language models on complex tasks. It involves decomposing a problem into a series of thought steps, generating multiple thoughts for each step, and then using these thoughts to guide the model's search process. By structuring the model's thinking process in this way, CoT helps the model to provide more reasoned and transparent responses.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "cohere_rag_chain.invoke('What is Chain of Thought?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM7gS9kGNOJp"
      },
      "source": [
        "##3. Building and Experimenting with the POC RAG Model\n",
        "\n",
        "Acquire document data, chunk it, vectorize it, and store embeddings in vector database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 Laying the Foundation for the Vector Database\n",
        "\n",
        "Use Qdrant to store document data (vectors) in memory and create the document retriever."
      ],
      "metadata": {
        "id": "uuAzyJi77O85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXxjbq6RsKq3"
      },
      "outputs": [],
      "source": [
        "qdrant_vectorstore = Qdrant.from_documents(splits,\n",
        "    base_embeddings,\n",
        "    location=\":memory:\",  # Local mode with in-memory storage only\n",
        "    collection_name=\"rag_tech_db\",\n",
        "    force_recreate=True\n",
        ")\n",
        "\n",
        "retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC9z0o5ZsLML"
      },
      "source": [
        "###3.2 Laying the Foundation for Data Acquisition, Chunking, and Vectorization\n",
        "\n",
        "Set chunk size and overlap, as well as the type of splitter, and load document data into vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_BR4rgYrCGn"
      },
      "outputs": [],
      "source": [
        "# Initial defaults for setup/testing (to be changed later)\n",
        "CHUNK_SIZE=128\n",
        "OVERLAP=0\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-vhTZ_tvIvD"
      },
      "source": [
        "RAG documents include the following four types:\n",
        "\n",
        "* A few recent arXiv research papers on RAG and NLP\n",
        "* Several question-answering/prompt engineering blog posts by applied AI researcher Lilian Weng\n",
        "* A number of Wikipedia articles on GenAI, information retrieval, and LLMs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEmdCyqqx5kl"
      },
      "outputs": [],
      "source": [
        "# Assign a unique number to each document ingested for reference\n",
        "global_doc_number = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_xFjAUSx7ES"
      },
      "source": [
        "Load the arXiv documents and split into chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Jj2vLAyxic"
      },
      "outputs": [],
      "source": [
        "arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
        "                 '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
        "                 '2311.08377', '2312.05708', '2401.06532', '2402.01306')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq8ETxzzygTj"
      },
      "outputs": [],
      "source": [
        "all_arxiv_pages = []\n",
        "\n",
        "# Loop through the papers\n",
        "for identifier in arxiv_numbers:\n",
        "    # Construct URL using the arXiv unique identifier\n",
        "    arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
        "\n",
        "    # Extract pages from the document and add them to the list of pages\n",
        "    arx_loader = PyMuPDFLoader(arx_url)\n",
        "    arx_pages = arx_loader.load()\n",
        "    for page_num in range(len(arx_pages)):\n",
        "        page = arx_pages[page_num]\n",
        "        page.metadata['page_num'] = page_num\n",
        "        page.metadata['doc_num'] = global_doc_number\n",
        "        page.metadata['doc_source'] = \"ArXiv\"\n",
        "        all_arxiv_pages.append(page)\n",
        "\n",
        "\n",
        "    global_doc_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXNS5MaMwrOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862beec7-a075-4363-ec1e-5b171c59def2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 documents in total\n",
            "420 pages in total\n"
          ]
        }
      ],
      "source": [
        "num_pages = len(all_arxiv_pages)\n",
        "num_docs = global_doc_number - 1\n",
        "\n",
        "print(f\"{num_docs} documents in total\")\n",
        "print(f\"{num_pages} pages in total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Qr75rnvLKJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba1d6a0d-50b9-4c5c-dd45-40597e37d2bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "all_arxiv_pages[5].page_content[:150]  # All pages of the document content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ln3nmeVvLBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3c2d64-3aa9-4a6d-8a5f-4a388d9703f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits/chunks:  14652\n"
          ]
        }
      ],
      "source": [
        "# Index document chunks\n",
        "splits = text_splitter.split_documents(all_arxiv_pages)\n",
        "for idx, text in enumerate(splits):\n",
        "    splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVEpjCBLE50m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ba0465-0030-4a59-8847-72047f06ecff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,', metadata={'source': 'https://arxiv.org/pdf/2005.11401.pdf', 'file_path': 'https://arxiv.org/pdf/2005.11401.pdf', 'page': 0, 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210413004838Z', 'modDate': 'D:20210413004838Z', 'trapped': '', 'page_num': 0, 'doc_num': 1, 'doc_source': 'ArXiv', 'split_id': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "splits[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6DUxHWuozJB"
      },
      "source": [
        "Add vectors to vectorstore and retrieve a nearest neighbor to a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSJmDkj6SvQQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "qdrant_vectorstore.add_documents(documents=splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xOhcgKd5ckk"
      },
      "outputs": [],
      "source": [
        "query = \"How can we train a model for preferences?\"\n",
        "found_docs = qdrant_vectorstore.similarity_search_with_score(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDelZRCF5Ite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58539ab2-3029-423f-ac04-f7373f964de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One path forward could be to train models that can be conditioned on the preferences of certain\n",
            "0.8218969669738703\n"
          ]
        }
      ],
      "source": [
        "print(found_docs[0][0].page_content)\n",
        "print(found_docs[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipT82FOghpr6"
      },
      "source": [
        "Load the Wikipedia documents, split into chunks, and add vectors to vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfWbJmg0vKfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046e82f9-dc74-441c-8d9f-aa3123ae1d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents:  4\n",
            "Number of splits/chunks:  145\n"
          ]
        }
      ],
      "source": [
        "wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
        "for idx, text in enumerate(wiki_docs):\n",
        "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
        "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
        "\n",
        "global_doc_number += 1\n",
        "\n",
        "print('Number of documents: ', len(wiki_docs))\n",
        "\n",
        "# Index document chunks\n",
        "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
        "for idx, text in enumerate(wiki_splits):\n",
        "    wiki_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(wiki_splits))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2LSdGwIcvn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "qdrant_vectorstore.add_documents(documents=wiki_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlO0AYTmTaPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193bf581-b245-4543-8964-760bf44d24b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents:  4\n",
            "Number of splits/chunks:  156\n"
          ]
        }
      ],
      "source": [
        "wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n",
        "for idx, text in enumerate(wiki_docs):\n",
        "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
        "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
        "\n",
        "global_doc_number += 1\n",
        "\n",
        "print('Number of documents: ', len(wiki_docs))\n",
        "\n",
        "# Index document chunks\n",
        "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
        "for idx, text in enumerate(wiki_splits):\n",
        "    wiki_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(wiki_splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQqX-w8tTjah"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "qdrant_vectorstore.add_documents(documents=wiki_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_uwyvZaTkx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d983e9d-fa6e-4366-dbc5-0a47d40353c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents:  4\n",
            "Number of splits/chunks:  141\n"
          ]
        }
      ],
      "source": [
        "wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n",
        "for idx, text in enumerate(wiki_docs):\n",
        "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
        "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
        "\n",
        "global_doc_number += 1\n",
        "\n",
        "print('Number of documents: ', len(wiki_docs))\n",
        "\n",
        "# Index document chunks\n",
        "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
        "for idx, text in enumerate(wiki_splits):\n",
        "    wiki_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(wiki_splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw-FQnNZTk7I"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "qdrant_vectorstore.add_documents(documents=wiki_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfs8ro_Ziekd"
      },
      "source": [
        "Load the blog documents, split into chunks, and add vectors to vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5LdggJw8nBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca14ac10-8d98-4a8b-c277-85913bf27523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents:  3\n"
          ]
        }
      ],
      "source": [
        "web_loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
        "               \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "               \"https://lilianweng.github.io/posts/2018-06-24-attention/\"),\n",
        "\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "web_documents = web_loader.load()\n",
        "\n",
        "for idx, text in enumerate(web_documents):\n",
        "    web_documents[idx].metadata['doc_num'] = global_doc_number\n",
        "    web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
        "global_doc_number += 1\n",
        "\n",
        "print('Number of documents: ', len(web_documents))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D29aVbKn8nBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56313e4-5808-465f-cb19-afef88c04211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits:  1143\n"
          ]
        }
      ],
      "source": [
        "web_splits = text_splitter.split_documents(web_documents)\n",
        "\n",
        "for idx, text in enumerate(web_splits):\n",
        "    web_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits: ', len(web_splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5slq33qt6Dc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "qdrant_vectorstore.add_documents(documents=web_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KVUbsDa3BBF"
      },
      "source": [
        "###3.3 Defining Validation and Test Data for the RAG System\n",
        "\n",
        "Establish \"ground truth\" answers (gold answers) to a series of relevant questions users might pose to the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxMnyWig7R-T"
      },
      "outputs": [],
      "source": [
        "validation_questions_answers = {\n",
        "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
        "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
        "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"},\n",
        "1: {\"question\": \"How does a large language model learn from text during training?\",\n",
        "  \"gold_answer_research\": \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\",\n",
        "  \"gold_answer_marketing\": \"A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\"},\n",
        "2: {\"question\": \"What are some key architectures behind the development of large language models?\",\n",
        "  \"gold_answer_research\": \"Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\",\n",
        "  \"gold_answer_marketing\": \"Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\"},\n",
        "3: {\"question\": \"Can you name some specific large language models and the companies or organizations that have developed them?\",\n",
        "  \"gold_answer_research\": \"Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\",\n",
        "  \"gold_answer_marketing\": \"Chinchilla by DeepMind, GPT-3 by OpenAI.\"},\n",
        "7: {\"question\": \"What licensing models have been adopted for the distribution of source-available language models?\",\n",
        "  \"gold_answer_research\": \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n",
        "  \"gold_answer_marketing\": \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\"},\n",
        "8: {\"question\": \"What are language models and what is their purpose in natural language processing?\",\n",
        "  \"gold_answer_research\": \"Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\",\n",
        "  \"gold_answer_marketing\": \"Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\"},\n",
        "9: {\"question\": \"How have language models evolved in terms of architecture, from the 1980s to present times?\",\n",
        "  \"gold_answer_research\": \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\",\n",
        "  \"gold_answer_marketing\": \"Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\"},\n",
        "11: {\"question\": \"Can you explain how maximum entropy language models work and what the partition function signifies?\",\n",
        "  \"gold_answer_research\": \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\",\n",
        "  \"gold_answer_marketing\": \"Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\"},\n",
        "12: {\"question\": \"What is the benefit of using continuous space embeddings in recurrent neural network language models?\",\n",
        "  \"gold_answer_research\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\",\n",
        "  \"gold_answer_marketing\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\"},\n",
        "13: {\"question\": \"What challenges do large language models face in mirroring human cognitive patterns?\",\n",
        "  \"gold_answer_research\": \"Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\",\n",
        "  \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\"},\n",
        "16: {\"question\": \"What factors influenced the development of generative language models by Anthropic?\",\n",
        "  \"gold_answer_research\": \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\",\n",
        "  \"gold_answer_marketing\": \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"},\n",
        "17: {\"question\": \"What is Constitutional AI and how does it affect the functionality of AI systems?\",\n",
        "  \"gold_answer_research\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\",\n",
        "  \"gold_answer_marketing\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"},\n",
        "18: {\"question\": \"How do advances in AI models impact their ability to interact with different types of data, such as images?\",\n",
        "  \"gold_answer_research\": \"Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\",\n",
        "  \"gold_answer_marketing\": \"Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\"},\n",
        "19: {\"question\": \"What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\",\n",
        "  \"gold_answer_research\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\",\n",
        "  \"gold_answer_marketing\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\"},\n",
        "20: {\"question\": \"How has the token handling capacity changed between different versions of the Claude model?\",\n",
        "  \"gold_answer_research\": \"The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\",\n",
        "  \"gold_answer_marketing\": \"The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\"},\n",
        "22: {\"question\": \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\",\n",
        "  \"gold_answer_research\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\",\n",
        "  \"gold_answer_marketing\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"},\n",
        "23: {\"question\": \"How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\",\n",
        "  \"gold_answer_research\": \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\",\n",
        "  \"gold_answer_marketing\": \"Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\"},\n",
        "24: {\"question\": \"Who developed the language model family known as Chinchilla?\",\n",
        "  \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n",
        "  \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"},\n",
        "25: {\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n",
        "  \"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n",
        "  \"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"},\n",
        "27: {\"question\": \"What is the relationship between Chinchilla and the Gopher language model families?\",\n",
        "  \"gold_answer_research\": \"The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\",\n",
        "  \"gold_answer_marketing\": \"Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\"},\n",
        "28: {\"question\": \"What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\",\n",
        "  \"gold_answer_research\": \"The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\",\n",
        "  \"gold_answer_marketing\": \"The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\"},\n",
        "30: {\"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n",
        "  \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n",
        "  \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"},\n",
        "33: {\"question\": \"What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\",\n",
        "  \"gold_answer_research\": \"Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\",\n",
        "  \"gold_answer_marketing\": \"Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\"},\n",
        "34: {\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n",
        "  \"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n",
        "  \"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"},\n",
        "35: {\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n",
        "  \"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n",
        "  \"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"},\n",
        "36: {\"question\": \"What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\",\n",
        "  \"gold_answer_research\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\",\n",
        "  \"gold_answer_marketing\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\"},\n",
        "38: {\"question\": \"What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\",\n",
        "  \"gold_answer_research\": \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\",\n",
        "  \"gold_answer_marketing\": \"The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\"},\n",
        "39: {\"question\": \"What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\",\n",
        "  \"gold_answer_research\": \"Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\",\n",
        "  \"gold_answer_marketing\": \"Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\"},\n",
        "41: {\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n",
        "  \"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n",
        "  \"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"},\n",
        "43: {\"question\": \"What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\",\n",
        "  \"gold_answer_research\": \"The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\",\n",
        "  \"gold_answer_marketing\": \"The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\"},\n",
        "44: {\"question\": \"What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\",\n",
        "  \"gold_answer_research\": \"Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\",\n",
        "  \"gold_answer_marketing\": \"The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\"},\n",
        "45: {\"question\": \"What are some challenges in training retrieval systems and how are negative samples used to address them?\",\n",
        "  \"gold_answer_research\": \"Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\",\n",
        "  \"gold_answer_marketing\": \"Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\"},\n",
        "46: {\"question\": \"What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\",\n",
        "  \"gold_answer_research\": \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\",\n",
        "  \"gold_answer_marketing\": \"Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\"},\n",
        "47: {\"question\": \"What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\",\n",
        "  \"gold_answer_research\": \"Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\",\n",
        "  \"gold_answer_marketing\": \"Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\"},\n",
        "48: {\"question\": \"What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\",\n",
        "  \"gold_answer_research\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\",\n",
        "  \"gold_answer_marketing\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\"},\n",
        "50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
        "  \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
        "  \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"},\n",
        "51: {\"question\": \"Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\",\n",
        "  \"gold_answer_research\": \"Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\",\n",
        "  \"gold_answer_marketing\": \"The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\"},\n",
        "52: {\"question\": \"What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\",\n",
        "  \"gold_answer_research\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\",\n",
        "  \"gold_answer_marketing\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\"},\n",
        "54: {\"question\": \"What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\",\n",
        "  \"gold_answer_research\": \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\",\n",
        "  \"gold_answer_marketing\": \"Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\"},\n",
        "55: {\"question\": \"What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\",\n",
        "  \"gold_answer_research\": \"The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\",\n",
        "  \"gold_answer_marketing\": \"The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\"},\n",
        "59: {\"question\": \"Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\",\n",
        "  \"gold_answer_research\": \"To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\",\n",
        "  \"gold_answer_marketing\": \"Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\"},\n",
        "60: {\"question\": \"What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\",\n",
        "  \"gold_answer_research\": \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\",\n",
        "  \"gold_answer_marketing\": \"Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\"},\n",
        "61: {\"question\": \"What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\",\n",
        "  \"gold_answer_research\": \"One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\",\n",
        "  \"gold_answer_marketing\": \"Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\"},\n",
        "62: {\"question\": \"What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\",\n",
        "  \"gold_answer_research\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\",\n",
        "  \"gold_answer_marketing\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\"},\n",
        "63: {\"question\": \"What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\",\n",
        "  \"gold_answer_research\": \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\",\n",
        "  \"gold_answer_marketing\": \"The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\"},\n",
        "64: {\"question\": \"What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\",\n",
        "  \"gold_answer_research\": \"In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\",\n",
        "  \"gold_answer_marketing\": \"The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\"},\n",
        "65: {\"question\": \"What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\",\n",
        "  \"gold_answer_research\": \"Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\",\n",
        "  \"gold_answer_marketing\": \"Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\"},\n",
        "67: {\"question\": \"What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\",\n",
        "  \"gold_answer_research\": \"Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\",\n",
        "  \"gold_answer_marketing\": \"The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\"},\n",
        "69: {\"question\": \"What is the role of manual assessment in the validation of language model predictions according to the text provided?\",\n",
        "  \"gold_answer_research\": \"Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\",\n",
        "  \"gold_answer_marketing\": \"Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\"},\n",
        "70: {\"question\": \"What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\",\n",
        "  \"gold_answer_research\": \"The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\",\n",
        "  \"gold_answer_marketing\": \"The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\"},\n",
        "73: {\"question\": \"What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\",\n",
        "  \"gold_answer_research\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\",\n",
        "  \"gold_answer_marketing\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\"},\n",
        "74: {\"question\": \"What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\",\n",
        "  \"gold_answer_research\": \"Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\",\n",
        "  \"gold_answer_marketing\": \"Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\"},\n",
        "75: {\"question\": \"Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\",\n",
        "  \"gold_answer_research\": \"Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\",\n",
        "  \"gold_answer_marketing\": \"Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\"},\n",
        "76: {\"question\": \"What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\",\n",
        "  \"gold_answer_research\": \"A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\",\n",
        "  \"gold_answer_marketing\": \"A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\"},\n",
        "78: {\"question\": \"What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\",\n",
        "  \"gold_answer_research\": \"According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\",\n",
        "  \"gold_answer_marketing\": \"The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\"},\n",
        "80: {\"question\": \"How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\",\n",
        "  \"gold_answer_research\": \"Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\",\n",
        "  \"gold_answer_marketing\": \"KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\"},\n",
        "81: {\"question\": \"What are some common approaches to building an open-domain question answering system?\",\n",
        "  \"gold_answer_research\": \"Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\",\n",
        "  \"gold_answer_marketing\": \"Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\"},\n",
        "82: {\"question\": \"What is the difference between open-book and closed-book question answering?\",\n",
        "  \"gold_answer_research\": \"Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\",\n",
        "  \"gold_answer_marketing\": \"Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\"},\n",
        "84: {\"question\": \"What are the basic components of the Retriever-Reader framework in open-domain QA?\",\n",
        "  \"gold_answer_research\": \"The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\",\n",
        "  \"gold_answer_marketing\": \"The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\"},\n",
        "85: {\"question\": \"How is the TF-IDF model used in question answering retrieval systems?\",\n",
        "  \"gold_answer_research\": \"In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\",\n",
        "  \"gold_answer_marketing\": \"The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\"},\n",
        "86: {\"question\": \"Can neural networks enhance the process of information retrieval in QA systems?\",\n",
        "  \"gold_answer_research\": \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\",\n",
        "  \"gold_answer_marketing\": \"Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\"},\n",
        "87: {\"question\": \"What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\",\n",
        "  \"gold_answer_research\": \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\",\n",
        "  \"gold_answer_marketing\": \"Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\"},\n",
        "88: {\"question\": \"How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\",\n",
        "  \"gold_answer_research\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\",\n",
        "  \"gold_answer_marketing\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\"},\n",
        "89: {\"question\": \"What is the main goal of prompt engineering in language models?\",\n",
        "  \"gold_answer_research\": \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n",
        "  \"gold_answer_marketing\": \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\"},\n",
        "91: {\"question\": \"What are some known biases that can affect the performance of few-shot classification in LLMs?\",\n",
        "  \"gold_answer_research\": \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\",\n",
        "  \"gold_answer_marketing\": \"Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\"},\n",
        "92: {\"question\": \"Why might increasing model size not reduce variance in model performance with varying prompts?\",\n",
        "  \"gold_answer_research\": \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\",\n",
        "  \"gold_answer_marketing\": \"Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\"},\n",
        "93: {\"question\": \"What is the benefit of instruction-based finetuning in language models?\",\n",
        "  \"gold_answer_research\": \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\",\n",
        "  \"gold_answer_marketing\": \"The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\"},\n",
        "94: {\"question\": \"Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\",\n",
        "  \"gold_answer_research\": \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\",\n",
        "  \"gold_answer_marketing\": \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\"},\n",
        "95: {\"question\": \"What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\",\n",
        "  \"gold_answer_research\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\",\n",
        "  \"gold_answer_marketing\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\"},\n",
        "96: {\"question\": \"How do augmented language models with external tools differ from regular models in functionality?\",\n",
        "  \"gold_answer_research\": \"Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\",\n",
        "  \"gold_answer_marketing\": \"Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\"},\n",
        "97: {\"question\": \"What can be inferred about the utilization of attention in neural networks?\",\n",
        "  \"gold_answer_research\": \"Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\",\n",
        "  \"gold_answer_marketing\": \"Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\"},\n",
        "101: {\"question\": \"Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\",\n",
        "  \"gold_answer_research\": \"Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\",\n",
        "  \"gold_answer_marketing\": \"Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\"},\n",
        "102: {\"question\": \"What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\",\n",
        "  \"gold_answer_research\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\",\n",
        "  \"gold_answer_marketing\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\"},\n",
        "103: {\"question\": \"How does the transformer model variate from traditional sequence-aligned recurrent architectures?\",\n",
        "  \"gold_answer_research\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\",\n",
        "  \"gold_answer_marketing\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\"},\n",
        "104: {\"question\": \"What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\",\n",
        "  \"gold_answer_research\": \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\",\n",
        "  \"gold_answer_marketing\": \"The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\"},\n",
        "}\n",
        "\n",
        "\n",
        "test_questions = {\n",
        "4: {\"question\": \"When was the transformer architecture introduced, and by which organization?\"},\n",
        "5: {\"question\": \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"},\n",
        "6: {\"question\": \"What benchmarks or ratings are used to compare the capabilities of different language models?\"},\n",
        "10: {\"question\": \"What are some of the primary applications for language models in technology and computing?\"},\n",
        "14: {\"question\": \"How are language models typically evaluated and what benchmarks are used for this purpose?\"},\n",
        "15: {\"question\": \"What datasets are available for evaluating language processing systems?\"},\n",
        "21: {\"question\": \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"},\n",
        "26: {\"question\": \"According to DeepMind, how should the number of training tokens change relative to the model size?\"},\n",
        "29: {\"question\": \"How do the sizes of models in the Gopher family range?\"},\n",
        "31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
        "32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
        "37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
        "40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
        "42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
        "49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
        "56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
        "57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
        "58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
        "66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
        "68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
        "71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
        "72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
        "77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
        "79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
        "83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
        "90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
        "98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
        "99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
        "100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABdr23NUSbTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def106b6-a71b-4209-e587-bb3c65b7ac87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What purpose do large language models serve in the field of natural language processing?',\n",
              " 'gold_answer_research': 'Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.',\n",
              " 'gold_answer_marketing': 'Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "validation_questions_answers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra_p5DhzSbeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058f44a7-bd48-4695-e8fc-1651ccf92653"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'When was the transformer architecture introduced, and by which organization?'}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "test_questions[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 Implementing the POC RAG System\n",
        "\n",
        "Define the RAG POC pipeline and relevant prompts."
      ],
      "metadata": {
        "id": "v9FpzfIFMOd_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQbNGECSq1g"
      },
      "source": [
        "The RAG system needs to support two types of users:\n",
        "\n",
        "1. The engineers, who require pretty detailed information when they ask questions.\n",
        "2. The marketing team and supporting staff, who want to better understand GenAI products as a whole, but at a higher-level."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.1 Defining the Embedding Model"
      ],
      "metadata": {
        "id": "2UajVKSJRZIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.1.a) Embedding model setup\n",
        "\n",
        "# NOTE: Model choices include:\n",
        "# (1) 'sentence-transformers/all-MiniLM-L6-v2' (embedding dimension = 384)\n",
        "# (2) 'sentence-transformers/multi-qa-mpnet-base-dot-v1' (embedding dimension = 768)\n",
        "# (3) 'avsolatorio/GIST-Embedding-v0' (embedding dimension = 768)\n",
        "\n",
        "%%capture\n",
        "# sd_base_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# sd_base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")\n",
        "sd_base_embeddings = HuggingFaceEmbeddings(model_name=\"avsolatorio/GIST-Embedding-v0\")"
      ],
      "metadata": {
        "id": "mAwGZs3CMKv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.1.b) Embedding model test\n",
        "\n",
        "sd_text = \"This is a test document.\"\n",
        "sd_query_result = sd_base_embeddings.embed_query(sd_text)\n",
        "print(f'Embedding dimension: {len(sd_query_result)}')\n",
        "sd_doc_result = sd_base_embeddings.embed_documents([sd_text, \"This is not a test document.\"])\n",
        "len(sd_doc_result)"
      ],
      "metadata": {
        "id": "NFhvXyj1RUVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ed2783-528a-48df-eede-dce1a59bfe73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension: 768\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.2 Chunking Data and Creating the Vector Database"
      ],
      "metadata": {
        "id": "cnS6U3iDRphY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.a) Load example document to seed vector DB once created\n",
        "\n",
        "sd_loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "sd_documents = sd_loader.load()"
      ],
      "metadata": {
        "id": "2LCplVDCUbC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.b) Split text into chunks (retrieval units)\n",
        "\n",
        "# DEFINE CHUNK SIZE AND OVERLAP PARAMETERS - FOR LONGER TEXTS, CONSIDER SMALLER CHUNKS WITH HIGHER OVERLAP\n",
        "SD_CHUNK_SIZE=400       # Originally 128\n",
        "SD_OVERLAP=20           # Originally 0\n",
        "\n",
        "# Apply parameters to text splitter\n",
        "sd_text_splitter = RecursiveCharacterTextSplitter(chunk_size=SD_CHUNK_SIZE, chunk_overlap=SD_OVERLAP)\n",
        "sd_splits = sd_text_splitter.split_documents(sd_documents)\n",
        "print('Number of splits/chunks: ', str(len(sd_splits)))"
      ],
      "metadata": {
        "id": "KRSManZbUdjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197b509b-ccbb-4a75-de4d-901831324c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits/chunks:  162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.c) Profile/view content from one chunk (split)\n",
        "\n",
        "sd_splits[4].page_content"
      ],
      "metadata": {
        "id": "exvKWUQAUjxJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0e6995b8-fd47-4d28-b296-53a1cd95627d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.d) Vector database (vector store) setup\n",
        "\n",
        "# NOTE: Defining a different vector DB here so as not to conflict with original/test example\n",
        "\n",
        "sd_qdrant_vectorstore = Qdrant.from_documents(sd_splits,\n",
        "    sd_base_embeddings,\n",
        "    location=\":memory:\",  # Local mode with in-memory storage only\n",
        "    collection_name=\"sd_rag_tech_db\",\n",
        "    force_recreate=True\n",
        ")\n",
        "\n",
        "sd_retriever = sd_qdrant_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "-HUjN3v1R-vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.e) Test query and similarity search\n",
        "\n",
        "sd_query = \"What is Chain of Thought doing?\"\n",
        "sd_docs = sd_qdrant_vectorstore.similarity_search_by_vector(sd_base_embeddings.embed_query(sd_query)) # will rank the splits"
      ],
      "metadata": {
        "id": "wqtZUCnBa7v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.2.f) View ranked docs from test query and similarity search\n",
        "\n",
        "sd_docs"
      ],
      "metadata": {
        "id": "g5AcVWWebQgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2fecd8-b009-4559-8ef9-71fe4dedd2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'a739aa9889144076809804eabde6241d', '_collection_name': 'sd_rag_tech_db'}),\n",
              " Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'e87878d661a74bc8b27468ac021a5f75', '_collection_name': 'sd_rag_tech_db'}),\n",
              " Document(page_content='model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'b416cb26cce74860a0a9af3c589607f0', '_collection_name': 'sd_rag_tech_db'}),\n",
              " Document(page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '711e450fc6a1454d9a97c665ec396216', '_collection_name': 'sd_rag_tech_db'})]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.3 Loading Documents into the Vector Database"
      ],
      "metadata": {
        "id": "eDNxOLEzWoVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.a) Split first set of documents into chunks for vector DB\n",
        "\n",
        "# arXiv Documents\n",
        "\n",
        "# Index document chunks\n",
        "sd_splits = sd_text_splitter.split_documents(all_arxiv_pages)\n",
        "for idx, text in enumerate(sd_splits):\n",
        "    sd_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(sd_splits))"
      ],
      "metadata": {
        "id": "TqwNHwcCWzr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333f66d0-4ac8-474e-eba2-68a07d6571a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits/chunks:  4133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.b) Profile/view a single chunk (split)\n",
        "\n",
        "sd_splits[0]"
      ],
      "metadata": {
        "id": "rc4rhPc-Xdml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13eb3fa9-7db8-4260-a322-fe2565742a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract', metadata={'source': 'https://arxiv.org/pdf/2005.11401.pdf', 'file_path': 'https://arxiv.org/pdf/2005.11401.pdf', 'page': 0, 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210413004838Z', 'modDate': 'D:20210413004838Z', 'trapped': '', 'page_num': 0, 'doc_num': 1, 'doc_source': 'ArXiv', 'split_id': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.c) Add first set of vectors to DB\n",
        "\n",
        "%%capture\n",
        "sd_qdrant_vectorstore.add_documents(documents=sd_splits)"
      ],
      "metadata": {
        "id": "osp0tBpiXlND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.d) Test retrieval of nearest neighbor to an example query\n",
        "\n",
        "sd_query = \"How can we train a model for preferences?\"\n",
        "sd_found_docs = sd_qdrant_vectorstore.similarity_search_with_score(sd_query)"
      ],
      "metadata": {
        "id": "e-3DtjQoYAky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.e) Display output/results of test\n",
        "\n",
        "print(sd_found_docs[0][0].page_content)\n",
        "print(sd_found_docs[0][1])"
      ],
      "metadata": {
        "id": "ZYZTB-JoYXIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e3172e-2c37-427c-ad79-f067e88c8c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For additional details about the human study, in-\n",
            "cluding the web interface presented to raters and\n",
            "the list of human volunteers, see Appendix D.3.\n",
            "7\n",
            "Discussion\n",
            "Learning from preferences is a powerful, scalable framework for training capable, aligned language\n",
            "models. We have introduced DPO, a simple training paradigm for training language models from\n",
            "0.869556410103083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.f) Split second set of documents into chunks for vector DB\n",
        "\n",
        "# Wikipedia Documents\n",
        "\n",
        "# Index document chunks\n",
        "sd_wiki_splits = sd_text_splitter.split_documents(wiki_docs)\n",
        "for idx, text in enumerate(sd_wiki_splits):\n",
        "    sd_wiki_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits/chunks: ', len(sd_wiki_splits))"
      ],
      "metadata": {
        "id": "DpZ_zSG0Yyde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e7d725-d800-4722-ca73-2320d636eb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits/chunks:  55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.g) Add second set of vectors to DB\n",
        "\n",
        "%%capture\n",
        "sd_qdrant_vectorstore.add_documents(documents=sd_wiki_splits)"
      ],
      "metadata": {
        "id": "ZwMa3VMQZeHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.h) Split third set of documents into chunks for vector DB\n",
        "\n",
        "# Web Documents\n",
        "\n",
        "# Index document chunks\n",
        "sd_web_splits = sd_text_splitter.split_documents(web_documents)\n",
        "for idx, text in enumerate(sd_web_splits):\n",
        "    sd_web_splits[idx].metadata['split_id'] = idx\n",
        "\n",
        "print('Number of splits: ', len(sd_web_splits))"
      ],
      "metadata": {
        "id": "qHb7SCR7ZlCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2c8543-617a-4ecb-db42-3a808ef2d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits:  418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.3.i) Add third set of vectors to DB\n",
        "\n",
        "%%capture\n",
        "sd_qdrant_vectorstore.add_documents(documents=sd_web_splits)"
      ],
      "metadata": {
        "id": "yomGQUkZaI_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.4 Setting up a RAG Chain with LLM"
      ],
      "metadata": {
        "id": "Xxc36c8BYh7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.a) Define formatting function that combines a list of chunks into one string\n",
        "\n",
        "def sd_format_docs(sd_docs):\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in sd_docs])"
      ],
      "metadata": {
        "id": "uYEbRvr0dk-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.b) Define second formatting function that ensures clean output from RAG Chain\n",
        "\n",
        "def clean_output(input_string):\n",
        "\n",
        "    # Split string at '[/INST]' to remove extra noise\n",
        "    split_string = input_string.split('[/INST]')\n",
        "\n",
        "    # If there are at least two parts after splitting, return the second part\n",
        "    if len(split_string) > 1:\n",
        "        formatted_text = split_string[1].strip().replace('\\n', '') # Return the content after '[/INST]' with leading/trailing whitespaces and '\\n' removed\n",
        "        return formatted_text\n",
        "    else:\n",
        "        formatted_text = input_string.replace('\\n', '') # Return original string with '\\n' removed\n",
        "        return formatted_text"
      ],
      "metadata": {
        "id": "qYB3OZkosBzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.c) Define output parser function, RAG template, and RAG prompt\n",
        "\n",
        "# EMPLOYING DIFFERENT PROMPTS FOR USER PERSONAS\n",
        "sd_output_parser = StrOutputParser()\n",
        "sd_rag_template_research = \"\"\"[INST]You are a highly capable Q&A assistant, skilled at providing clear and detailed responses tailored to the engineering staff and technical researchers at your tech company. Your task is to answer the question based solely on the provided context information. Avoid starting your response with phrases like 'Based on the given context', 'Based on the provided context', or 'According to the context provided'. Aim for a response between four and six sentences in length, with each sentence averaging between 100 and 160 words. Please ensure your answer is technically accurate and aligns with the provided context. Where appropriate, support your answer with concise references to relevant examples. If the question is open-ended or complex, break down the problem and provide a structured explanation.\\n\\nHere is the context:\\n{context} \\n\\nHere is the question:\\n{question}[/INST]\"\"\"\n",
        "sd_rag_template_marketing = \"\"\"[INST]You are a highly capable Q&A assistant, skilled at crafting concise and high-level responses suited for the marketing team at your tech company. Your task is to answer the question based solely on the provided context information. Avoid starting your response with phrases like 'Based on the given context', 'Based on the provided context', or 'According to the context provided'. Keep your answer concise and simple, aiming for one or two sentences with each sentence averaging between 120 and 180 words. Please ensure your answer provides a quick and high-level snapshot of the most important insights from the context that relate to the question. If the question is open-ended or complex, break down the problem and provide a reasonable explanation or best guess.\\n\\nHere is the context:\\n{context} \\n\\nHere is the question:\\n{question}[/INST]\"\"\"\n",
        "sd_rag_prompt_research = ChatPromptTemplate.from_template(sd_rag_template_research)\n",
        "sd_rag_prompt_marketing = ChatPromptTemplate.from_template(sd_rag_template_marketing)\n"
      ],
      "metadata": {
        "id": "CX_w_8m2c3zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.d) Set up OS LLM (Mistral) RAG Chains\n",
        "\n",
        "# NOTE: Mistral has 7B trainable parameters\n",
        "\n",
        "# RESEARCH TEAM CHAIN\n",
        "sd_os_rag_chain_research = (\n",
        "    {\"context\": sd_retriever | sd_format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | sd_rag_prompt_research\n",
        "    | mistral_llm_lc\n",
        ")\n",
        "\n",
        "# MARKETING TEAM CHAIN\n",
        "sd_os_rag_chain_marketing = (\n",
        "    {\"context\": sd_retriever | sd_format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | sd_rag_prompt_marketing\n",
        "    | mistral_llm_lc\n",
        ")"
      ],
      "metadata": {
        "id": "JhxX0wl6ex5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.e) Test OS LLM (Mistral) RAG Chain outputs\n",
        "\n",
        "# RESEARCH TEAM TEST\n",
        "test_os_llm_rag_output_research = sd_os_rag_chain_research.invoke('What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?')\n",
        "cleaned_test_os_llm_rag_output_research = clean_output(test_os_llm_rag_output_research)\n",
        "print('Research Team Answer Test:', cleaned_test_os_llm_rag_output_research)\n",
        "\n",
        "# MARKETING TEAM TEST\n",
        "test_os_llm_rag_output_marketing = sd_os_rag_chain_marketing.invoke('What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?')\n",
        "cleaned_test_os_llm_rag_output_marketing = clean_output(test_os_llm_rag_output_marketing)\n",
        "print('Marketing Team Answer Test:', cleaned_test_os_llm_rag_output_marketing)"
      ],
      "metadata": {
        "id": "rxi1Nn6LeJkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d413c8-1823-4400-f18a-dcbfb3e8fa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research Team Answer Test: The Chain-of-Thought (CoT) prompting technique involves generating a sequence of short sentences that describes reasoning logic step by step, leading to the final answer. This method helps enhance model performance on complex tasks, especially when large models such as those with over 50 billion parameters are used. In general, simple tasks do not greatly benefit from CoT. However, this technique can be applied to various fields such as natural language processing (NLP), where it has been shown to improve model interpretability and elicit reasoning in large language models.\n",
            "Marketing Team Answer Test: Chain-of-Thought (CoT) prompting is a technique used to enhance the performance of large language models on complex tasks. It involves generating a sequence of short sentences to describe reasoning logics step by step, leading to the final answer. This method helps the model think through problems systematically, making them easier to solve. CoT is especially beneficial for tasks that require multi-step reasoning or decomposition of difficult problems into smaller, more manageable parts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.f) Set up proprietary LLM (Cohere) RAG Chains\n",
        "\n",
        "# NOTE: Cohere has >50B (?) trainable parameters\n",
        "\n",
        "# RESEARCH TEAM CHAIN\n",
        "sd_pro_rag_chain_research = (\n",
        "    {\"context\": sd_retriever | sd_format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | sd_rag_prompt_research\n",
        "    | cohere_chat_model\n",
        "    | sd_output_parser\n",
        ")\n",
        "\n",
        "# MARKETING TEAM CHAIN\n",
        "sd_pro_rag_chain_marketing = (\n",
        "    {\"context\": sd_retriever | sd_format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | sd_rag_prompt_marketing\n",
        "    | cohere_chat_model\n",
        "    | sd_output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "Q5MlJU5NeYcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.4.g) Test proprietary LLM (Cohere) RAG Chain outputs\n",
        "\n",
        "# RESEARCH TEAM TEST\n",
        "test_pro_llm_rag_output_research = sd_pro_rag_chain_research.invoke('What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?')\n",
        "cleaned_test_pro_llm_rag_output_research = clean_output(test_pro_llm_rag_output_research)\n",
        "print('Research Team Answer Test:', cleaned_test_pro_llm_rag_output_research)\n",
        "\n",
        "# MARKETING TEAM TEST\n",
        "test_pro_llm_rag_output_marketing = sd_pro_rag_chain_marketing.invoke('What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?')\n",
        "cleaned_test_pro_llm_rag_output_marketing = clean_output(test_pro_llm_rag_output_marketing)\n",
        "print('Marketing Team Answer Test:', cleaned_test_pro_llm_rag_output_marketing)"
      ],
      "metadata": {
        "id": "AXe4cD40ecSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db06181a-a1cb-46b7-a3f5-28c6c1ad097f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research Team Answer Test: Chain-of-Thought (CoT) prompting is a technique where a model is instructed to generate a sequence of short sentences that describe its reasoning logic step by step, ultimately leading to a final answer. This approach is particularly beneficial for complicated reasoning tasks when using large language models. By breaking down complex tasks into a series of smaller, more manageable steps, CoT improves model performance and provides interpretability into the model's thought process. While CoT can offer slight enhancements to simple tasks, its strength lies in tackling intricate problems, leveraging the power of large language models with billions of parameters. This method enhances the model's ability to handle challenging tasks and provides insights into their decision-making processes.\n",
            "Marketing Team Answer Test: The Chain-of-Thought (CoT) prompting technique enhances large language models' performance on complex tasks by instructing them to generate a sequence of short sentences that describe the step-by-step reasoning process. CoT is particularly beneficial for complicated reasoning tasks, helping to break down challenging problems into smaller, more manageable steps, and providing insight into the model's decision-making process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.5 Brief EDA of Question and 'Gold' Answer Set"
      ],
      "metadata": {
        "id": "tqi6hmBujZWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.a) Split validation questions and answers into separate lists\n",
        "\n",
        "# PART 1: Store all validation questions in a single list\n",
        "v_questions = [item['question'] for item in validation_questions_answers.values()]\n",
        "\n",
        "# PART 2: Store all optimal \"research\" answers in a single list\n",
        "v_research_answers = [item['gold_answer_research'] for item in validation_questions_answers.values()]\n",
        "\n",
        "# PART 3: Store all optimal \"marketing\" answers in a single list\n",
        "v_marketing_answers = [item['gold_answer_marketing'] for item in validation_questions_answers.values()]\n",
        "\n",
        "# PART 4: Print the lists to verify results\n",
        "print('Validation Questions:', v_questions)\n",
        "print('\"Gold\" Answers - Research:', v_research_answers)\n",
        "print('\"Gold\" Answers - Marketing:', v_marketing_answers)"
      ],
      "metadata": {
        "id": "T24XRJ9KxLqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70259de4-30c4-4454-b0b7-9fa9d73c644f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Questions: ['What purpose do large language models serve in the field of natural language processing?', 'How does a large language model learn from text during training?', 'What are some key architectures behind the development of large language models?', 'Can you name some specific large language models and the companies or organizations that have developed them?', 'What licensing models have been adopted for the distribution of source-available language models?', 'What are language models and what is their purpose in natural language processing?', 'How have language models evolved in terms of architecture, from the 1980s to present times?', 'Can you explain how maximum entropy language models work and what the partition function signifies?', 'What is the benefit of using continuous space embeddings in recurrent neural network language models?', 'What challenges do large language models face in mirroring human cognitive patterns?', 'What factors influenced the development of generative language models by Anthropic?', 'What is Constitutional AI and how does it affect the functionality of AI systems?', 'How do advances in AI models impact their ability to interact with different types of data, such as images?', 'What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?', 'How has the token handling capacity changed between different versions of the Claude model?', \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\", 'How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?', 'Who developed the language model family known as Chinchilla?', 'What benchmark did Chinchilla achieve an average accuracy of 67.5% on?', 'What is the relationship between Chinchilla and the Gopher language model families?', 'What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?', 'What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?', 'What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?', 'What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?', 'How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?', 'What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?', 'What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?', 'What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?', \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\", 'What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?', 'What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?', 'What are some challenges in training retrieval systems and how are negative samples used to address them?', 'What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?', 'What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?', 'What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?', 'What methods are typically employed to create training data for embedding models that use task-specific instructions?', 'Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?', 'What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?', 'What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?', 'What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?', 'Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?', 'What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?', 'What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?', 'What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?', 'What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?', 'What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?', 'What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?', 'What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?', 'What is the role of manual assessment in the validation of language model predictions according to the text provided?', 'What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?', 'What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?', 'What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?', 'Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?', 'What components does a fact verification task typically involve in order to assess the accuracy of a given statement?', 'What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?', 'How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?', 'What are some common approaches to building an open-domain question answering system?', 'What is the difference between open-book and closed-book question answering?', 'What are the basic components of the Retriever-Reader framework in open-domain QA?', 'How is the TF-IDF model used in question answering retrieval systems?', 'Can neural networks enhance the process of information retrieval in QA systems?', 'What is the importance of fine-tuning in the context of QA data for open-domain question answering models?', 'How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?', 'What is the main goal of prompt engineering in language models?', 'What are some known biases that can affect the performance of few-shot classification in LLMs?', 'Why might increasing model size not reduce variance in model performance with varying prompts?', 'What is the benefit of instruction-based finetuning in language models?', 'Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?', 'What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?', 'How do augmented language models with external tools differ from regular models in functionality?', 'What can be inferred about the utilization of attention in neural networks?', 'Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?', 'What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?', 'How does the transformer model variate from traditional sequence-aligned recurrent architectures?', 'What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?']\n",
            "\"Gold\" Answers - Research: ['Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.', \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\", 'Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.', 'Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.', 'Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.', 'Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.', \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\", \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\", 'Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.', 'Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.', \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\", 'Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.', 'Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.', \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\", 'The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.', \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\", \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\", \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\", 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).', 'The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.', 'The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.', 'The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.', 'Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.', 'One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.', \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\", 'Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.', \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\", 'Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.', \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\", 'The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.', 'Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.', 'Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.', \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\", 'Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.', 'The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.', 'To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.', 'Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.', 'A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.', \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\", 'The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.', 'To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.', \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\", 'One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.', 'Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.', \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\", 'In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.', 'Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.', 'Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.', 'Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.', 'The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.', \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\", 'Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.', 'Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.', 'A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.', 'According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.', 'Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.', 'Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.', 'Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.', 'The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.', 'In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.', \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\", \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\", 'Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.', \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\", \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\", \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\", \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\", \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\", \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\", 'Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.', 'Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.', 'Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.', 'Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.', \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\", \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\"]\n",
            "\"Gold\" Answers - Marketing: ['Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.', 'A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.', 'Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.', 'Chinchilla by DeepMind, GPT-3 by OpenAI.', 'Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.', 'Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.', 'Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.', 'Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.', 'Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.', 'Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.', 'Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.', \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\", 'Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.', 'The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.', 'The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.', \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\", 'Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.', 'The research team at DeepMind developed the language model family known as Chinchilla.', 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).', 'Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.', 'The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.', 'The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.', 'Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.', 'Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.', 'The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.', 'Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.', 'The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.', 'Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.', 'Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.', 'The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.', 'The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.', 'Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.', 'Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.', 'Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.', 'The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.', 'Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.', 'The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.', 'A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.', 'Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.', 'The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.', 'Answer: Evaluate based on faithfulness, answer relevance, and context relevance.', 'Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.', 'Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.', 'Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.', 'The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.', 'The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.', 'Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.', 'The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.', 'Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.', 'The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.', 'The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.', 'Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.', 'Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.', 'A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.', 'The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).', 'KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.', 'Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.', 'Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.', 'The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.', 'The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.', 'Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.', 'Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.', 'Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.', 'The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.', 'Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.', 'Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.', 'The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.', \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\", 'Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.', 'Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.', 'Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.', 'Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.', 'Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.', 'The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.', 'The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.b) Examine average, minimum, and maximum sequence length of optimal \"research\" answers for validation questions\n",
        "\n",
        "# PART 1: Compute average length of \"research\" answers\n",
        "v_avg_research_answer_length = sum(len(answer) for answer in v_research_answers) / len(v_research_answers)\n",
        "\n",
        "# PART 2: Compute minimum length of \"research\" answers\n",
        "v_min_research_answer_length = min(len(answer) for answer in v_research_answers)\n",
        "\n",
        "# PART 3: Compute maximum length of \"research\" answers\n",
        "v_max_research_answer_length = max(len(answer) for answer in v_research_answers)\n",
        "\n",
        "# PART 4: Print average length, minimum length, and maximum length\n",
        "print('Average \"Gold\" Research Answer Length:', v_avg_research_answer_length)\n",
        "print('Minimum \"Gold\" Research Answer Length:', v_min_research_answer_length)\n",
        "print('Maximum \"Gold\" Research Answer Length:', v_max_research_answer_length)"
      ],
      "metadata": {
        "id": "T2FeDxMr1dag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febf024c-facc-4308-9f79-b631d895c132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average \"Gold\" Research Answer Length: 613.1733333333333\n",
            "Minimum \"Gold\" Research Answer Length: 124\n",
            "Maximum \"Gold\" Research Answer Length: 797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.c) Examine histogram of sequence lengths of optimal \"research\" answers for validation questions\n",
        "\n",
        "# Plot histogram showing lengths of \"research\" answers\n",
        "plt.hist([len(answer) for answer in v_research_answers], bins=20)\n",
        "plt.xlabel('Answer Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of \"Gold\" Research Answer Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0yy2SG23LI-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "bce51499-b47c-473a-f30b-631ce51d55fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.d) Examine average, minimum, and maximum number of sentences in optimal \"research\" answers for validation questions\n",
        "\n",
        "# PART 1: Define empty list to hold number of sentences within each \"research\" answer\n",
        "v_research_answer_sentence_counter = []\n",
        "\n",
        "# PART 2: Loop through each \"research\" answer, compute number of sentences, and store number in list\n",
        "for answer in v_research_answers:\n",
        "    v_research_answer_sentences = re.split(r'[.!?]+', answer)\n",
        "    v_research_answer_num_sentences = max(len(v_research_answer_sentences)-1, 1)\n",
        "    v_research_answer_sentence_counter.append(v_research_answer_num_sentences)\n",
        "\n",
        "# PART 3: Compute average number of sentences in \"research\" answers\n",
        "v_avg_research_answer_sentences = sum(v_research_answer_sentence_counter) / len(v_research_answer_sentence_counter)\n",
        "\n",
        "# PART 4: Compute minimum number of sentences in \"research\" answers\n",
        "v_min_research_answer_sentences = min(v_research_answer_sentence_counter)\n",
        "\n",
        "# PART 5: Compute maximum number of sentences in \"research\" answers\n",
        "v_max_research_answer_sentences = max(v_research_answer_sentence_counter)\n",
        "\n",
        "# Part 6: Print average sentences, minimum sentences, and maximum sentences\n",
        "print('Average \"Gold\" Research Answer Sentences:', v_avg_research_answer_sentences)\n",
        "print('Minimum \"Gold\" Research Answer Sentences:', v_min_research_answer_sentences)\n",
        "print('Maximum \"Gold\" Research Answer Sentences:', v_max_research_answer_sentences)"
      ],
      "metadata": {
        "id": "ukvykllA6aQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c86ccbf-d896-43d6-8005-3776707d2294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average \"Gold\" Research Answer Sentences: 3.7333333333333334\n",
            "Minimum \"Gold\" Research Answer Sentences: 2\n",
            "Maximum \"Gold\" Research Answer Sentences: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.e) Examine histogram of number of sentences in optimal \"research\" answers for validation questions\n",
        "\n",
        "# Plot histogram showing sentences in \"research\" answers\n",
        "plt.hist(v_research_answer_sentence_counter, bins=4)\n",
        "plt.xlabel('Number of Sentences')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of \"Gold\" Research Answer Sentences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nMPjDsfT9d7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "f56840f5-f263-4f64-d683-55828540de53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.f) Examine average, minimum, and maximum sequence length of optimal \"marketing\" answers for validation questions\n",
        "\n",
        "# PART 1: Compute average length of \"marketing\" answers\n",
        "v_avg_marketing_answer_length = sum(len(answer) for answer in v_marketing_answers) / len(v_marketing_answers)\n",
        "\n",
        "# PART 2: Compute minimum length of \"marketing\" answers\n",
        "v_min_marketing_answer_length = min(len(answer) for answer in v_marketing_answers)\n",
        "\n",
        "# PART 3: Compute maximum length of \"marketing\" answers\n",
        "v_max_marketing_answer_length = max(len(answer) for answer in v_marketing_answers)\n",
        "\n",
        "# PART 4: Print average length, minimum length, and maximum length\n",
        "print('Average \"Gold\" Marketing Answer Length:', v_avg_marketing_answer_length)\n",
        "print('Minimum \"Gold\" Marketing Answer Length:', v_min_marketing_answer_length)\n",
        "print('Maximum \"Gold\" Marketing Answer Length:', v_max_marketing_answer_length)"
      ],
      "metadata": {
        "id": "CjgYjdR82jUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f05e3a-0735-4289-a5cb-6a42868b18a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average \"Gold\" Marketing Answer Length: 266.0933333333333\n",
            "Minimum \"Gold\" Marketing Answer Length: 40\n",
            "Maximum \"Gold\" Marketing Answer Length: 582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.g) Examine histogram of sequence lengths of optimal \"marketing\" answers for validation questions\n",
        "\n",
        "# Plot histogram showing lengths of \"marketing\" answers\n",
        "plt.hist([len(answer) for answer in v_marketing_answers], bins=20)\n",
        "plt.xlabel('Answer Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of \"Gold\" Marketing Answer Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_4VGxQld3wgp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "566ffde4-5d68-4de4-c3db-fdc3406964a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.h) Examine average, minimum, and maximum number of sentences in optimal \"marketing\" answers for validation questions\n",
        "\n",
        "# PART 1: Define empty list to hold number of sentences within each \"marketing\" answer\n",
        "v_marketing_answer_sentence_counter = []\n",
        "\n",
        "# PART 2: Loop through each \"marketing\" answer, compute number of sentences, and store number in list\n",
        "for answer in v_marketing_answers:\n",
        "    v_marketing_answer_sentences = re.split(r'[.!?]+', answer)\n",
        "    v_marketing_answer_num_sentences = max(len(v_marketing_answer_sentences)-1, 1)\n",
        "    v_marketing_answer_sentence_counter.append(v_marketing_answer_num_sentences)\n",
        "\n",
        "# PART 3: Compute average number of sentences in \"marketing\" answers\n",
        "v_avg_marketing_answer_sentences = sum(v_marketing_answer_sentence_counter) / len(v_marketing_answer_sentence_counter)\n",
        "\n",
        "# PART 4: Compute minimum number of sentences in \"marketing\" answers\n",
        "v_min_marketing_answer_sentences = min(v_marketing_answer_sentence_counter)\n",
        "\n",
        "# PART 5: Compute maximum number of sentences in \"marketing\" answers\n",
        "v_max_marketing_answer_sentences = max(v_marketing_answer_sentence_counter)\n",
        "\n",
        "# PART 6: Print average sentences, minimum sentences, and maximum sentences\n",
        "print('Average \"Gold\" Marketing Answer Sentences:', v_avg_marketing_answer_sentences)\n",
        "print('Minimum \"Gold\" Marketing Answer Sentences:', v_min_marketing_answer_sentences)\n",
        "print('Maximum \"Gold\" Marketing Answer Sentences:', v_max_marketing_answer_sentences)"
      ],
      "metadata": {
        "id": "hFh_C32l99tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1db6f9-84bf-4a6c-e659-2949fd8b501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average \"Gold\" Marketing Answer Sentences: 1.4933333333333334\n",
            "Minimum \"Gold\" Marketing Answer Sentences: 1\n",
            "Maximum \"Gold\" Marketing Answer Sentences: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.5.i) Examine histogram of number of sentences in optimal \"marketing\" answers for validation questions\n",
        "\n",
        "# Plot histogram showing sentences in \"marketing\" answers\n",
        "plt.hist(v_marketing_answer_sentence_counter, bins=3)\n",
        "plt.xlabel('Number of Sentences')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of \"Gold\" Marketing Answer Sentences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VyIaty0O-R_4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c04a9e72-f019-405d-c90a-766501718cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.4.6 Mini-Batch Test (MBT) Setup"
      ],
      "metadata": {
        "id": "nwK3UgBi1Pbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.6.a) Create functions to generate RAG data for metric evaluations and tests\n",
        "\n",
        "# PART 1: Define RAG data generation function for validation question and answer set\n",
        "def generate_rag_data_dict_v(q_id_list, qa_dict, llm, persona):\n",
        "  \"\"\"Runs pre-defined RAG system using a supplied list of questions, answers, a specific llm, and a specific persona\n",
        "  and outputs a dictionary showing questions, RAG answers, contexts, document sources, and ground truths\n",
        "\n",
        "  Args:\n",
        "  * q_id_list = list of question ids (e.g., question dictionary keys) in the form of numbers\n",
        "  * qa_dict = dictionary of question and answer pairs (e.g., validation set)\n",
        "  * llm = large language model (e.g., mistral, cohere)\n",
        "  * persona = user persona/audience for generated answers (e.g., research, marketing)\n",
        "\n",
        "  Returns:\n",
        "  * rag_data_dict = dictionary of questions, RAG answers, contexts, document sources, and ground truths\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing question IDs, questions, RAG answers, contexts, document sources, and ground truths\n",
        "  question_ids = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  ground_truths = []\n",
        "  contexts = []\n",
        "  sources = []\n",
        "\n",
        "  # Obtain question IDs, questions, RAG answers, contexts, and document sources to append to lists\n",
        "  for id in q_id_list:\n",
        "\n",
        "    # Append question IDs\n",
        "    question_ids.append(id)\n",
        "\n",
        "    # Append questions\n",
        "    questions.append(qa_dict.get(id).get('question'))\n",
        "\n",
        "    # Append RAG answers\n",
        "    if llm == 'mistral':\n",
        "      if persona == 'research':\n",
        "        answers.append(clean_output(sd_os_rag_chain_research.invoke(qa_dict.get(id).get('question'))))\n",
        "      else:\n",
        "        answers.append(clean_output(sd_os_rag_chain_marketing.invoke(qa_dict.get(id).get('question'))))\n",
        "    else:\n",
        "      if persona == 'research':\n",
        "        answers.append(clean_output(sd_pro_rag_chain_research.invoke(qa_dict.get(id).get('question'))))\n",
        "      else:\n",
        "        answers.append(clean_output(sd_pro_rag_chain_marketing.invoke(qa_dict.get(id).get('question'))))\n",
        "\n",
        "    # Append ground truths\n",
        "    if persona == 'research':\n",
        "      ground_truths.append(qa_dict.get(id).get('gold_answer_research'))\n",
        "    else:\n",
        "      ground_truths.append(qa_dict.get(id).get('gold_answer_marketing'))\n",
        "\n",
        "    # Append contexts\n",
        "    contexts.append(\n",
        "        [doc.page_content for doc in sd_retriever.get_relevant_documents(qa_dict.get(id).get('question'))])\n",
        "\n",
        "    # Append document sources\n",
        "    sources.append(\n",
        "        [doc.metadata['source'] for doc in sd_retriever.get_relevant_documents(qa_dict.get(id).get('question'))])\n",
        "\n",
        "  # Convert lists to dictionaries of lists for indexing and evaluation metrics\n",
        "  rag_data_dict = {\n",
        "      \"q_id\": question_ids,\n",
        "      \"question\": questions,\n",
        "      \"answer\": answers,\n",
        "      \"contexts\": contexts,\n",
        "      \"sources\": sources,\n",
        "      \"ground_truth\": ground_truths\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_data_dict\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# PART 2: Define RAG data generation function for test question set (with no answers)\n",
        "def generate_rag_data_dict_t(q_id_list, q_dict, llm, persona):\n",
        "  \"\"\"Runs pre-defined RAG system using a supplied list of questions, a specific llm, and a specific persona\n",
        "  and outputs a dictionary showing questions, RAG answers, contexts, and document sources\n",
        "\n",
        "  Args:\n",
        "  * q_id_list = list of question ids (e.g., question dictionary keys) in the form of numbers\n",
        "  * q_dict = dictionary of questions (e.g., test set)\n",
        "  * llm = large language model (e.g., mistral, cohere)\n",
        "  * persona = user persona/audience for generated answers (e.g., research, marketing)\n",
        "\n",
        "  Returns:\n",
        "  * rag_data_dict = dictionary of questions, RAG answers, contexts, and document sources\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing question IDs, questions, RAG answers, contexts, and document sources\n",
        "  question_ids = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  contexts = []\n",
        "  sources = []\n",
        "\n",
        "  # Obtain question IDs, questions, RAG answers, contexts, and document sources to append to lists\n",
        "  for id in q_id_list:\n",
        "\n",
        "    # Append question IDs\n",
        "    question_ids.append(id)\n",
        "\n",
        "    # Append questions\n",
        "    questions.append(q_dict.get(id).get('question'))\n",
        "\n",
        "    # Append RAG answers\n",
        "    if llm == 'mistral':\n",
        "      if persona == 'research':\n",
        "        answers.append(clean_output(sd_os_rag_chain_research.invoke(q_dict.get(id).get('question'))))\n",
        "      else:\n",
        "        answers.append(clean_output(sd_os_rag_chain_marketing.invoke(q_dict.get(id).get('question'))))\n",
        "    else:\n",
        "      if persona == 'research':\n",
        "        answers.append(clean_output(sd_pro_rag_chain_research.invoke(q_dict.get(id).get('question'))))\n",
        "      else:\n",
        "        answers.append(clean_output(sd_pro_rag_chain_marketing.invoke(q_dict.get(id).get('question'))))\n",
        "\n",
        "    # Append contexts\n",
        "    contexts.append(\n",
        "        [doc.page_content for doc in sd_retriever.get_relevant_documents(q_dict.get(id).get('question'))])\n",
        "\n",
        "    # Append document sources\n",
        "    sources.append(\n",
        "        [doc.metadata['source'] for doc in sd_retriever.get_relevant_documents(q_dict.get(id).get('question'))])\n",
        "\n",
        "  # Convert lists to dictionaries of lists for indexing and evaluation metrics\n",
        "  rag_data_dict = {\n",
        "      \"q_id\": question_ids,\n",
        "      \"question\": questions,\n",
        "      \"answer\": answers,\n",
        "      \"contexts\": contexts,\n",
        "      \"sources\": sources\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_data_dict"
      ],
      "metadata": {
        "id": "xiP-B-KjpzFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.6.b) Test RAG system on mini-batch of validation question/answer pairs and test questions\n",
        "\n",
        "# REFERENCE: Count of validation questions and test questions\n",
        "# * Validation = 75\n",
        "# ** Question IDs:\n",
        "#   [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27,\n",
        "#    28, 30, 33, 34, 35, 36, 38, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52,\n",
        "#    54, 55, 59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 73, 74, 75, 76, 78, 80,\n",
        "#    81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 101, 102, 103,\n",
        "#    104]\n",
        "# * Test = 29\n",
        "# ** Question IDs:\n",
        "#   [4, 5, 6, 10, 14, 15, 21, 26, 29, 31, 32, 37, 40, 42, 49, 56, 57, 58, 66,\n",
        "#    68, 71, 72, 77, 79, 83, 90, 98, 99, 100]\n",
        "\n",
        "# PART 1: Define validation question IDs to run through RAG system\n",
        "v_q_ids_mbt = [\n",
        "    0,    # 5.2.1 Test Question 1\n",
        "    50,   # 5.2.2 Test Question 2\n",
        "    # 75,\n",
        "    # 95,\n",
        "    # 104,\n",
        "]\n",
        "\n",
        "# PART 2: Define test question IDs to run through RAG system\n",
        "t_q_ids_mbt = [\n",
        "    # 4,\n",
        "    # 10,\n",
        "    # 40,\n",
        "    83,   # 5.2.3 Test Question 3\n",
        "    # 100,\n",
        "]\n",
        "\n",
        "# PART 3: Generate RAG output for validation questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_v_research_mistral_mbt = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids_mbt,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_v_research_cohere_mbt = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids_mbt,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_v_marketing_mistral_mbt = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids_mbt,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_v_marketing_cohere_mbt = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids_mbt,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='marketing')\n",
        "\n",
        "# PART 4: Generate RAG output for test questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_t_research_mistral_mbt = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids_mbt,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_t_research_cohere_mbt = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids_mbt,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_t_marketing_mistral_mbt = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids_mbt,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_t_marketing_cohere_mbt = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids_mbt,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='marketing')"
      ],
      "metadata": {
        "id": "r4UjMqJwp6P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.6.c) View sample RAG system output in dictionary format for troubelshooting/inspection\n",
        "\n",
        "# Select one of the following outputs to view and comment out all others\n",
        "rag_output_sample_view_mbt = [\n",
        "    # rag_output_v_research_mistral_mbt,\n",
        "    rag_output_v_research_cohere_mbt,\n",
        "    # rag_output_t_research_mistral_mbt,\n",
        "    # rag_output_t_research_cohere_mbt,\n",
        "    # rag_output_v_marketing_mistral_mbt,\n",
        "    # rag_output_v_marketing_cohere_mbt,\n",
        "    # rag_output_t_marketing_mistral_mbt,\n",
        "    # rag_output_t_marketing_cohere_mbt,\n",
        "]\n",
        "\n",
        "# View sample output\n",
        "display(rag_output_sample_view_mbt)"
      ],
      "metadata": {
        "id": "aU6aGGrRyCkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45e75cfa-f474-40ab-b1f3-c6bee2ab38d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'q_id': [0, 50],\n",
              "  'question': ['What purpose do large language models serve in the field of natural language processing?',\n",
              "   'What methods are typically employed to create training data for embedding models that use task-specific instructions?'],\n",
              "  'answer': ['Large language models (LLMs) have become integral tools in natural language processing (NLP), offering a wide range of applications. These models are designed to process and analyze large volumes of text data, learning statistical relationships between words and phrases. This enables LLMs to generate human-like text, understand and interpret natural language input, and perform a variety of tasks such as speech recognition, machine translation, and information retrieval. The advanced capabilities of LLMs in NLP have led to their widespread use in developing intelligent systems that can interact with users in a more human-like manner. However, one of the challenges in the development of LLMs is aligning their training objectives with human objectives, ensuring that the models not only generate responses but also follow instructions and perform tasks as expected.',\n",
              "   \"Training data for embedding models that utilize task-specific instructions is typically generated through a combination of existing datasets and the creation of new datasets specifically designed for this purpose. In the provided context, the INSTRUCTOR method is introduced, which combines 300 datasets from Super-NaturalInstructions (super-NI) and 30 datasets from other sources to form the Multitask Embeddings Data with Instructions (MEDI) collection. This diverse dataset collection enables the model to learn from a wide range of tasks and instructions. Another approach mentioned is to formulate various tasks as text-to-text problems, where the model learns to generate the desired output given a specific input and instruction. This allows for a more flexible and adaptable embedding model that can handle a broad range of use cases. Additionally, the use of asymmetric and symmetric tasks, as mentioned in the context, also plays a role in enhancing the model's performance when combined with instruction fine-tuning.\"],\n",
              "  'contexts': [['A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text',\n",
              "    'models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, handwriting recognition, grammar induction, and information retrieval.Large language models, currently their most advanced form, are a',\n",
              "    'at https://github.com/DaoD/INTERS.\\n1\\nIntroduction\\nLarge language models (LLMs) have shown re-\\nmarkable capabilities across various natural lan-\\nguage processing (NLP) tasks. While these models\\nhave learned vast knowledge from large text cor-\\npora, their (pre-)training objective is not aligned\\nwith human’s objective: the latter requires models\\nto “follow human instructions and perform tasks”',\n",
              "    '1\\nIntroduction\\nLarge language models (LMs) can be “prompted” to perform a range of natural language process-\\ning (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following'],\n",
              "   ['variety of tasks for embedding training with in-\\nstructions. We thus construct a collection of 330\\ndatasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,\\n2022b) with 30 datasets from existing collections',\n",
              "    'Abstract\\nWe introduce INSTRUCTOR, a new method\\nfor computing text embeddings given task in-\\nstructions: every text input is embedded to-\\ngether with instructions explaining the use case\\n(e.g., task and domain descriptions). Unlike\\nencoders from prior work that are more special-\\nized, INSTRUCTOR is a single embedder that\\ncan generate text embeddings tailored to differ-',\n",
              "    'finetuned embedding models. Given an input text\\nx and a task instruction Ix, INSTRUCTOR encodes\\ntheir concatenation Ix ⊕x. We then generate a\\nfixed-sized, task-specific embedding EI(Ix, x) by\\napplying mean pooling to the last hidden represen-\\ntations over the tokens in x.\\n2.2\\nTraining Objective\\nINSTRUCTOR is trained by formulating a wide\\nvariety of tasks as a text-to-text problem of distin-',\n",
              "    'ric data (see that the rightmost bar gets additive\\nperformance gains from the asymmetric and sym-\\nmetric tasks). This result demonstrates the impor-\\ntance of instruction finetuning when diverse data\\nare used for embedding training. Note that train-\\ning on symmetric tasks only without instructions\\nis similar to Sent-T5. Similarly, training on asym-']],\n",
              "  'sources': [['https://en.wikipedia.org/wiki/Large_language_model',\n",
              "    'https://en.wikipedia.org/wiki/Language_model',\n",
              "    'https://arxiv.org/pdf/2401.06532.pdf',\n",
              "    'https://arxiv.org/pdf/2203.02155.pdf'],\n",
              "   ['https://arxiv.org/pdf/2212.09741.pdf',\n",
              "    'https://arxiv.org/pdf/2212.09741.pdf',\n",
              "    'https://arxiv.org/pdf/2212.09741.pdf',\n",
              "    'https://arxiv.org/pdf/2212.09741.pdf']],\n",
              "  'ground_truth': ['Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.',\n",
              "   'To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.']}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3.4.6.d) Convert RAG outputs to abbreviated datasets and pandas dataframes to prepare for performance measurements/metric evaluations\n",
        "\n",
        "# PART 1: Create \"research\" and \"marketing\" RAG output datasets for validation question set\n",
        "# rag_output_v_research_mistral_mbt_abbrev = {key: rag_output_v_research_mistral_mbt[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "# rag_output_v_research_mistral_mbt_ds = Dataset.from_dict(rag_output_v_research_mistral_mbt_abbrev)\n",
        "rag_output_v_research_cohere_mbt_abbrev = {key: rag_output_v_research_cohere_mbt[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "rag_output_v_research_cohere_mbt_ds = Dataset.from_dict(rag_output_v_research_cohere_mbt_abbrev)\n",
        "# rag_output_v_marketing_mistral_mbt_abbrev = {key: rag_output_v_marketing_mistral_mbt[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "# rag_output_v_marketing_mistral_mbt_ds = Dataset.from_dict(rag_output_v_marketing_mistral_mbt_abbrev)\n",
        "rag_output_v_marketing_cohere_mbt_abbrev = {key: rag_output_v_marketing_cohere_mbt[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "rag_output_v_marketing_cohere_mbt_ds = Dataset.from_dict(rag_output_v_marketing_cohere_mbt_abbrev)\n",
        "\n",
        "# PART 2: Create \"research\" and \"marketing\" RAG output dataframes for validation question set\n",
        "# rag_output_v_research_mistral_mbt_df = Dataset.from_dict(rag_output_v_research_mistral_mbt).to_pandas()\n",
        "rag_output_v_research_cohere_mbt_df = Dataset.from_dict(rag_output_v_research_cohere_mbt).to_pandas()\n",
        "# rag_output_v_marketing_mistral_mbt_df = Dataset.from_dict(rag_output_v_marketing_mistral_mbt).to_pandas()\n",
        "rag_output_v_marketing_cohere_mbt_df = Dataset.from_dict(rag_output_v_marketing_cohere_mbt).to_pandas()\n",
        "\n",
        "# PART 3: Create \"research\" and \"marketing\" RAG output dataframes for testing question set\n",
        "# rag_output_t_research_mistral_mbt_df = Dataset.from_dict(rag_output_t_research_mistral_mbt).to_pandas()\n",
        "rag_output_t_research_cohere_mbt_df = Dataset.from_dict(rag_output_t_research_cohere_mbt).to_pandas()\n",
        "# rag_output_t_marketing_mistral_mbt_df = Dataset.from_dict(rag_output_t_marketing_mistral_mbt).to_pandas()\n",
        "rag_output_t_marketing_cohere_mbt_df = Dataset.from_dict(rag_output_t_marketing_cohere_mbt).to_pandas()"
      ],
      "metadata": {
        "id": "y3No8B_F0DO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJE1wexvV7ay"
      },
      "source": [
        "##4. POC RAG Tests & Evaluations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1 Evaluation Metrics**\n",
        "\n",
        "I've chosen the following metrics to assess the performance of the POC RAG system. Most of these metrics evaluate the *answer generation* aspect of the RAG system, while a few evaluate the *retrieval* aspect. Additionally, some of these metrics score RAG question responses based on token/word overlap with the provided \"gold\" (or ground truth) answers for those questions, while others score the responses based on semantic similarity to these \"gold\" answers. This allows for both word-for-word and thought-for-thought assessments (i.e., \"how close are RAG answers to 'gold' answers based strictly on choice of words\" versus \"how close are RAG answers to 'gold' answers based on overall meaning/gist\"). All metrics apply to both *research* (engineering) team answers and *marketing* team answers.\n",
        "\n",
        "-------------------------------------------------------------\n",
        "1. **Answer Length / Ratio to \"Gold\"**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Length of RAG-generated answers (across a batch of answers) as measured by number of characters; the average length is taken across tested answers, and the ratio of this average length to the average length of \"gold\" (ground truth) answers is computed\n",
        "* **`Purpose`**: Provides simple comparison of RAG answers to \"gold\" (ground truth) answers in terms of verbosity/succinctness\n",
        "* **`Outcome Range`**: Any positive number (e.g., 365.4, 180.3, etc.) for lengths, and a smaller positive number (e.g., 1.4, 0.8) for ratios\n",
        "* **`Goal`**: Where possible, minimize length differences between RAG-generated answers and \"gold\" (ground truth) answers and aim for a ratio close to 1.0\n",
        "\n",
        "-------------------------------------------------------------\n",
        "2. **BLEU**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Comparison of overlap in tokens/words between RAG-generated answers and \"gold\" (ground truth) answers; the average scores are taken across tested questions and answers\n",
        "  * *BLEU-1* compares unigrams (single tokens/words)\n",
        "  * *BLEU-2* compares bigrams (pairs of adjacent tokens/words)\n",
        "  * *BLEU-3* compares trigrams (sequences of three consecutive tokens/words)\n",
        "  * *BLEU-4* compares fourgrams (sequences of four consecutive tokens/words)\n",
        "* **`Purpose`**: Provides a measure of token/word correspondence between RAG answers and reference answers (i.e., do they generally contain the same words), used primarily to assess machine-generated translations\n",
        "* **`Outcome Range`**: Any number between 0 and 1 (higher is better)\n",
        "* **`Goal`**: Where possible, maximize BLEU score, especially BLEU-4 (standard version used in most translation tasks)\n",
        "\n",
        "-------------------------------------------------------------\n",
        "3. **ROUGE**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Comparison of overlap in tokens/words between RAG-generated answers and \"gold\" (ground truth) answers, accounting for both presence and absence of tokens/words; the average scores are taken across tested questions and answers\n",
        "  * *ROUGE-1* computes overlap of unigrams (single tokens/words)\n",
        "  * *ROUGE-2* computes overlap of bigrams (pairs of adjacent tokens/words)\n",
        "  * *ROUGE-L* computes the longest common subsequence (LCS), considering any length of n-grams\n",
        "  * *ROUGE-Lsum* computes the LCS (similar to ROUGE-L) but weights it by length\n",
        "* **`Purpose`**: Provides another measure of token/word correspondence between RAG answers and reference answers (i.e., do they generally contain the same words), used primarily to assess machine-generated summaries\n",
        "* **`Outcome Range`**: Any number between 0 and 1 (higher is better)\n",
        "* **`Goal`**: Where possible, maximize ROUGE score, especially ROUGE-2 (standard version used in most summary tasks)\n",
        "\n",
        "-------------------------------------------------------------\n",
        "4. **BERTScore**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Comparison of BERT-produced contextual embeddings for RAG-generated answers and \"gold\" (ground truth) answers to capture semantic similarity; the average scores are taken across tested questions and answers\n",
        "  * *Precision* measures the proportion of relevant tokens among those in the RAG-generated answers\n",
        "  * *Recall* measures the proportion of relevant tokens in the \"gold\" (ground truth) answers captured by the RAG-generated answers\n",
        "  * *F1* represents the harmonic mean of Precision and Recall, providing a balanced measure of both text capture and text retention in the RAG-generated answers compared to the \"gold\" (ground truth) answers\n",
        "* **`Purpose`**: Provides a measure of semantic similarity/equivalence between RAG answers and reference answers (i.e., do they generally mean the same thing)\n",
        "* **`Outcome Range`**: Any number between 0 and 1 (higher is better)\n",
        "* **`Goal`**: Where possible, maximize BERTScore components, especially F1 (harmonic mean of precision and recall)\n",
        "\n",
        "-------------------------------------------------------------\n",
        "5. **RAGAS**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Suite of several metrics evaluating RAG-generated answers and \"gold\" (ground truth) answers, as well as context relevance and retrieval; the average scores are taken across tested questions and answers\n",
        "  * ***RAG Retrieval Metrics***:\n",
        "    * *Context Precision* measures the signal-to-noise ratio of RAG-retrieved contexts (e.g., for each question, are the retrieved contexts relevant to the question)\n",
        "    * *Context Recall* measures the relevance of RAG-retrieved information (e.g., for each ground truth answer, are the retrieved contexts retaining info from the answer)\n",
        "  * ***RAG Generation Metrics***:\n",
        "    * *Faithfulness* measures the factual accuracy of RAG-generated answers (e.g., for each RAG answer, is there a close match to the ground truth answer)\n",
        "    * *Answer Relevancy* measures the relevance of RAG-generated answers (e.g., for each RAG answer and each question, does the answer capture what's being asked in the question)\n",
        "  * ***RAG End-to-End Metrics***:\n",
        "    * *Answer Correctness* measures overall alignment between RAG-generated answers and \"gold\" (ground truth) answers based on semantic similarity and factual similarity\n",
        "* **`Purpose`**: Provides an assessment of various components of a RAG system, including the quality of retrieved information (i.e., does the system return appropriate and relevant contexts in which to anchor its answers) and the quality of generated answers compared to ground truth answers (i.e., do they generally mean the same thing and capture the same information)\n",
        "* **`Outcome Range`**: Any number between 0 and 1 (higher is better)\n",
        "* **`Goal`**: Where possible, maximize RAG Retrieval, RAG Generation, and RAG End-to-End metrics, especially the RAG Generation metrics\n",
        "\n",
        "-------------------------------------------------------------\n",
        "6. **Pass Rate (Manual Review)**\n",
        "-------------------------------------------------------------\n",
        "* **`Definition`**: Eye-test/human judgment comparison of RAG-generated answers and \"gold\" (ground truth) answers; RAG answers are compared to \"gold\" answers and contexts and marked as \"passing\" or \"not passing\" based on relevance and similarity to come up with an overall rate of passing answers\n",
        "* **`Purpose`**: Provides a simple and intuitive measure of \"goodness\" of RAG-generated answers based on the overall similarity of RAG answers to \"gold\" (ground truth) answers and relevance to RAG-retrieved contexts\n",
        "* **`Outcome Range`**: Percentage between 0% and 100% (higher is better)\n",
        "* **`Goal`**: Where possible, maximize Pass Rate as a final test of reasonability in RAG answers\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxgArHeu0_oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.1) Create functions to define RAG evaluation metrics\n",
        "\n",
        "# METRIC 1: Response Length\n",
        "def rag_eval_response_length(rag_output_df):\n",
        "  \"\"\"Evaluates RAG answer response lengths based on a provided dataframe of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Args:\n",
        "  * rag_output_df = dataframe containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_response_lengths = dictionary of RAG answer and ground truth response lengths (measured in words)\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing response lengths for RAG-generated answers and ground truths\n",
        "  rag_answer_response_lengths = []\n",
        "  rag_ground_truth_response_lengths = []\n",
        "\n",
        "  # For each answer and ground truth in RAG output dataframe, calculate its length and append it to the appropriate list\n",
        "  for i in range(len(rag_output_df)):\n",
        "    rag_answer_response_length = len(rag_output_df['answer'][i])\n",
        "    rag_answer_response_lengths.append(rag_answer_response_length)\n",
        "    rag_ground_truth_response_length = len(rag_output_df['ground_truth'][i])\n",
        "    rag_ground_truth_response_lengths.append(rag_ground_truth_response_length)\n",
        "\n",
        "  # Convert response length lists to dictionary of lists\n",
        "  rag_response_lengths = {\n",
        "      \"RAG_Answer_Lengths\": rag_answer_response_lengths,\n",
        "      \"Gold_Answer_Lengths\": rag_ground_truth_response_lengths\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_response_lengths\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# METRIC 2: BLEU\n",
        "def rag_eval_bleu(rag_output_df):\n",
        "  \"\"\"Evaluates RAG answer BLEU scores based on a provided dataframe of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Args:\n",
        "  * rag_output_df = dataframe containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_BLEU_scores = dictionary of RAG answer BLEU scores (1-gram, 2-gram, 3-gram, 4-gram)\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing BLEU scores for RAG-generated answers\n",
        "  rag_BLEU_1g = []\n",
        "  rag_BLEU_2g = []\n",
        "  rag_BLEU_3g = []\n",
        "  rag_BLEU_4g = []\n",
        "\n",
        "  # For each answer in RAG output dataframe, calculate its BLEU scores and append them to the appropriate list\n",
        "  for i in range(len(rag_output_df)):\n",
        "    rag_answer_reference = rag_output_df['ground_truth'][i].lower()\n",
        "    rag_answer_candidate = rag_output_df['answer'][i].lower()\n",
        "    rag_BLEU_1g.append(corpus_bleu([[rag_answer_reference]], [rag_answer_candidate], weights=(1, 0, 0, 0)))\n",
        "    rag_BLEU_2g.append(corpus_bleu([[rag_answer_reference]], [rag_answer_candidate], weights=(0, 1, 0, 0)))\n",
        "    rag_BLEU_3g.append(corpus_bleu([[rag_answer_reference]], [rag_answer_candidate], weights=(0, 0, 1, 0)))\n",
        "    rag_BLEU_4g.append(corpus_bleu([[rag_answer_reference]], [rag_answer_candidate], weights=(0, 0, 0, 1)))\n",
        "\n",
        "  # Convert BLEU score lists to dictionary of lists\n",
        "  rag_BLEU_scores = {\n",
        "      \"BLEU-1\": rag_BLEU_1g,\n",
        "      \"BLEU-2\": rag_BLEU_2g,\n",
        "      \"BLEU-3\": rag_BLEU_3g,\n",
        "      \"BLEU-4\": rag_BLEU_4g\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_BLEU_scores\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# METRIC 3: ROUGE\n",
        "def rag_eval_rouge(rag_output_df):\n",
        "  \"\"\"Evaluates RAG answer ROUGE scores based on a provided dataframe of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Args:\n",
        "  * rag_output_df = dataframe containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_ROUGE_scores = dictionary of RAG answer ROUGE scores (1, 2, L, and Lsum)\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing ROUGE scores for RAG-generated answers\n",
        "  rag_ROUGE_1 = []\n",
        "  rag_ROUGE_2 = []\n",
        "  rag_ROUGE_L = []\n",
        "  rag_ROUGE_Lsum = []\n",
        "\n",
        "  # Load rouge scoring mechanism\n",
        "  rouge = evaluate.load('rouge')\n",
        "\n",
        "  # For each answer in RAG output dataframe, calculate its ROUGE scores and append them to the appropriate list\n",
        "  for i in range(len(rag_output_df)):\n",
        "    rag_answer_reference = rag_output_df['ground_truth'][i].lower()\n",
        "    rag_answer_prediction = rag_output_df['answer'][i].lower()\n",
        "    rag_ROUGE_results = rouge.compute(predictions=[rag_answer_prediction], references=[rag_answer_reference])\n",
        "    rag_ROUGE_1.append(rag_ROUGE_results['rouge1'])\n",
        "    rag_ROUGE_2.append(rag_ROUGE_results['rouge2'])\n",
        "    rag_ROUGE_L.append(rag_ROUGE_results['rougeL'])\n",
        "    rag_ROUGE_Lsum.append(rag_ROUGE_results['rougeLsum'])\n",
        "\n",
        "  # Convert ROUGE score lists to dictionary of lists\n",
        "  rag_ROUGE_scores = {\n",
        "      \"ROUGE-1\": rag_ROUGE_1,\n",
        "      \"ROUGE-2\": rag_ROUGE_2,\n",
        "      \"ROUGE-L\": rag_ROUGE_L,\n",
        "      \"ROUGE-Lsum\": rag_ROUGE_Lsum\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_ROUGE_scores\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# METRIC 4: BERTScore\n",
        "def rag_eval_bertscore(rag_output_df):\n",
        "  \"\"\"Evaluates RAG answer BERTScore metrics based on a provided dataframe of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Args:\n",
        "  * rag_output_df = dataframe containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_BERTScore_metrics = dictionary of RAG answer BERTScore metrics (precision, recall, and F1)\n",
        "  \"\"\"\n",
        "\n",
        "  # Define empty lists for storing BERTScore metrics for RAG-generated answers\n",
        "  rag_BERTScore_precision = []\n",
        "  rag_BERTScore_recall = []\n",
        "  rag_BERTScore_F1 = []\n",
        "\n",
        "  # Load BERTScore scoring mechanism and model (standard BERT model)\n",
        "  scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "\n",
        "  # For each answer in RAG output dataframe, calculate its BERTScore metrics and append them to the appropriate list\n",
        "  for i in range(len(rag_output_df)):\n",
        "    rag_answer_reference = rag_output_df['ground_truth'][i].lower()\n",
        "    rag_answer_prediction = rag_output_df['answer'][i].lower()\n",
        "    rag_BERTScore_result = scorer.score([rag_answer_prediction], [rag_answer_reference])\n",
        "    rag_BERTScore_precision.append(rag_BERTScore_result[0])\n",
        "    rag_BERTScore_recall.append(rag_BERTScore_result[1])\n",
        "    rag_BERTScore_F1.append(rag_BERTScore_result[2])\n",
        "\n",
        "  # Convert BERTScore metric lists to dictionary of lists\n",
        "  rag_BERTScore_metrics_raw = {\n",
        "      \"BERTScore-Precision\": rag_BERTScore_precision,\n",
        "      \"BERTScore-Recall\": rag_BERTScore_recall,\n",
        "      \"BERTScore-F1\": rag_BERTScore_F1\n",
        "  }\n",
        "\n",
        "  # Fix output to remove tensor components\n",
        "  rag_BERTScore_metrics = {\n",
        "      key: [value_item.item() for value_item in value] for key, value in rag_BERTScore_metrics_raw.items()\n",
        "  }\n",
        "\n",
        "  # Save output\n",
        "  return rag_BERTScore_metrics\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# METRIC 5: RAGAS\n",
        "def rag_eval_ragas(rag_output_ds):\n",
        "  \"\"\"Evaluates RAG system RAGAS metrics based on a provided dataset of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Args:\n",
        "  * rag_output_ds = dataset containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_RAGAS_metrics_df = dataframe of RAGAS metrics (context precision, context recall, faithfulness, answer relevancy, answer correctness)\n",
        "  \"\"\"\n",
        "\n",
        "  # Get OpenAI API key for RAGAS evaluations\n",
        "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "  # Calculate results for defined list of RAGAS metrics\n",
        "  RAGAS_results = ragas_evaluate(\n",
        "      dataset=rag_output_ds,\n",
        "      metrics=[\n",
        "          context_precision,\n",
        "          context_recall,\n",
        "          faithfulness,\n",
        "          answer_relevancy,\n",
        "          answer_correctness]\n",
        "      )\n",
        "\n",
        "  # Convert RAGAS results output to pandas dataframe\n",
        "  rag_RAGAS_metrics_df = RAGAS_results.to_pandas()\n",
        "\n",
        "  # Save output\n",
        "  return rag_RAGAS_metrics_df\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# METRIC 6: Pass Rate (Manual Review)\n",
        "# No function - answer reviews are manually performed"
      ],
      "metadata": {
        "id": "PVHDOvzY3-jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKktTilyTy_z"
      },
      "source": [
        "###4.2. Evaluation Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.a) Create functions to score RAG system output and append scores to output, calculate mean scores, and plot scores\n",
        "\n",
        "# Compute metric scores (except manual review metric) and combine with RAG output\n",
        "def rag_score_combine(rag_output_df, rag_output_ds):\n",
        "  \"\"\"Scores and appends metrics to RAG system output based on a provided dataframe of RAG outputs\n",
        "  (e.g., questions, answers, ground truths, etc.) and a similar dataset of RAG outputs\n",
        "\n",
        "  Args:\n",
        "  * rag_output_df = dataframe containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "  * rag_output_ds = dataset containing RAG outputs (e.g., questions, answers, ground truths, etc.)\n",
        "\n",
        "  Returns:\n",
        "  * rag_scored_output_df = dataframe of RAG output and metric scores\n",
        "  \"\"\"\n",
        "\n",
        "  # Score RAG system output data\n",
        "  response_length_scores = rag_eval_response_length(rag_output_df)\n",
        "  BLEU_scores = rag_eval_bleu(rag_output_df)\n",
        "  ROUGE_scores = rag_eval_rouge(rag_output_df)\n",
        "  BERTScore_scores = rag_eval_bertscore(rag_output_df)\n",
        "  RAGAS_scores = rag_eval_ragas(rag_output_ds)\n",
        "\n",
        "  # Convert metric results to dataframes\n",
        "  response_length_scores_df = pd.DataFrame(response_length_scores)\n",
        "  BLEU_scores_df = pd.DataFrame(BLEU_scores)\n",
        "  ROUGE_scores_df = pd.DataFrame(ROUGE_scores)\n",
        "  BERTScore_scores_df = pd.DataFrame(BERTScore_scores)\n",
        "  RAGAS_scores_df = RAGAS_scores.drop(['question', 'answer', 'contexts', 'ground_truth'], axis=1)\n",
        "\n",
        "  # Concatenate RAG output data and metrics\n",
        "  rag_scored_output_df = pd.concat(\n",
        "    [\n",
        "        rag_output_df,\n",
        "        response_length_scores_df,\n",
        "        BLEU_scores_df,\n",
        "        ROUGE_scores_df,\n",
        "        BERTScore_scores_df,\n",
        "        RAGAS_scores_df\n",
        "    ], axis=1)\n",
        "\n",
        "  # Replace any NaN values with zeros\n",
        "  rag_scored_output_df = rag_scored_output_df.fillna(0)\n",
        "\n",
        "  # Save output\n",
        "  return rag_scored_output_df\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Calculate mean metric scores for plot preparation\n",
        "def rag_score_means(rag_scored_output_df):\n",
        "  \"\"\"Computes the means of each numeric metric in RAG scored output for use in visualizations\n",
        "\n",
        "  Args:\n",
        "  * rag_scored_output_df = dataframe of RAG output and metric scores\n",
        "\n",
        "  Returns:\n",
        "  * rag_mean_scores = dictionary of RAG mean metric scores\n",
        "  \"\"\"\n",
        "\n",
        "  # Define RAG metric columns\n",
        "  RAG_metric_columns = [\n",
        "      'RAG_Answer_Lengths',\n",
        "      'Gold_Answer_Lengths',\n",
        "      'BLEU-1',\n",
        "      'BLEU-2',\n",
        "      'BLEU-3',\n",
        "      'BLEU-4',\n",
        "      'ROUGE-1',\n",
        "      'ROUGE-2',\n",
        "      'ROUGE-L',\n",
        "      'ROUGE-Lsum',\n",
        "      'BERTScore-Precision',\n",
        "      'BERTScore-Recall',\n",
        "      'BERTScore-F1',\n",
        "      'context_precision',\n",
        "      'context_recall',\n",
        "      'faithfulness',\n",
        "      'answer_relevancy',\n",
        "      'answer_correctness'\n",
        "  ]\n",
        "\n",
        "  # Compute means across all RAG metric columns\n",
        "  rag_mean_scores = rag_scored_output_df[RAG_metric_columns].mean().to_dict()\n",
        "\n",
        "  # Compute value of new RAG_Gold_Answer_Lengths_Ratio column\n",
        "  if rag_mean_scores['Gold_Answer_Lengths'] == 0:\n",
        "    RAG_Gold_Answer_Lengths_Ratio = 0\n",
        "  else:\n",
        "    RAG_Gold_Answer_Lengths_Ratio = rag_mean_scores['RAG_Answer_Lengths'] / rag_mean_scores['Gold_Answer_Lengths']\n",
        "\n",
        "  # Add new RAG_Gold_Answer_Lengths_Ratio column and value\n",
        "  rag_mean_scores['RAG_Gold_Answer_Lengths_Ratio'] = RAG_Gold_Answer_Lengths_Ratio\n",
        "\n",
        "  # Show resulting means output\n",
        "  return rag_mean_scores\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Plot mean metric scores on a scatter polar plot\n",
        "def rag_score_polar_plot(rag_mean_scores_research, rag_mean_scores_marketing):\n",
        "  \"\"\"Plots RAG system metrics on a polar plot based on provided dataframes with RAG\n",
        "  scored outputs for research and marketing personas\n",
        "\n",
        "  Args:\n",
        "  * rag_mean_scores_research = dictionary containing RAG mean scores for research persona\n",
        "  * rag_mean_scores_marketing = dictionary containing RAG mean scores for marketing persona\n",
        "\n",
        "  Returns:\n",
        "  * rag_polar_plot = polar plot visualizing RAG system metrics\n",
        "  \"\"\"\n",
        "\n",
        "  # Create data for polar plot to visualize RAG performance metrics\n",
        "  rag_plot_data_research = {\n",
        "      key: value for key, value in rag_mean_scores_research.items() if key not in ['RAG_Answer_Lengths', 'Gold_Answer_Lengths']\n",
        "  }\n",
        "  rag_plot_data_marketing = {\n",
        "      key: value for key, value in rag_mean_scores_marketing.items() if key not in ['RAG_Answer_Lengths', 'Gold_Answer_Lengths']\n",
        "  }\n",
        "\n",
        "  # Define and add datasets to polar plot figure\n",
        "  rag_pp_fig = go.Figure()\n",
        "  rag_pp_fig.add_trace(go.Scatterpolar(\n",
        "      r=list(rag_plot_data_research.values()),\n",
        "      theta=list(rag_plot_data_research.keys()),\n",
        "      fill='toself',\n",
        "      name='Research'\n",
        "  ))\n",
        "  rag_pp_fig.add_trace(go.Scatterpolar(\n",
        "      r=list(rag_plot_data_marketing.values()),\n",
        "      theta=list(rag_plot_data_marketing.keys()),\n",
        "      fill='toself',\n",
        "      name='Marketing'\n",
        "  ))\n",
        "  rag_pp_fig.add_trace(go.Scatterpolar(\n",
        "      r=[1.0] * len(rag_plot_data_research.values()),\n",
        "      theta=list(rag_plot_data_research.keys()),\n",
        "      mode='lines',\n",
        "      line=dict(color='black'),\n",
        "      showlegend=False\n",
        "  ))\n",
        "\n",
        "  # Generate polar plot\n",
        "  rag_pp_fig.update_layout(\n",
        "      polar=dict(\n",
        "          radialaxis=dict(\n",
        "              visible=True,\n",
        "              range=[0, 2]\n",
        "          )),\n",
        "      showlegend=True,\n",
        "      title='RAG System Output Evaluation (Mean Scores)',\n",
        "      width=800\n",
        "  )\n",
        "\n",
        "  # Save output\n",
        "  return rag_pp_fig"
      ],
      "metadata": {
        "id": "Vpoe1U2UIGzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2.1 Mini-Batch Test (MBT) Results\n",
        "\n",
        "Run a small batch of validation and test questions through the RAG system to evaluate RAG pipeline and scoring."
      ],
      "metadata": {
        "id": "vIs8RjLf2PIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.b) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Score RAG system output\n",
        "# RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_MBT = rag_score_combine(\n",
        "#     rag_output_v_research_mistral_mbt_df,\n",
        "#     rag_output_v_research_mistral_mbt_ds\n",
        "# )\n",
        "RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT = rag_score_combine(\n",
        "    rag_output_v_research_cohere_mbt_df,\n",
        "    rag_output_v_research_cohere_mbt_ds\n",
        ")\n",
        "\n",
        "# Show resulting scored output\n",
        "print('RAG System Scored Output - Mini-Batch Test (Research):\\n')\n",
        "# display(RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_MBT)\n",
        "display(RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT)"
      ],
      "metadata": {
        "id": "mg5bQgtwkrSb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616,
          "referenced_widgets": [
            "3d94b8911b2e4f3b891db37156573473",
            "4a54d472f2f948a681dd7b32c9362957",
            "e7a1897292d047c2aa155785401b609f",
            "9f29ad4ef9074c57a683df349460b722",
            "98b300b5fbb746cca35009e8f45dec32",
            "19de87c0fbe7429f867ae8e0341d774c",
            "65fa7772833442c3b64063508eba0999",
            "f42dbc6e8af94927849690314986447b",
            "abe69d8bef0c4da884b9b194dd719d3f",
            "14329d4468a64f90bc095456108853d8",
            "2ef5414753f048d8886f766ffdf6e375",
            "37dd310d794f4a76927bcb1bf2cc910a",
            "de4ebe4452cd495082c3bae5feef2991",
            "bbd5e59b13c04a3f9b27e3ba5ea08829",
            "8f05dea707ae42a999ea7bdcdd006a60",
            "75477af511f94bf59069ab6d956c07f3",
            "b80546491a0e47979edb7157486c01c0",
            "f9074995b028487fb338f25d92cca0df",
            "52164085387548199943e0c3dc8bd9be",
            "95a769b339c44921b642176442e82aa8",
            "3a9cf6ea82c849018c989e7f765a8791",
            "41eb39f02b0d41afbd74f7177b6fe5be",
            "35dfe6a5bdca4df2a90f47fbb0f7b8e4",
            "a67429625f914562a9542dd841063fa6",
            "1cb607c160224975a525f71c4effd4cb",
            "dd736a5fc15c48799f1f4e12d979b5af",
            "d86b6b6158114844a512afcd20e05684",
            "88fc71e8cf484dceb140d497402e8476",
            "34a0afa569a041bc936792c9f67ab187",
            "a286a451acba4548be2b8ec51c13f17e",
            "cdd0b9ec6e29492082bcb39b3114fe65",
            "ff2658d1bd134fdb9ac8245cd4a29853",
            "a218bb8b77b945a8bef60c83d65d85a4",
            "f8627c3f5e0b4c9287913fa88d284f5b",
            "43d4264c79444d528902a36e49f756b5",
            "b577383603df4e36ad6dfca39765766c",
            "dd2be183ad8c475b9db7a3e9d1de7196",
            "74ac000804874e92ae36e54e2b0d3eff",
            "6687d793a7fc4bcb893dce81c60b82d7",
            "b0692f68e9c7411889d6e4b6e6b3d370",
            "0fc39ab5a42b4fbd94ef96956507f000",
            "3d8ef0b7c26f4c77a133da93dd5846b0",
            "bdaf422342f94d3cb3c15be8b8f2a9e8",
            "2c67f454438a4c52894324684d833601",
            "f5786f812fb74f3ea70fb2c409e9136f",
            "95c88f1c67f94b97be4d14d3aef1c9c6",
            "a1bdc5b5e6584073bd8f9add20a18ff5",
            "731aefe487674ceda8211858c1d86a6f",
            "7c056b4998b945e4a2dddf027876bc1f",
            "b777e3e786a4401eaa49b94e2a2b598b",
            "4e6983554fa8402d91ea5cb550884141",
            "79b20cc45042499aa81390d9d53aab88",
            "0e971edc49194241954025fcb50fe2b8",
            "b31faa9c980443e3a91f50b2e4f7dea8",
            "f747fc5ad53a44aebe3d21c3f6cbcb22",
            "168ff720a5214612849f93b9dd08cb71",
            "46c47397366948de960c6c0730a4fa80",
            "a537e75bf1a44353857da086cd49ab5d",
            "d396f8bedd9e4c93ae8616c34e43d013",
            "b81e3d4c9ac04261b6f41879eefdf648",
            "ae857bd9e4fa4502960be177e025b514",
            "fda0fcc6d86e4fef8ba354184926a0f2",
            "6e45ec7c251245158984d317f7c46098",
            "e419d38fd9b84d0fb822822328d5249f",
            "6ac20e673a8640e484df598c0a5b86da",
            "ae48b99ab2b040a69063aeb200235473",
            "74337a6316864621a500b9ab26d6d8dd",
            "af46ace5a79a480aa39a934e2603b24e",
            "23b812957476458ca2b6f8f9106a5dcd",
            "95db2722ff71474f8503fe955144b7a0",
            "0d9840872a5045e9bebb86cd6c1cd1ff",
            "d6073362a88a464a97a04fd3c4d6bb82",
            "8ad119222faf430798ce5eda0828d05f",
            "f756c939f28942f787fe917471693aee",
            "a58dd7f79a6b4a54a9842065d963040d",
            "a99140a00a2047558cff420176fcb41f",
            "4bcf47607b38493d8c0eff4760cf9644"
          ]
        },
        "outputId": "f6e3da10-70e6-45e1-f44f-879e6250f020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d94b8911b2e4f3b891db37156573473"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37dd310d794f4a76927bcb1bf2cc910a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35dfe6a5bdca4df2a90f47fbb0f7b8e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8627c3f5e0b4c9287913fa88d284f5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5786f812fb74f3ea70fb2c409e9136f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "168ff720a5214612849f93b9dd08cb71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74337a6316864621a500b9ab26d6d8dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Scored Output - Mini-Batch Test (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) have become integ...   \n",
              "1  Training data for embedding models that utiliz...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models (LLMs) serve the purpose...                 872   \n",
              "1  To create training data for embedding models t...                1027   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "0                  639  0.723624  0.603904  0.435632  0.323360  0.387097   \n",
              "1                  738  0.715677  0.611111  0.461463  0.378906  0.426357   \n",
              "\n",
              "   ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.15814  0.248848    0.248848             0.638069          0.686313   \n",
              "1  0.18750  0.263566    0.263566             0.658139          0.715702   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.661313                1.0             1.0           0.8   \n",
              "1      0.685715                1.0             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.921056            0.573880  \n",
              "1          0.969059            0.733687  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3679f19-11d0-4b59-bfad-52c6e478d34c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) have become integ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models (LLMs) serve the purpose...</td>\n",
              "      <td>872</td>\n",
              "      <td>639</td>\n",
              "      <td>0.723624</td>\n",
              "      <td>0.603904</td>\n",
              "      <td>0.435632</td>\n",
              "      <td>0.323360</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.15814</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.638069</td>\n",
              "      <td>0.686313</td>\n",
              "      <td>0.661313</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.921056</td>\n",
              "      <td>0.573880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that utiliz...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>To create training data for embedding models t...</td>\n",
              "      <td>1027</td>\n",
              "      <td>738</td>\n",
              "      <td>0.715677</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.461463</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.426357</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.263566</td>\n",
              "      <td>0.263566</td>\n",
              "      <td>0.658139</td>\n",
              "      <td>0.715702</td>\n",
              "      <td>0.685715</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.969059</td>\n",
              "      <td>0.733687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3679f19-11d0-4b59-bfad-52c6e478d34c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3679f19-11d0-4b59-bfad-52c6e478d34c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3679f19-11d0-4b59-bfad-52c6e478d34c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-289b2c29-c89f-4cfe-90b3-3606a52b2c97\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-289b2c29-c89f-4cfe-90b3-3606a52b2c97')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-289b2c29-c89f-4cfe-90b3-3606a52b2c97 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f6f0da92-5a25-4e26-b3a5-491b332f74e6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f6f0da92-5a25-4e26-b3a5-491b332f74e6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.c) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Compute mean scores for RAG system output\n",
        "# RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_MBT = rag_score_means(RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_MBT)\n",
        "RAG_V_RESEARCH_COHERE_MEAN_SCORES_MBT = rag_score_means(RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT)\n",
        "\n",
        "# Show resulting means output\n",
        "print('RAG Metric Mean Scores - Mini-Batch Test (Research):\\n')\n",
        "# display(RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_MBT)\n",
        "display(RAG_V_RESEARCH_COHERE_MEAN_SCORES_MBT)"
      ],
      "metadata": {
        "id": "ud-bLuJGku_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "dfb9a55b-2951-4a53-b627-004b58b658fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Mean Scores - Mini-Batch Test (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'RAG_Answer_Lengths': 949.5,\n",
              " 'Gold_Answer_Lengths': 688.5,\n",
              " 'BLEU-1': 0.7196502907729827,\n",
              " 'BLEU-2': 0.6075073351192755,\n",
              " 'BLEU-3': 0.44854779927109617,\n",
              " 'BLEU-4': 0.3511332170598389,\n",
              " 'ROUGE-1': 0.4067266816704176,\n",
              " 'ROUGE-2': 0.17281976744186048,\n",
              " 'ROUGE-L': 0.2562069088700747,\n",
              " 'ROUGE-Lsum': 0.2562069088700747,\n",
              " 'BERTScore-Precision': 0.6481043696403503,\n",
              " 'BERTScore-Recall': 0.7010077238082886,\n",
              " 'BERTScore-F1': 0.6735137403011322,\n",
              " 'context_precision': 0.999999999975,\n",
              " 'context_recall': 1.0,\n",
              " 'faithfulness': 0.9,\n",
              " 'answer_relevancy': 0.9450578064304669,\n",
              " 'answer_correctness': 0.6537833901500196,\n",
              " 'RAG_Gold_Answer_Lengths_Ratio': 1.3790849673202614}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.d) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Score RAG system output\n",
        "# RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_MBT = rag_score_combine(\n",
        "#     rag_output_v_marketing_mistral_mbt_df,\n",
        "#     rag_output_v_marketing_mistral_mbt_ds\n",
        "# )\n",
        "RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT = rag_score_combine(\n",
        "    rag_output_v_marketing_cohere_mbt_df,\n",
        "    rag_output_v_marketing_cohere_mbt_ds\n",
        ")\n",
        "\n",
        "# Show resulting scored output\n",
        "print('RAG System Scored Output - Mini-Batch Test (Marketing):\\n')\n",
        "# display(RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_MBT)\n",
        "display(RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT)"
      ],
      "metadata": {
        "id": "0kdztTq8lLRu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "4f8c3479c54f43218a60f2775fdf2fa4",
            "b8140201ee714214be6f1778793c42e9",
            "143005359db245399d95dcab5053698c",
            "a26c9f23fd9b42c4a7881543ae8cd9e4",
            "00ee819db4a84a9eaeea71439b04f892",
            "bd529d3deb614f56a3b6a2043b4b4299",
            "5e15639388f744d2a9d2cd066dd50e22",
            "52a82c9cef9a497693ae57e3679b9a0c",
            "72231c8bec47467b939c1e744b496ea7",
            "603e30b730744102adf29fc4acaeb3a2",
            "989add017ba8483b918aa0e9c63c0ba7"
          ]
        },
        "outputId": "141c700a-a085-4187-ab89-106c295b5c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f8c3479c54f43218a60f2775fdf2fa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Scored Output - Mini-Batch Test (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) are an essential ...   \n",
              "1  Training data for embedding models that use ta...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models serve the purpose of imp...                 485   \n",
              "1  Training data for embedding models that use ta...                 334   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "0                  290  0.589691  0.473140  0.289855  0.224066  0.264151   \n",
              "1                  348  0.852719  0.659458  0.534355  0.480924  0.560748   \n",
              "\n",
              "    ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.076923  0.207547    0.207547             0.546218          0.683818   \n",
              "1  0.400000  0.504673    0.504673             0.771538          0.775630   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.607321           1.000000             1.0           1.0   \n",
              "1      0.773579           0.805556             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.937324            0.531020  \n",
              "1          0.966024            0.492167  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09999f33-e056-4638-a4ed-c2c81c645ed9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) are an essential ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models serve the purpose of imp...</td>\n",
              "      <td>485</td>\n",
              "      <td>290</td>\n",
              "      <td>0.589691</td>\n",
              "      <td>0.473140</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.224066</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.546218</td>\n",
              "      <td>0.683818</td>\n",
              "      <td>0.607321</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.937324</td>\n",
              "      <td>0.531020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>334</td>\n",
              "      <td>348</td>\n",
              "      <td>0.852719</td>\n",
              "      <td>0.659458</td>\n",
              "      <td>0.534355</td>\n",
              "      <td>0.480924</td>\n",
              "      <td>0.560748</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.771538</td>\n",
              "      <td>0.775630</td>\n",
              "      <td>0.773579</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.966024</td>\n",
              "      <td>0.492167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09999f33-e056-4638-a4ed-c2c81c645ed9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09999f33-e056-4638-a4ed-c2c81c645ed9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09999f33-e056-4638-a4ed-c2c81c645ed9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7db44810-f532-4656-b473-ff55955c7925\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7db44810-f532-4656-b473-ff55955c7925')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7db44810-f532-4656-b473-ff55955c7925 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_eb703620-4188-4185-887e-b5d414724fee\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eb703620-4188-4185-887e-b5d414724fee button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.e) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Compute mean scores for RAG system output\n",
        "# RAG_V_MARKETING_MISTRAL_MEAN_SCORES_MBT = rag_score_means(RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_MBT)\n",
        "RAG_V_MARKETING_COHERE_MEAN_SCORES_MBT = rag_score_means(RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT)\n",
        "\n",
        "# Show resulting means output\n",
        "print('RAG Metric Mean Scores - Mini-Batch Test (Marketing):\\n')\n",
        "# display(RAG_V_MARKETING_MISTRAL_MEAN_SCORES_MBT)\n",
        "display(RAG_V_MARKETING_COHERE_MEAN_SCORES_MBT)"
      ],
      "metadata": {
        "id": "j4MSTRTYlLYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "8c580d97-ba41-4036-f133-997b5b71c250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Mean Scores - Mini-Batch Test (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'RAG_Answer_Lengths': 409.5,\n",
              " 'Gold_Answer_Lengths': 319.0,\n",
              " 'BLEU-1': 0.7212049416118438,\n",
              " 'BLEU-2': 0.5662993598344945,\n",
              " 'BLEU-3': 0.41210491744192634,\n",
              " 'BLEU-4': 0.35249501968389463,\n",
              " 'ROUGE-1': 0.41244930347381414,\n",
              " 'ROUGE-2': 0.23846153846153845,\n",
              " 'ROUGE-L': 0.3561100335037912,\n",
              " 'ROUGE-Lsum': 0.3561100335037912,\n",
              " 'BERTScore-Precision': 0.6588780879974365,\n",
              " 'BERTScore-Recall': 0.7297240495681763,\n",
              " 'BERTScore-F1': 0.6904501616954803,\n",
              " 'context_precision': 0.9027777777476851,\n",
              " 'context_recall': 1.0,\n",
              " 'faithfulness': 1.0,\n",
              " 'answer_relevancy': 0.9516741478175019,\n",
              " 'answer_correctness': 0.511593322223689,\n",
              " 'RAG_Gold_Answer_Lengths_Ratio': 1.2836990595611286}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.f) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Both\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Plot mean scores for RAG system output\n",
        "# RAG_V_MISTRAL_PP_FIG_MBT = rag_score_polar_plot(\n",
        "#     RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_MBT,\n",
        "#     RAG_V_MARKETING_MISTRAL_MEAN_SCORES_MBT\n",
        "# )\n",
        "RAG_V_COHERE_PP_FIG_MBT = rag_score_polar_plot(\n",
        "    RAG_V_RESEARCH_COHERE_MEAN_SCORES_MBT,\n",
        "    RAG_V_MARKETING_COHERE_MEAN_SCORES_MBT\n",
        ")\n",
        "\n",
        "# Display polar plot\n",
        "print('RAG Metric Polar Plot - Mini-Batch Test (Research & Marketing):\\n')\n",
        "# RAG_V_MISTRAL_PP_FIG_MBT.show()\n",
        "RAG_V_COHERE_PP_FIG_MBT.show()"
      ],
      "metadata": {
        "id": "zGH29aZGlLfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "202e9594-bccc-41a4-ced5-c13e5de4795e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Polar Plot - Mini-Batch Test (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"922dbfbc-ee58-4b8b-991d-a5986e21a1b6\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"922dbfbc-ee58-4b8b-991d-a5986e21a1b6\")) {                    Plotly.newPlot(                        \"922dbfbc-ee58-4b8b-991d-a5986e21a1b6\",                        [{\"fill\":\"toself\",\"name\":\"Research\",\"r\":[0.7196502907729827,0.6075073351192755,0.44854779927109617,0.3511332170598389,0.4067266816704176,0.17281976744186048,0.2562069088700747,0.2562069088700747,0.6481043696403503,0.7010077238082886,0.6735137403011322,0.999999999975,1.0,0.9,0.9450578064304669,0.6537833901500196,1.3790849673202614],\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"name\":\"Marketing\",\"r\":[0.7212049416118438,0.5662993598344945,0.41210491744192634,0.35249501968389463,0.41244930347381414,0.23846153846153845,0.3561100335037912,0.3561100335037912,0.6588780879974365,0.7297240495681763,0.6904501616954803,0.9027777777476851,1.0,1.0,0.9516741478175019,0.511593322223689,1.2836990595611286],\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"},{\"line\":{\"color\":\"black\"},\"mode\":\"lines\",\"r\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"showlegend\":false,\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"radialaxis\":{\"visible\":true,\"range\":[0,2]}},\"showlegend\":true,\"title\":{\"text\":\"RAG System Output Evaluation (Mean Scores)\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('922dbfbc-ee58-4b8b-991d-a5986e21a1b6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.g) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_RESEARCH_MISTRAL_OUTPUT_MBT = rag_output_t_research_mistral_mbt_df\n",
        "RAG_T_RESEARCH_COHERE_OUTPUT_MBT = rag_output_t_research_cohere_mbt_df\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Mini-Batch Test (Research):\\n')\n",
        "# display(RAG_T_RESEARCH_MISTRAL_OUTPUT_MBT)\n",
        "display(RAG_T_RESEARCH_COHERE_OUTPUT_MBT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ZKHJSH6sQOBA",
        "outputId": "3e3be42f-bf17-4d6f-b209-9489f168b0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Mini-Batch Test (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03e4dd6e-4700-425c-9f27-69058b143b2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03e4dd6e-4700-425c-9f27-69058b143b2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03e4dd6e-4700-425c-9f27-69058b143b2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03e4dd6e-4700-425c-9f27-69058b143b2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_c0058de1-909c-477c-bbdb-0640454b0846\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_research_cohere_mbt_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c0058de1-909c-477c-bbdb-0640454b0846 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_research_cohere_mbt_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_research_cohere_mbt_df",
              "summary": "{\n  \"name\": \"rag_output_t_research_cohere_mbt_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly influenced by the diversity and breadth of questions it encounters during its training phase. If a model is exposed to a wide range of question types, it can develop a more robust understanding of the task and perform better when faced with novel queries. This is because the model learns to generalize from the varied examples it has seen, enabling it to extrapolate and apply its knowledge to new situations. However, if a model primarily learns from a narrow set of question types, its performance may be limited to those specific patterns, struggling with questions that deviate from its training data. Thus, it is crucial to provide a comprehensive training dataset that covers different question categories to ensure the model can effectively answer a broader spectrum of queries.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.h) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_MARKETING_MISTRAL_OUTPUT_MBT = rag_output_t_marketing_mistral_mbt_df\n",
        "RAG_T_MARKETING_COHERE_OUTPUT_MBT = rag_output_t_marketing_cohere_mbt_df\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Mini-Batch Test (Marketing):\\n')\n",
        "# display(RAG_T_MARKETING_MISTRAL_OUTPUT_MBT)\n",
        "display(RAG_T_MARKETING_COHERE_OUTPUT_MBT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "COgcOKJRQORM",
        "outputId": "1b37524e-c8b2-4f82-c0dc-bb334489a2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Mini-Batch Test (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0bb479b-7a5e-4f55-8aa6-7f8768054329\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0bb479b-7a5e-4f55-8aa6-7f8768054329')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d0bb479b-7a5e-4f55-8aa6-7f8768054329 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d0bb479b-7a5e-4f55-8aa6-7f8768054329');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_a89c346f-b702-40aa-a683-55a9afe9a052\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_marketing_cohere_mbt_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a89c346f-b702-40aa-a683-55a9afe9a052 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_marketing_cohere_mbt_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_marketing_cohere_mbt_df",
              "summary": "{\n  \"name\": \"rag_output_t_marketing_cohere_mbt_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly related to its exposure to specific question types during training. The more diverse and comprehensive the training data, the better equipped the model is to handle novel questions and provide accurate responses, even for unseen answers.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2.2 Formal Test Runs and Results\n",
        "\n",
        "This section comprises RAG experimental trials and involves running a larger batch of \"validation\" and \"test\" questions through the RAG system to evaluate RAG metrics using different parameters.\n",
        "\n",
        "**Note: RAG parameters – including chunk size, embeddings, LLM features, etc. – are evaluated and adjusted with each testing iteration, resulting in different, iteratively-named RAG output data files. Meanwhile, the input questions tested within the following code blocks remain constant across testing iterations. Results for each iteration/run are exported to a .xlsx file for tracking and reference.*"
      ],
      "metadata": {
        "id": "RF3QxbVc2jh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.a) Test RAG system on larger batch of validation question/answer pairs for analysis and fine-tuning\n",
        "\n",
        "# REFERENCE: Count of validation questions and test questions\n",
        "# * Validation = 75\n",
        "# ** Question IDs:\n",
        "#   [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27,\n",
        "#    28, 30, 33, 34, 35, 36, 38, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52,\n",
        "#    54, 55, 59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 73, 74, 75, 76, 78, 80,\n",
        "#    81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 101, 102, 103,\n",
        "#    104]\n",
        "# * Test = 29\n",
        "# ** Question IDs:\n",
        "#   [4, 5, 6, 10, 14, 15, 21, 26, 29, 31, 32, 37, 40, 42, 49, 56, 57, 58, 66,\n",
        "#    68, 71, 72, 77, 79, 83, 90, 98, 99, 100]\n",
        "\n",
        "# PART 1: Define validation question IDs to run through RAG system (use 15 to capture ~20% of validation data)\n",
        "v_q_ids = [\n",
        "    0,    # 5.2.1 Test Question 1\n",
        "    # 1,\n",
        "    # 7,\n",
        "    # 22,\n",
        "    # 33,\n",
        "    50,   # 5.2.2 Test Question 2\n",
        "    # 62,\n",
        "    # 63,\n",
        "    # 76,\n",
        "    # 80,\n",
        "    # 91,\n",
        "    # 95,\n",
        "    # 96,\n",
        "    # 101,\n",
        "    # 102,\n",
        "]\n",
        "\n",
        "###########################################################################################\n",
        "# NOTE: This section groups the validation question IDs into sets of 15 for chunked runs\n",
        "# v_q_ids = [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20]             # Chunk01\n",
        "# v_q_ids = [22, 23, 24, 25, 27, 28, 30, 33, 34, 35, 36, 38, 39, 41, 43]      # Chunk02\n",
        "# v_q_ids = [44, 45, 46, 47, 48, 50, 51, 52, 54, 55, 59, 60, 61, 62, 63]      # Chunk03\n",
        "# v_q_ids = [64, 65, 67, 69, 70, 73, 74, 75, 76, 78, 80, 81, 82, 84, 85]      # Chunk04\n",
        "# v_q_ids = [86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 101, 102, 103, 104]  # Chunk05\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "# PART 2: Define test question IDs to run through RAG system (use 10 to capture ~33% of test data)\n",
        "t_q_ids = [\n",
        "    # 5,\n",
        "    # 14,\n",
        "    # 21,\n",
        "    # 31,\n",
        "    # 37,\n",
        "    # 56,\n",
        "    # 71,\n",
        "    83,   # 5.2.3 Test Question 3\n",
        "    # 90,\n",
        "    # 99,\n",
        "]\n",
        "\n",
        "# PART 3: Generate RAG output for validation questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_v_research_mistral = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_v_research_cohere = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_v_marketing_mistral = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_v_marketing_cohere = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='marketing')\n",
        "\n",
        "# PART 4: Generate RAG output for test questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_t_research_mistral = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_t_research_cohere = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_t_marketing_mistral = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_t_marketing_cohere = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='marketing')\n",
        "\n",
        "# PART 5: Convert RAG outputs to abbreviated datasets and pandas dataframes to prepare for performance measurements/metric evaluations\n",
        "\n",
        "# Create \"research\" and \"marketing\" RAG output datasets for validation question set\n",
        "# rag_output_v_research_mistral_abbrev = {key: rag_output_v_research_mistral[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "# rag_output_v_research_mistral_ds = Dataset.from_dict(rag_output_v_research_mistral_abbrev)\n",
        "rag_output_v_research_cohere_abbrev = {key: rag_output_v_research_cohere[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "rag_output_v_research_cohere_ds = Dataset.from_dict(rag_output_v_research_cohere_abbrev)\n",
        "# rag_output_v_marketing_mistral_abbrev = {key: rag_output_v_marketing_mistral[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "# rag_output_v_marketing_mistral_ds = Dataset.from_dict(rag_output_v_marketing_mistral_abbrev)\n",
        "rag_output_v_marketing_cohere_abbrev = {key: rag_output_v_marketing_cohere[key] for key in ['question', 'answer', 'contexts', 'ground_truth']}\n",
        "rag_output_v_marketing_cohere_ds = Dataset.from_dict(rag_output_v_marketing_cohere_abbrev)\n",
        "\n",
        "# Create \"research\" and \"marketing\" RAG output dataframes for validation question set\n",
        "# rag_output_v_research_mistral_df = Dataset.from_dict(rag_output_v_research_mistral).to_pandas()\n",
        "rag_output_v_research_cohere_df = Dataset.from_dict(rag_output_v_research_cohere).to_pandas()\n",
        "# rag_output_v_marketing_mistral_df = Dataset.from_dict(rag_output_v_marketing_mistral).to_pandas()\n",
        "rag_output_v_marketing_cohere_df = Dataset.from_dict(rag_output_v_marketing_cohere).to_pandas()\n",
        "\n",
        "# Create \"research\" and \"marketing\" RAG output dataframes for testing question set\n",
        "# rag_output_t_research_mistral_df = Dataset.from_dict(rag_output_t_research_mistral).to_pandas()\n",
        "rag_output_t_research_cohere_df = Dataset.from_dict(rag_output_t_research_cohere).to_pandas()\n",
        "# rag_output_t_marketing_mistral_df = Dataset.from_dict(rag_output_t_marketing_mistral).to_pandas()\n",
        "rag_output_t_marketing_cohere_df = Dataset.from_dict(rag_output_t_marketing_cohere).to_pandas()"
      ],
      "metadata": {
        "id": "zXd5BCgg29LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.b) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Score RAG system output\n",
        "# RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT = rag_score_combine(\n",
        "#     rag_output_v_research_mistral_df,\n",
        "#     rag_output_v_research_mistral_ds\n",
        "# )\n",
        "RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT = rag_score_combine(\n",
        "    rag_output_v_research_cohere_df,\n",
        "    rag_output_v_research_cohere_ds\n",
        ")\n",
        "\n",
        "# Show resulting scored output\n",
        "print('RAG System Scored Output (Research):\\n')\n",
        "# display(RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT)\n",
        "display(RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "c0c15db3fd3e4910a5c068857405890f",
            "47ec7878b44749238f1f548f6a9465d4",
            "3ad92e9328e54636829d778674cff266",
            "a5d42065b8314841ba7550afba846fa8",
            "83389b9430f845e2b330a4b5dc7decc9",
            "3790f799ce6941a780d465fac4830d80",
            "05b03ad91ea54455b66d7d66a58aa1a3",
            "ea4303d4145f4b549275a1914ea8e299",
            "9bd9dfcb42264397aff233abf9ccc543",
            "b28021973a354e308dcab500c5c83e47",
            "8d54feb2b1274c14b62b0888c5ec1d6b"
          ]
        },
        "id": "pkHbZcYP43Qe",
        "outputId": "3a9becbc-f36c-4f23-9f43-3eb935bd3fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0c15db3fd3e4910a5c068857405890f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Scored Output (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) have become integ...   \n",
              "1  Training data for embedding models that utiliz...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models (LLMs) serve the purpose...                 786   \n",
              "1  To create training data for embedding models t...                1022   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "0                  639  0.793893  0.658599  0.483418  0.371648  0.448980   \n",
              "1                  738  0.719178  0.617042  0.464706  0.379784  0.423529   \n",
              "\n",
              "    ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.206186  0.265306    0.265306             0.666541          0.703703   \n",
              "1  0.189723  0.266667    0.266667             0.652804          0.712703   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.684618                1.0             1.0           1.0   \n",
              "1      0.681439                1.0             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.902529            0.650536  \n",
              "1          0.968592            0.695218  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e7c98db-07b2-4416-a1bd-7676ec0424f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) have become integ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models (LLMs) serve the purpose...</td>\n",
              "      <td>786</td>\n",
              "      <td>639</td>\n",
              "      <td>0.793893</td>\n",
              "      <td>0.658599</td>\n",
              "      <td>0.483418</td>\n",
              "      <td>0.371648</td>\n",
              "      <td>0.448980</td>\n",
              "      <td>0.206186</td>\n",
              "      <td>0.265306</td>\n",
              "      <td>0.265306</td>\n",
              "      <td>0.666541</td>\n",
              "      <td>0.703703</td>\n",
              "      <td>0.684618</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.902529</td>\n",
              "      <td>0.650536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that utiliz...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>To create training data for embedding models t...</td>\n",
              "      <td>1022</td>\n",
              "      <td>738</td>\n",
              "      <td>0.719178</td>\n",
              "      <td>0.617042</td>\n",
              "      <td>0.464706</td>\n",
              "      <td>0.379784</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>0.189723</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.652804</td>\n",
              "      <td>0.712703</td>\n",
              "      <td>0.681439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.968592</td>\n",
              "      <td>0.695218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e7c98db-07b2-4416-a1bd-7676ec0424f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e7c98db-07b2-4416-a1bd-7676ec0424f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e7c98db-07b2-4416-a1bd-7676ec0424f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c75ef8f7-dfb7-478a-bfbf-fcd857a0a363\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c75ef8f7-dfb7-478a-bfbf-fcd857a0a363')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c75ef8f7-dfb7-478a-bfbf-fcd857a0a363 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d5b0a051-4c11-4ba1-b7e9-06587a204b10\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d5b0a051-4c11-4ba1-b7e9-06587a204b10 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.c) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Compute mean scores for RAG system output\n",
        "# RAG_V_RESEARCH_MISTRAL_MEAN_SCORES = rag_score_means(RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT)\n",
        "RAG_V_RESEARCH_COHERE_MEAN_SCORES = rag_score_means(RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT)\n",
        "\n",
        "# Show resulting means output\n",
        "print('RAG Metric Mean Scores (Research):\\n')\n",
        "# display(RAG_V_RESEARCH_MISTRAL_MEAN_SCORES)\n",
        "display(RAG_V_RESEARCH_COHERE_MEAN_SCORES)"
      ],
      "metadata": {
        "id": "NCeoPhI85FGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "ef758644-a586-4d67-85c7-e1204c8deefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Mean Scores (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'RAG_Answer_Lengths': 904.0,\n",
              " 'Gold_Answer_Lengths': 688.5,\n",
              " 'BLEU-1': 0.7565356059813866,\n",
              " 'BLEU-2': 0.6378204208438087,\n",
              " 'BLEU-3': 0.47406212484994,\n",
              " 'BLEU-4': 0.375715805819694,\n",
              " 'ROUGE-1': 0.43625450180072034,\n",
              " 'ROUGE-2': 0.197954443584206,\n",
              " 'ROUGE-L': 0.26598639455782314,\n",
              " 'ROUGE-Lsum': 0.26598639455782314,\n",
              " 'BERTScore-Precision': 0.6596726775169373,\n",
              " 'BERTScore-Recall': 0.7082026898860931,\n",
              " 'BERTScore-F1': 0.6830288767814636,\n",
              " 'context_precision': 0.999999999975,\n",
              " 'context_recall': 1.0,\n",
              " 'faithfulness': 1.0,\n",
              " 'answer_relevancy': 0.9355608311701502,\n",
              " 'answer_correctness': 0.6728770196467763,\n",
              " 'RAG_Gold_Answer_Lengths_Ratio': 1.3129992737835876}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.d) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Score RAG system output\n",
        "# RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT = rag_score_combine(\n",
        "#     rag_output_v_marketing_mistral_df,\n",
        "#     rag_output_v_marketing_mistral_ds\n",
        "# )\n",
        "RAG_V_MARKETING_COHERE_COMBINED_OUTPUT = rag_score_combine(\n",
        "    rag_output_v_marketing_cohere_df,\n",
        "    rag_output_v_marketing_cohere_ds\n",
        ")\n",
        "\n",
        "# Show resulting scored output\n",
        "print('RAG System Scored Output (Marketing):\\n')\n",
        "# display(RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT)\n",
        "display(RAG_V_MARKETING_COHERE_COMBINED_OUTPUT)"
      ],
      "metadata": {
        "id": "ZGrJvRgj5QSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "311e05851b4749bf9d8d85dcc6395677",
            "52ebc5c991f24794ad68e564b4c5be33",
            "82af016063464f5bbfa961463766faa4",
            "afda63a9ede0426a97c1a593c649757e",
            "ea2d6a4f876b439dbd6d2968c5f952bd",
            "f4b9835349994459a9e5dac9eef9a241",
            "4d51196b306c4c709c7c6ba20690cbc1",
            "da2443ca718942098b504b120c80ebed",
            "79ed2a2489da47b3ab485fa6bd4c7f41",
            "43a8e577c39e43f0bc26bbfed984df31",
            "074a3352e0c1427fbf412a7e6d406183"
          ]
        },
        "outputId": "e27504bd-255e-4928-83e4-dba6357095c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "311e05851b4749bf9d8d85dcc6395677"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Scored Output (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) are an essential ...   \n",
              "1  Training data for embedding models that use ta...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models serve the purpose of imp...                 476   \n",
              "1  Training data for embedding models that use ta...                 334   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "0                  290  0.596639  0.471579  0.282700  0.211416  0.245283   \n",
              "1                  348  0.852719  0.659458  0.534355  0.480924  0.560748   \n",
              "\n",
              "    ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.076923  0.207547    0.207547             0.539629          0.680908   \n",
              "1  0.400000  0.504673    0.504673             0.771538          0.775630   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.602092           1.000000             1.0          0.75   \n",
              "1      0.773579           0.805556             1.0          1.00   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.946459            0.338096  \n",
              "1          0.965484            0.492159  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2c0d4a4-1ef8-468f-a804-f24ef8f40f75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) are an essential ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models serve the purpose of imp...</td>\n",
              "      <td>476</td>\n",
              "      <td>290</td>\n",
              "      <td>0.596639</td>\n",
              "      <td>0.471579</td>\n",
              "      <td>0.282700</td>\n",
              "      <td>0.211416</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.539629</td>\n",
              "      <td>0.680908</td>\n",
              "      <td>0.602092</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.946459</td>\n",
              "      <td>0.338096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>334</td>\n",
              "      <td>348</td>\n",
              "      <td>0.852719</td>\n",
              "      <td>0.659458</td>\n",
              "      <td>0.534355</td>\n",
              "      <td>0.480924</td>\n",
              "      <td>0.560748</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.771538</td>\n",
              "      <td>0.775630</td>\n",
              "      <td>0.773579</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.965484</td>\n",
              "      <td>0.492159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2c0d4a4-1ef8-468f-a804-f24ef8f40f75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2c0d4a4-1ef8-468f-a804-f24ef8f40f75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2c0d4a4-1ef8-468f-a804-f24ef8f40f75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-30cd47f4-818a-490a-a465-5dbe0cbe4160\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-30cd47f4-818a-490a-a465-5dbe0cbe4160')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-30cd47f4-818a-490a-a465-5dbe0cbe4160 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5bd00c06-5d1a-498e-9ffa-35abde518e2b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_V_MARKETING_COHERE_COMBINED_OUTPUT')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5bd00c06-5d1a-498e-9ffa-35abde518e2b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_V_MARKETING_COHERE_COMBINED_OUTPUT');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_V_MARKETING_COHERE_COMBINED_OUTPUT"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.e) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Compute mean scores for RAG system output\n",
        "# RAG_V_MARKETING_MISTRAL_MEAN_SCORES = rag_score_means(RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT)\n",
        "RAG_V_MARKETING_COHERE_MEAN_SCORES = rag_score_means(RAG_V_MARKETING_COHERE_COMBINED_OUTPUT)\n",
        "\n",
        "# Show resulting means output\n",
        "print('RAG Metric Mean Scores (Marketing):\\n')\n",
        "# display(RAG_V_MARKETING_MISTRAL_MEAN_SCORES)\n",
        "display(RAG_V_MARKETING_COHERE_MEAN_SCORES)"
      ],
      "metadata": {
        "id": "qELP8c9G5b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "7ca52378-d292-4e09-b299-8d0162d4b8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Mean Scores (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'RAG_Answer_Lengths': 405.0,\n",
              " 'Gold_Answer_Lengths': 319.0,\n",
              " 'BLEU-1': 0.7246789085181939,\n",
              " 'BLEU-2': 0.5655185855848207,\n",
              " 'BLEU-3': 0.4085275921805064,\n",
              " 'BLEU-4': 0.3461700699062767,\n",
              " 'ROUGE-1': 0.40301534120966315,\n",
              " 'ROUGE-2': 0.23846153846153845,\n",
              " 'ROUGE-L': 0.3561100335037912,\n",
              " 'ROUGE-Lsum': 0.3561100335037912,\n",
              " 'BERTScore-Precision': 0.655583530664444,\n",
              " 'BERTScore-Recall': 0.7282693982124329,\n",
              " 'BERTScore-F1': 0.6878354251384735,\n",
              " 'context_precision': 0.9027777777476851,\n",
              " 'context_recall': 1.0,\n",
              " 'faithfulness': 0.875,\n",
              " 'answer_relevancy': 0.9559716187609533,\n",
              " 'answer_correctness': 0.4151271686534985,\n",
              " 'RAG_Gold_Answer_Lengths_Ratio': 1.2695924764890283}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.f) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Both\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Plot mean scores for RAG system output\n",
        "# RAG_V_MISTRAL_PP_FIG = rag_score_polar_plot(\n",
        "#     RAG_V_RESEARCH_MISTRAL_MEAN_SCORES,\n",
        "#     RAG_V_MARKETING_MISTRAL_MEAN_SCORES\n",
        "# )\n",
        "RAG_V_COHERE_PP_FIG = rag_score_polar_plot(\n",
        "    RAG_V_RESEARCH_COHERE_MEAN_SCORES,\n",
        "    RAG_V_MARKETING_COHERE_MEAN_SCORES\n",
        ")\n",
        "\n",
        "# Display polar plot\n",
        "print('RAG Metric Polar Plot (Research & Marketing):\\n')\n",
        "# RAG_V_MISTRAL_PP_FIG.show()\n",
        "RAG_V_COHERE_PP_FIG.show()"
      ],
      "metadata": {
        "id": "0aWnCbXl5i5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "cdad5b9c-ee4a-4504-c8e6-520689c9b8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Metric Polar Plot (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"463a8a26-24bb-46b9-aed3-b6cd51d0b49e\" class=\"plotly-graph-div\" style=\"height:525px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"463a8a26-24bb-46b9-aed3-b6cd51d0b49e\")) {                    Plotly.newPlot(                        \"463a8a26-24bb-46b9-aed3-b6cd51d0b49e\",                        [{\"fill\":\"toself\",\"name\":\"Research\",\"r\":[0.7565356059813866,0.6378204208438087,0.47406212484994,0.375715805819694,0.43625450180072034,0.197954443584206,0.26598639455782314,0.26598639455782314,0.6596726775169373,0.7082026898860931,0.6830288767814636,0.999999999975,1.0,1.0,0.9355608311701502,0.6728770196467763,1.3129992737835876],\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"name\":\"Marketing\",\"r\":[0.7246789085181939,0.5655185855848207,0.4085275921805064,0.3461700699062767,0.40301534120966315,0.23846153846153845,0.3561100335037912,0.3561100335037912,0.655583530664444,0.7282693982124329,0.6878354251384735,0.9027777777476851,1.0,0.875,0.9559716187609533,0.4151271686534985,1.2695924764890283],\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"},{\"line\":{\"color\":\"black\"},\"mode\":\"lines\",\"r\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"showlegend\":false,\"theta\":[\"BLEU-1\",\"BLEU-2\",\"BLEU-3\",\"BLEU-4\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"ROUGE-Lsum\",\"BERTScore-Precision\",\"BERTScore-Recall\",\"BERTScore-F1\",\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\",\"answer_correctness\",\"RAG_Gold_Answer_Lengths_Ratio\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"radialaxis\":{\"visible\":true,\"range\":[0,2]}},\"showlegend\":true,\"title\":{\"text\":\"RAG System Output Evaluation (Mean Scores)\"},\"width\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('463a8a26-24bb-46b9-aed3-b6cd51d0b49e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # EXTRA EXPORT STEP FOR CHUNKED RUNS/ITERATIONS - COMMENTED OUT\n",
        "\n",
        "# # Export RAG system output data and metric results\n",
        "\n",
        "# # PART 1: Set RAG system outputs to export objects and convert RAG mean score dictionaries to dataframes\n",
        "# # RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_EXPORT = RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT\n",
        "# # RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_EXPORT = RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT\n",
        "# # RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_RESEARCH_MISTRAL_MEAN_SCORES])\n",
        "# # RAG_V_MARKETING_MISTRAL_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_MARKETING_MISTRAL_MEAN_SCORES])\n",
        "# # RAG_T_RESEARCH_MISTRAL_OUTPUT_EXPORT = RAG_T_RESEARCH_MISTRAL_OUTPUT\n",
        "# # RAG_T_MARKETING_MISTRAL_OUTPUT_EXPORT = RAG_T_MARKETING_MISTRAL_OUTPUT\n",
        "# RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_EXPORT = RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT\n",
        "# RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_EXPORT = RAG_V_MARKETING_COHERE_COMBINED_OUTPUT\n",
        "# RAG_V_RESEARCH_COHERE_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_RESEARCH_COHERE_MEAN_SCORES])\n",
        "# RAG_V_MARKETING_COHERE_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_MARKETING_COHERE_MEAN_SCORES])\n",
        "\n",
        "# # PART 2: Write each dataframe to a different worksheet in an Excel (.xlsx) file\n",
        "# # Note - The 'Specs' codes below are defined as follows:\n",
        "# # * Questions (V = Validation, T = Test)\n",
        "# # * LLM (M = Mistral, C = Cohere)\n",
        "# # * Embedding Models (1 = all-MiniLM-L6-v2, 2 = multi-qa-mpnet-base-dot-v1, 3 = GIST-Embedding-v0)\n",
        "# # * Chunk Size (Number - e.g. 160)\n",
        "# # * Chunk Overlap (Number - e.g., 40)\n",
        "# # * Comments (Space for additional notes - e.g., LLM temperature = 0.6, Prompt Changes, etc.)\n",
        "\n",
        "# with pd.ExcelWriter(\n",
        "#     # 'RAG_V_scored_full_chunk01.xlsx',   # First 15 validation questions\n",
        "#     # 'RAG_V_scored_full_chunk02.xlsx'    # Next 15 validation questions\n",
        "#     # 'RAG_V_scored_full_chunk03.xlsx',   # Next 15 validation questions\n",
        "#     # 'RAG_V_scored_full_chunk04.xlsx',   # Next 15 validation questions\n",
        "#     'RAG_V_scored_full_chunk05.xlsx',   # Final 15 validation questions\n",
        "#     ) as writer:\n",
        "#     # RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_R_Output', index=False)\n",
        "#     # RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_R_MeanScores', index=False)\n",
        "#     # RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_M_Output', index=False)\n",
        "#     # RAG_V_MARKETING_MISTRAL_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_M_MeanScores', index=False)\n",
        "#     RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_R_Output', index=False)\n",
        "#     RAG_V_RESEARCH_COHERE_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_R_MeanScores', index=False)\n",
        "#     RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_M_Output', index=False)\n",
        "#     RAG_V_MARKETING_COHERE_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_M_MeanScores', index=False)\n",
        "# print('DataFrames written to Excel file successfully.')"
      ],
      "metadata": {
        "id": "VVfnVa57oOy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.g) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_RESEARCH_MISTRAL_OUTPUT = rag_output_t_research_mistral_df\n",
        "RAG_T_RESEARCH_COHERE_OUTPUT = rag_output_t_research_cohere_df\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output (Research):\\n')\n",
        "# display(RAG_T_RESEARCH_MISTRAL_OUTPUT)\n",
        "display(RAG_T_RESEARCH_COHERE_OUTPUT)"
      ],
      "metadata": {
        "id": "i-UDEfuK5FNY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "003b41e2-a3f5-456b-af97-bd6ae2a245c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-686c9146-61dc-4897-94d8-f2a6dabb5df0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-686c9146-61dc-4897-94d8-f2a6dabb5df0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-686c9146-61dc-4897-94d8-f2a6dabb5df0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-686c9146-61dc-4897-94d8-f2a6dabb5df0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_0e04abdb-2fdd-4e51-88cc-6878d3c913e9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_research_cohere_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0e04abdb-2fdd-4e51-88cc-6878d3c913e9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_research_cohere_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_research_cohere_df",
              "summary": "{\n  \"name\": \"rag_output_t_research_cohere_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly influenced by the diversity and breadth of questions it encounters during its training phase. If a model is exposed to a wide range of question types, it can develop a more robust understanding of the task and perform better when faced with novel queries. This is because the model learns to generalize from the varied examples it has seen, enabling it to extrapolate and apply its knowledge to new situations. However, if a model primarily learns from a narrow set of question types, its performance may be limited to those specific patterns, struggling with questions that deviate from its training data. Thus, it is essential to provide a comprehensive training dataset that covers different question structures, topics, and levels of complexity to ensure the model can effectively answer a broader spectrum of questions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.h) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_MARKETING_MISTRAL_OUTPUT = rag_output_t_marketing_mistral_df\n",
        "RAG_T_MARKETING_COHERE_OUTPUT = rag_output_t_marketing_cohere_df\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output (Marketing):\\n')\n",
        "# display(RAG_T_MARKETING_MISTRAL_OUTPUT)\n",
        "display(RAG_T_MARKETING_COHERE_OUTPUT)"
      ],
      "metadata": {
        "id": "3gKe1CvZ5yfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d5f468c0-86c4-406b-f573-00d94a8c93d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67f118b7-422b-4e5b-b33b-374a0dd593c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67f118b7-422b-4e5b-b33b-374a0dd593c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67f118b7-422b-4e5b-b33b-374a0dd593c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67f118b7-422b-4e5b-b33b-374a0dd593c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2e7b7051-5119-4b41-8585-1a67344788ff\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_marketing_cohere_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2e7b7051-5119-4b41-8585-1a67344788ff button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_marketing_cohere_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_marketing_cohere_df",
              "summary": "{\n  \"name\": \"rag_output_t_marketing_cohere_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly linked to its exposure to specific question types during training. Models can provide answers from a set of choices, including those not encountered during training, depending on their training and design.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4.2.2.i) Export RAG system output data and metric results\n",
        "\n",
        "# PART 1: Set RAG system outputs to export objects and convert RAG mean score dictionaries to dataframes\n",
        "# RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_EXPORT = RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT\n",
        "# RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_EXPORT = RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT\n",
        "# RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_RESEARCH_MISTRAL_MEAN_SCORES])\n",
        "# RAG_V_MARKETING_MISTRAL_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_MARKETING_MISTRAL_MEAN_SCORES])\n",
        "# RAG_T_RESEARCH_MISTRAL_OUTPUT_EXPORT = RAG_T_RESEARCH_MISTRAL_OUTPUT\n",
        "# RAG_T_MARKETING_MISTRAL_OUTPUT_EXPORT = RAG_T_MARKETING_MISTRAL_OUTPUT\n",
        "RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_EXPORT = RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT\n",
        "RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_EXPORT = RAG_V_MARKETING_COHERE_COMBINED_OUTPUT\n",
        "RAG_V_RESEARCH_COHERE_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_RESEARCH_COHERE_MEAN_SCORES])\n",
        "RAG_V_MARKETING_COHERE_MEAN_SCORES_EXPORT = pd.DataFrame.from_dict([RAG_V_MARKETING_COHERE_MEAN_SCORES])\n",
        "RAG_T_RESEARCH_COHERE_OUTPUT_EXPORT = RAG_T_RESEARCH_COHERE_OUTPUT\n",
        "RAG_T_MARKETING_COHERE_OUTPUT_EXPORT = RAG_T_MARKETING_COHERE_OUTPUT\n",
        "\n",
        "# PART 2: Write each dataframe to a different worksheet in an Excel (.xlsx) file\n",
        "# Note - The 'Specs' codes below are defined as follows:\n",
        "# * Questions (V = Validation, T = Test)\n",
        "# * LLM (M = Mistral, C = Cohere)\n",
        "# * Embedding Models (1 = all-MiniLM-L6-v2, 2 = multi-qa-mpnet-base-dot-v1, 3 = GIST-Embedding-v0)\n",
        "# * Chunk Size (Number - e.g. 160)\n",
        "# * Chunk Overlap (Number - e.g., 40)\n",
        "# * Comments (Space for additional notes - e.g., LLM temperature = 0.6, Prompt Changes, etc.)\n",
        "\n",
        "with pd.ExcelWriter(\n",
        "    # Comment out old iteration runs below and add new lines as needed\n",
        "    # 'RAG_V_scored_iter001.xlsx',  # Specs: V | M | 1 | 160 | 40 | Baseline test\n",
        "    # 'RAG_V_scored_iter002.xlsx',  # Specs: V | C | 1 | 160 | 40 | LLM C > LLM M\n",
        "    # 'RAG_V_scored_iter003.xlsx',  # Specs: V | C | 2 | 160 | 40 | Emb 2 > Emb 1\n",
        "    # 'RAG_V_scored_iter004.xlsx',  # Specs: V | C | 3 | 160 | 40 | Emb 3 ~ Emb 2\n",
        "    # 'RAG_V_scored_iter005.xlsx',  # Specs: V | C | 3 |  50 | 20 | Worse with CS 50 / CO 20\n",
        "    # 'RAG_V_scored_iter006.xlsx',  # Specs: V | C | 3 | 500 | 20 | Better with CS 500 / CO 20\n",
        "    # 'RAG_V_scored_iter007.xlsx',  # Specs: V | C | 3 | 400 | 20 | Better with CS 400 / CO 20\n",
        "    # 'RAG_V_scored_iter008.xlsx',  # Specs: V | C | 3 | 300 | 20 | Slightly worse with CS 300 / CO 20\n",
        "    # 'RAG_V_scored_iter009.xlsx',  # Specs: V | C | 3 | 350 | 20 | Slightly worse with CS 350 / CO 20\n",
        "    'RAG_V_scored_iter010.xlsx',    # Specs: V | C | 3 | 400 | 20 | Improved prompts\n",
        "    ) as writer:\n",
        "    # RAG_V_RESEARCH_MISTRAL_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_R_Output', index=False)\n",
        "    # RAG_V_RESEARCH_MISTRAL_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_R_MeanScores', index=False)\n",
        "    # RAG_V_MARKETING_MISTRAL_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_M_Output', index=False)\n",
        "    # RAG_V_MARKETING_MISTRAL_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_M_MeanScores', index=False)\n",
        "    RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_R_Output', index=False)\n",
        "    RAG_V_RESEARCH_COHERE_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_R_MeanScores', index=False)\n",
        "    RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_EXPORT.to_excel(writer, sheet_name='V_M_Output', index=False)\n",
        "    RAG_V_MARKETING_COHERE_MEAN_SCORES_EXPORT.to_excel(writer, sheet_name='V_M_MeanScores', index=False)\n",
        "print('DataFrames written to Excel file successfully.')\n",
        "with pd.ExcelWriter(\n",
        "    # Comment out old iteration runs below and add new lines as needed\n",
        "    # 'RAG_T_nonscored_iter001.xlsx',  # Specs: T | M | 1 | 160 | 40 | Baseline test\n",
        "    # 'RAG_T_nonscored_iter002.xlsx',  # Specs: T | C | 1 | 160 | 40 | LLM C > LLM M\n",
        "    # 'RAG_T_nonscored_iter003.xlsx',  # Specs: T | C | 2 | 160 | 40 | Emb 2 > Emb 1\n",
        "    # 'RAG_T_nonscored_iter004.xlsx',  # Specs: T | C | 3 | 160 | 40 | Emb 3 ~ Emb 2\n",
        "    # 'RAG_T_nonscored_iter005.xlsx',  # Specs: T | C | 3 |  50 | 20 | Worse with CS 50 / CO 20\n",
        "    # 'RAG_T_nonscored_iter006.xlsx',  # Specs: T | C | 3 | 500 | 20 | Better with CS 500 / CO 20\n",
        "    # 'RAG_T_nonscored_iter007.xlsx',  # Specs: T | C | 3 | 400 | 20 | Better with CS 400 / CO 20\n",
        "    # 'RAG_T_nonscored_iter008.xlsx',  # Specs: T | C | 3 | 300 | 20 | Slightly worse with CS 300 / CO 20\n",
        "    # 'RAG_T_nonscored_iter009.xlsx',  # Specs: T | C | 3 | 350 | 20 | Slightly worse with CS 350 / CO 20\n",
        "    'RAG_T_nonscored_iter010.xlsx',    # Specs: T | C | 3 | 400 | 20 | Improved prompts\n",
        "    ) as writer:\n",
        "    # RAG_T_RESEARCH_MISTRAL_OUTPUT_EXPORT.to_excel(writer, sheet_name='T_R_Output', index=False)\n",
        "    # RAG_T_MARKETING_MISTRAL_OUTPUT_EXPORT.to_excel(writer, sheet_name='T_M_Output', index=False)\n",
        "    RAG_T_RESEARCH_COHERE_OUTPUT_EXPORT.to_excel(writer, sheet_name='T_R_Output', index=False)\n",
        "    RAG_T_MARKETING_COHERE_OUTPUT_EXPORT.to_excel(writer, sheet_name='T_M_Output', index=False)\n",
        "print('DataFrames written to Excel file successfully.')"
      ],
      "metadata": {
        "id": "4_rQ5zQW5FRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cabea4f4-93d4-41bd-89e5-bc5d29770fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames written to Excel file successfully.\n",
            "DataFrames written to Excel file successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdOhlN23AYiA"
      },
      "source": [
        "##5. POC RAG Details, Findings, and Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.1 Model Specifications"
      ],
      "metadata": {
        "id": "2a9qaIha7JIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompts:** *Separate prompts were designed and employed for the engineering (\"research\") and marketing support (\"marketing\") personas within the RAG system.* This required twice as many runs of the RAG system – once to generate answers for the engineering team and a second time to generate answers for the marketing team – but led to responses that were generally well-tailored to each group's unique needs. No active trade-offs between the needs of each team were required given the independence of the prompts. However, for this POC, extensive fine-tuning of the \"marketing\" prompt (relative to the \"research\" prompt) was not performed due to the smaller size of the marketing team (40 employees, compared to 300 engineers), suspected average system use differences among engineers and marketing support staff, and time limitations.\n",
        "\n",
        "**Large Language Model (LLM):** *Cohere was chosen as the LLM for the RAG system given its stronger performance.* Some estimates state that Cohere has >50B parameters while Mistral has only ~7B, so the performance difference observed between Cohere and Mistral during RAG testing trials makes sense (generally, more parameters are better). Default parameters were used for Cohere, with no adjustments made for temperature, top-p, max length, etc. With additional time (i.e., beyond the POC), the LLM could be fine-tuned with additional GenAI/NLP question-answer pairs to further improve overall performance and even more effectively adapt answers to the GenAI/NLP domain.\n",
        "\n",
        "**Embedding Model:** *avsolatorio/GIST-Embedding-v0 was chosen as the embedding model for the RAG system given its stronger performance.* The GIST embedding model has an embedding dimension of 768, which is the same as multi-qa-mpnet-base-dot-v1 but twice as large as all-MiniLM-L6-v2 with an embedding dimension of 384, so the performance differences observed between this model and others during RAG testing trials makes sense (generally, a higher embedding dimension is better).\n",
        "\n",
        "**Chunking Parameters:** *A chunk size of 400 with a chunk overlap of 20 was chosen as the chunking strategy for the RAG system given its performance.* Multiple chunk size and overlap values were tested – including some larger and smaller than 400 / 20 – but none performed better than these two values. With additional time (i.e., beyond the POC), more values could be tested to optimize the chunking strategy further. Directionally, it seems that reducing the chunk size below 400 (at least to a limited degree) and/or incrementing up the overlap value beyond 20 (at least to a limited degree) could potentially help with increasing the focus/relevance of RAG-generated answers.\n",
        "\n",
        "**Retrieval Strategy/Method:** *The default method of similarity search using a single vector store-backend retriever was chosen as the retrieval strategy for the RAG system.* Other methods – like maximal marginal redundancy (MMR) search, incorporating a similarity score threshold value, or specifying a top-k for retrieval – were not tested for this POC."
      ],
      "metadata": {
        "id": "EzF0FrKblSkV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7J41pWiyh06"
      },
      "source": [
        "\n",
        "###5.2 Exploring a Few Test Questions & Answers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5.2.1 First Test Question (Validation Set)"
      ],
      "metadata": {
        "id": "Y4ugbru6ZpXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.a-1a) Return RAG system output for test question 1\n",
        "\n",
        "# Define question ID\n",
        "q_id_1 = 0\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_research_tq1 = RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT[\n",
        "    RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT['q_id']==q_id_1\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.1.a-1a: RAG Results - Test Question 1 (Research):\\n')\n",
        "display(RAG_output_research_tq1)"
      ],
      "metadata": {
        "id": "VdT4Wcl7X2AD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1635
        },
        "outputId": "766a02c9-33fb-4384-d26e-501438a4143c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.a-1a: RAG Results - Test Question 1 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) have become integ...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models (LLMs) serve the purpose...                 872   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3   BLEU-4   ROUGE-1  \\\n",
              "0                  639  0.723624  0.603904  0.435632  0.32336  0.387097   \n",
              "\n",
              "   ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.15814  0.248848    0.248848             0.638069          0.686313   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.661313                1.0             1.0           0.8   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.921056             0.57388  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f02ad8f9-df8b-449c-84d0-0c604bf14b96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) have become integ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models (LLMs) serve the purpose...</td>\n",
              "      <td>872</td>\n",
              "      <td>639</td>\n",
              "      <td>0.723624</td>\n",
              "      <td>0.603904</td>\n",
              "      <td>0.435632</td>\n",
              "      <td>0.32336</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.15814</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.638069</td>\n",
              "      <td>0.686313</td>\n",
              "      <td>0.661313</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.921056</td>\n",
              "      <td>0.57388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f02ad8f9-df8b-449c-84d0-0c604bf14b96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f02ad8f9-df8b-449c-84d0-0c604bf14b96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f02ad8f9-df8b-449c-84d0-0c604bf14b96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2cc5ba72-715f-4aad-a7d1-8d30b0182901\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_research_tq1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2cc5ba72-715f-4aad-a7d1-8d30b0182901 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_research_tq1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_research_tq1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (24) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.a-1b) Return RAG system output for test question 1\n",
        "\n",
        "# Return RAG \"research\" answer only\n",
        "RAG_research_answer_tq1 = RAG_output_research_tq1['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.1.a-1b: RAG Response - Test Question 1 (Research):\\n')\n",
        "display(RAG_research_answer_tq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "E9uBBgmrAB0O",
        "outputId": "54eb245c-a257-42be-98b8-811140f1ab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.a-1b: RAG Response - Test Question 1 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Large language models (LLMs) have become integral tools in natural language processing (NLP), offering a wide range of applications. These models are designed to process and analyze large volumes of text data, learning statistical relationships between words and phrases. This enables LLMs to generate human-like text, understand and interpret natural language input, and perform a variety of tasks such as speech recognition, machine translation, and information retrieval. The advanced capabilities of LLMs in NLP have led to their widespread use in developing intelligent systems that can interact with users in a more human-like manner. However, one of the challenges in the development of LLMs is aligning their training objectives with human objectives, ensuring that the models not only generate responses but also follow instructions and perform tasks as expected.']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.a-2a) Return RAG system output for test question 1\n",
        "\n",
        "# Define question ID\n",
        "q_id_1 = 0\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_marketing_tq1 = RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT[\n",
        "    RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT['q_id']==q_id_1\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.1.a-2a: RAG Results - Test Question 1 (Marketing):\\n')\n",
        "display(RAG_output_marketing_tq1)"
      ],
      "metadata": {
        "id": "7f3d7MPWghiz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "ad9045c1-bd37-4002-af8c-98e4db25f0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.a-2a: RAG Results - Test Question 1 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0     0  What purpose do large language models serve in...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Large language models (LLMs) are an essential ...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A large language model (LLM) is a language mo...   \n",
              "\n",
              "                                             sources  \\\n",
              "0  [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "0  Large language models serve the purpose of imp...                 485   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1   BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "0                  290  0.589691  0.47314  0.289855  0.224066  0.264151   \n",
              "\n",
              "    ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "0  0.076923  0.207547    0.207547             0.546218          0.683818   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "0      0.607321                1.0             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "0          0.937324             0.53102  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b83380c8-11dc-4618-bd32-280ad08e4003\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) are an essential ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models serve the purpose of imp...</td>\n",
              "      <td>485</td>\n",
              "      <td>290</td>\n",
              "      <td>0.589691</td>\n",
              "      <td>0.47314</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.224066</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.546218</td>\n",
              "      <td>0.683818</td>\n",
              "      <td>0.607321</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.937324</td>\n",
              "      <td>0.53102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b83380c8-11dc-4618-bd32-280ad08e4003')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b83380c8-11dc-4618-bd32-280ad08e4003 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b83380c8-11dc-4618-bd32-280ad08e4003');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_463715be-7a35-4e3b-9026-3e741508e454\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_marketing_tq1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_463715be-7a35-4e3b-9026-3e741508e454 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_marketing_tq1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_marketing_tq1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (24) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.a-2b) Return RAG system output for test question 1\n",
        "\n",
        "# Return RAG \"marketing\" answer only\n",
        "RAG_marketing_answer_tq1 = RAG_output_marketing_tq1['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.1.a-2b: RAG Response - Test Question 1 (Marketing):\\n')\n",
        "display(RAG_marketing_answer_tq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "91TrTv1SBAY6",
        "outputId": "44f29d96-26e2-4fa4-c61d-4c2bbac4025c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.a-2b: RAG Response - Test Question 1 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Large language models (LLMs) are an essential tool in natural language processing (NLP), offering advanced capabilities for a wide range of tasks, from speech and text recognition to more complex functions like generating human-like text and understanding instructions. LLMs are trained using vast text corpora to acquire knowledge and skills that align with human objectives, enabling them to process and generate language with a level of sophistication that rivals human performance.']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.b) Return RAG system output for test question 1\n",
        "\n",
        "# Return RAG contexts for question ID only\n",
        "RAG_contexts_tq1 = RAG_output_research_tq1['contexts'].to_list()\n",
        "\n",
        "# View RAG contexts\n",
        "print('5.2.1.b: RAG Contexts - Test Question 1 (Research & Marketing):\\n')\n",
        "display(RAG_contexts_tq1)"
      ],
      "metadata": {
        "id": "s-uQeCn6gltz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "9f68d19c-9053-417c-950f-492f0c409078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.b: RAG Contexts - Test Question 1 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text',\n",
              "        'models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, handwriting recognition, grammar induction, and information retrieval.Large language models, currently their most advanced form, are a',\n",
              "        'at https://github.com/DaoD/INTERS.\\n1\\nIntroduction\\nLarge language models (LLMs) have shown re-\\nmarkable capabilities across various natural lan-\\nguage processing (NLP) tasks. While these models\\nhave learned vast knowledge from large text cor-\\npora, their (pre-)training objective is not aligned\\nwith human’s objective: the latter requires models\\nto “follow human instructions and perform tasks”',\n",
              "        '1\\nIntroduction\\nLarge language models (LMs) can be “prompted” to perform a range of natural language process-\\ning (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following'],\n",
              "       dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.1.c) Return RAG system output for test question 1\n",
        "\n",
        "# Return RAG document sources for question ID only\n",
        "RAG_sources_tq1 = RAG_output_research_tq1['sources'].to_list()\n",
        "\n",
        "# View RAG document sources\n",
        "print('5.2.1.c: RAG Document Sources - Test Question 1 (Research & Marketing):\\n')\n",
        "display(RAG_sources_tq1)"
      ],
      "metadata": {
        "id": "yNB5nJi7jGCL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "439838c4-4db4-4454-aa58-0d3d554e6a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.1.c: RAG Document Sources - Test Question 1 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['https://en.wikipedia.org/wiki/Large_language_model',\n",
              "        'https://en.wikipedia.org/wiki/Language_model',\n",
              "        'https://arxiv.org/pdf/2401.06532.pdf',\n",
              "        'https://arxiv.org/pdf/2203.02155.pdf'], dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZdkNySoUDy3"
      },
      "source": [
        "####5.2.2 Second Test Question (Validation Set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.a-1a) Return RAG system output for test question 2\n",
        "\n",
        "# Define question ID\n",
        "q_id_2 = 50\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_research_tq2 = RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT[\n",
        "    RAG_V_RESEARCH_COHERE_COMBINED_OUTPUT_MBT['q_id']==q_id_2\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.2.a-1a: RAG Results - Test Question 2 (Research):\\n')\n",
        "display(RAG_output_research_tq2)"
      ],
      "metadata": {
        "id": "MzV-l4y6jqwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1304
        },
        "outputId": "6ed2aeab-a637-45b9-ad1c-6e87dd71e818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.a-1a: RAG Results - Test Question 2 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "1  Training data for embedding models that utiliz...   \n",
              "\n",
              "                                            contexts  \\\n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "1  To create training data for embedding models t...                1027   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "1                  738  0.715677  0.611111  0.461463  0.378906  0.426357   \n",
              "\n",
              "   ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "1   0.1875  0.263566    0.263566             0.658139          0.715702   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "1      0.685715                1.0             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "1          0.969059            0.733687  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec382df4-d5e5-47f0-bb1b-dc3258ade6b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that utiliz...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>To create training data for embedding models t...</td>\n",
              "      <td>1027</td>\n",
              "      <td>738</td>\n",
              "      <td>0.715677</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.461463</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.426357</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.263566</td>\n",
              "      <td>0.263566</td>\n",
              "      <td>0.658139</td>\n",
              "      <td>0.715702</td>\n",
              "      <td>0.685715</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.969059</td>\n",
              "      <td>0.733687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec382df4-d5e5-47f0-bb1b-dc3258ade6b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec382df4-d5e5-47f0-bb1b-dc3258ade6b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec382df4-d5e5-47f0-bb1b-dc3258ade6b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_4365a4b9-4106-4d30-b7ac-5fe4aeab1d0a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_research_tq2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4365a4b9-4106-4d30-b7ac-5fe4aeab1d0a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_research_tq2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_research_tq2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (24) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.a-1b) Return RAG system output for test question 2\n",
        "\n",
        "# Return RAG \"research\" answer only\n",
        "RAG_research_answer_tq2 = RAG_output_research_tq2['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.2.a-1b: RAG Response - Test Question 2 (Research):\\n')\n",
        "display(RAG_research_answer_tq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "6eBw_Su0BdYa",
        "outputId": "c7f9fa77-9d7e-497a-8cdf-83f318e9ffb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.a-1b: RAG Response - Test Question 2 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\"Training data for embedding models that utilize task-specific instructions is typically generated through a combination of existing datasets and the creation of new datasets specifically designed for this purpose. In the provided context, the INSTRUCTOR method is introduced, which combines 300 datasets from Super-NaturalInstructions (super-NI) and 30 datasets from other sources to form the Multitask Embeddings Data with Instructions (MEDI) collection. This diverse dataset collection enables the model to learn from a wide range of tasks and instructions. Another approach mentioned is to formulate various tasks as text-to-text problems, where the model learns to generate the desired output given a specific input and instruction. This allows for a more flexible and adaptable embedding model that can handle a broad range of use cases. Additionally, the use of asymmetric and symmetric tasks, as mentioned in the context, also plays a role in enhancing the model's performance when combined with instruction fine-tuning.\"]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.a-2a) Return RAG system output for test question 2\n",
        "\n",
        "# Define question ID\n",
        "q_id_2 = 50\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_marketing_tq2 = RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT[\n",
        "    RAG_V_MARKETING_COHERE_COMBINED_OUTPUT_MBT['q_id']==q_id_2\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.2.a-2a: RAG Results - Test Question 2 (Marketing):\\n')\n",
        "display(RAG_output_marketing_tq2)"
      ],
      "metadata": {
        "id": "79RADevJjqwT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "4ea0ba67-7d3f-4b5e-fc1a-c4b3e4cc3ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.a-2a: RAG Results - Test Question 2 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "1    50  What methods are typically employed to create ...   \n",
              "\n",
              "                                              answer  \\\n",
              "1  Training data for embedding models that use ta...   \n",
              "\n",
              "                                            contexts  \\\n",
              "1  [variety of tasks for embedding training with ...   \n",
              "\n",
              "                                             sources  \\\n",
              "1  [https://arxiv.org/pdf/2212.09741.pdf, https:/...   \n",
              "\n",
              "                                        ground_truth  RAG_Answer_Lengths  \\\n",
              "1  Training data for embedding models that use ta...                 334   \n",
              "\n",
              "   Gold_Answer_Lengths    BLEU-1    BLEU-2    BLEU-3    BLEU-4   ROUGE-1  \\\n",
              "1                  348  0.852719  0.659458  0.534355  0.480924  0.560748   \n",
              "\n",
              "   ROUGE-2   ROUGE-L  ROUGE-Lsum  BERTScore-Precision  BERTScore-Recall  \\\n",
              "1      0.4  0.504673    0.504673             0.771538           0.77563   \n",
              "\n",
              "   BERTScore-F1  context_precision  context_recall  faithfulness  \\\n",
              "1      0.773579           0.805556             1.0           1.0   \n",
              "\n",
              "   answer_relevancy  answer_correctness  \n",
              "1          0.966024            0.492167  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e98fa63a-e605-4589-955c-e82eaa0aa204\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>RAG_Answer_Lengths</th>\n",
              "      <th>Gold_Answer_Lengths</th>\n",
              "      <th>BLEU-1</th>\n",
              "      <th>BLEU-2</th>\n",
              "      <th>BLEU-3</th>\n",
              "      <th>BLEU-4</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>ROUGE-Lsum</th>\n",
              "      <th>BERTScore-Precision</th>\n",
              "      <th>BERTScore-Recall</th>\n",
              "      <th>BERTScore-F1</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>What methods are typically employed to create ...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>[variety of tasks for embedding training with ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2212.09741.pdf, https:/...</td>\n",
              "      <td>Training data for embedding models that use ta...</td>\n",
              "      <td>334</td>\n",
              "      <td>348</td>\n",
              "      <td>0.852719</td>\n",
              "      <td>0.659458</td>\n",
              "      <td>0.534355</td>\n",
              "      <td>0.480924</td>\n",
              "      <td>0.560748</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.504673</td>\n",
              "      <td>0.771538</td>\n",
              "      <td>0.77563</td>\n",
              "      <td>0.773579</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.966024</td>\n",
              "      <td>0.492167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e98fa63a-e605-4589-955c-e82eaa0aa204')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e98fa63a-e605-4589-955c-e82eaa0aa204 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e98fa63a-e605-4589-955c-e82eaa0aa204');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_440c4b9c-7d99-4b41-9ff7-078396d27546\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_marketing_tq2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_440c4b9c-7d99-4b41-9ff7-078396d27546 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_marketing_tq2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_marketing_tq2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (24) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.a-2b) Return RAG system output for test question 1\n",
        "\n",
        "# Return RAG \"marketing\" answer only\n",
        "RAG_marketing_answer_tq2 = RAG_output_marketing_tq2['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.2.a-2b: RAG Response - Test Question 2 (Marketing):\\n')\n",
        "display(RAG_marketing_answer_tq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "qDJWzkwABeAU",
        "outputId": "1418885d-b243-46e9-c3a4-7e222b4224cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.a-2b: RAG Response - Test Question 2 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Training data for embedding models that use task-specific instructions is created by formulating a wide array of tasks as text-to-text problems, where the model learns to map input text and task instructions to a fixed-size embedding. This approach allows the model to generate tailored text embeddings for specific tasks and domains.']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.b) Return RAG system output for test question 2\n",
        "\n",
        "# Return RAG contexts for question ID only\n",
        "RAG_contexts_tq2 = RAG_output_research_tq2['contexts'].to_list()\n",
        "\n",
        "# View RAG contexts\n",
        "print('5.2.2.b: RAG Contexts - Test Question 2 (Research & Marketing):\\n')\n",
        "display(RAG_contexts_tq2)"
      ],
      "metadata": {
        "id": "0E2nXI6IjqwU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "46bd5a2c-c05f-4979-cacd-43dd4578351a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.b: RAG Contexts - Test Question 2 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['variety of tasks for embedding training with in-\\nstructions. We thus construct a collection of 330\\ndatasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,\\n2022b) with 30 datasets from existing collections',\n",
              "        'Abstract\\nWe introduce INSTRUCTOR, a new method\\nfor computing text embeddings given task in-\\nstructions: every text input is embedded to-\\ngether with instructions explaining the use case\\n(e.g., task and domain descriptions). Unlike\\nencoders from prior work that are more special-\\nized, INSTRUCTOR is a single embedder that\\ncan generate text embeddings tailored to differ-',\n",
              "        'finetuned embedding models. Given an input text\\nx and a task instruction Ix, INSTRUCTOR encodes\\ntheir concatenation Ix ⊕x. We then generate a\\nfixed-sized, task-specific embedding EI(Ix, x) by\\napplying mean pooling to the last hidden represen-\\ntations over the tokens in x.\\n2.2\\nTraining Objective\\nINSTRUCTOR is trained by formulating a wide\\nvariety of tasks as a text-to-text problem of distin-',\n",
              "        'ric data (see that the rightmost bar gets additive\\nperformance gains from the asymmetric and sym-\\nmetric tasks). This result demonstrates the impor-\\ntance of instruction finetuning when diverse data\\nare used for embedding training. Note that train-\\ning on symmetric tasks only without instructions\\nis similar to Sent-T5. Similarly, training on asym-'],\n",
              "       dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.2.c) Return RAG system output for test question 2\n",
        "\n",
        "# Return RAG document sources for question ID only\n",
        "RAG_sources_tq2 = RAG_output_research_tq2['sources'].to_list()\n",
        "\n",
        "# View RAG document sources\n",
        "print('5.2.2.c: RAG Document Sources - Test Question 2 (Research & Marketing):\\n')\n",
        "display(RAG_sources_tq2)"
      ],
      "metadata": {
        "id": "oIfJGwZajqwU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8ce06f29-c776-478e-82a6-a67c18bf4034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.2.c: RAG Document Sources - Test Question 2 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['https://arxiv.org/pdf/2212.09741.pdf',\n",
              "        'https://arxiv.org/pdf/2212.09741.pdf',\n",
              "        'https://arxiv.org/pdf/2212.09741.pdf',\n",
              "        'https://arxiv.org/pdf/2212.09741.pdf'], dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflections on Test Question Answer Differences across Engineering and Marketing Groups:**\n",
        "\n",
        "For question 1, both of the RAG-generated answers resemble the \"gold\" answers in terms of overall meaning/semantic similarity, and the metrics generally capture that similarity. While the RAG-generated answers tend to be slightly longer than the \"gold\" answers in terms of character count (with RAG_Gold_Answer_Lengths_Ratio values ranging from ~1.36 to ~1.67) and have a few differences in terms of concepts referenced (e.g., the engineering/research \"gold\" answer talks about next word prediction while the RAG-generated answer does not), they ultimately score decently on metrics like BERTScore-recall (with values around ~0.68), RAGAS faithfulness (with values between 0.80 and 1.00), and RAGAS answer correctness (with values ranging from ~0.53 to ~0.57), indicating that they're generally aligned with the content and themes of the \"gold\" answers.\n",
        "\n",
        "For question 2, there appears to be a very pronounced difference between the RAG-generated engineering/research and marketing answers on RAGAS answer correctness score (with values of ~0.49 and ~0.73), even though both answers score even better than their question 1 counterparts on several other key metrics. The < 0.50 answer correctness score for the marketing answer appears to be due to a non-trivial difference between this answer and the \"gold\" answer's explanation of how text-to-text problems create training data (so it's good that one of the RAGAS metrics reflected this): the marketing answer says this happens from the model learning to \"map input text and task instructions to a fixed-size embedding\", while the \"gold\" model indicates this happens from \"distinguishing good/bad candidate outputs given an input text\" and \"constructing positive and negative pairs for training.\" Overall, a few metrics in particular – namely, the length ratio metric, BERTScore metrics, and RAGAS metrics (especially faithfulness and answer correctness) – do a good job of pointing out *similarities and differences* between RAG-generated and \"gold\" answers, especially in terms of level of detail/appropriateness for the target audience (user persona), semantic similarity, and factual accuracy."
      ],
      "metadata": {
        "id": "s9HOmyeefMwJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxDhzMXsT48H"
      },
      "source": [
        "####5.2.3 Third Test Question (Test Set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.a-1a) Return RAG system output for test question 3\n",
        "\n",
        "# Define question ID\n",
        "q_id_3 = 83\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_research_tq3 = RAG_T_RESEARCH_COHERE_OUTPUT_MBT[\n",
        "    RAG_T_RESEARCH_COHERE_OUTPUT_MBT['q_id']==q_id_3\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.3.a-1a: RAG Results - Test Question 3 (Research):\\n')\n",
        "display(RAG_output_research_tq3)"
      ],
      "metadata": {
        "id": "yhoZ8egpkAnr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f4a185de-1221-4530-98a5-b8971f7d651e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.a-1a: RAG Results - Test Question 3 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe7de2e4-4f94-42af-b6d4-49ac3d956c15\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe7de2e4-4f94-42af-b6d4-49ac3d956c15')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe7de2e4-4f94-42af-b6d4-49ac3d956c15 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe7de2e4-4f94-42af-b6d4-49ac3d956c15');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_f5d3b110-3540-4903-a725-305ee08afbb4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_research_tq3')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f5d3b110-3540-4903-a725-305ee08afbb4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_research_tq3');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_research_tq3",
              "summary": "{\n  \"name\": \"RAG_output_research_tq3\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly influenced by the diversity and breadth of questions it encounters during its training phase. If a model is exposed to a wide range of question types, it can develop a more robust understanding of the task and perform better when faced with novel queries. This is because the model learns to generalize from the varied examples it has seen, enabling it to extrapolate and apply its knowledge to new situations. However, if a model primarily learns from a narrow set of question types, its performance may be limited to those specific patterns, struggling with questions that deviate from its training data. Thus, it is crucial to provide a comprehensive training dataset that covers different question categories to ensure the model can effectively answer a broader spectrum of queries.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.a-1b) Return RAG system output for test question 3\n",
        "\n",
        "# Return RAG \"research\" answer only\n",
        "RAG_research_answer_tq3 = RAG_output_research_tq3['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.3.a-1b: RAG Response - Test Question 3 (Research):\\n')\n",
        "display(RAG_research_answer_tq3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "cQTgCTRrCnyk",
        "outputId": "3bf1f776-077f-43be-b03f-f36c20cfb1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.a-1b: RAG Response - Test Question 3 (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\"A model's ability to answer questions is directly influenced by the diversity and breadth of questions it encounters during its training phase. If a model is exposed to a wide range of question types, it can develop a more robust understanding of the task and perform better when faced with novel queries. This is because the model learns to generalize from the varied examples it has seen, enabling it to extrapolate and apply its knowledge to new situations. However, if a model primarily learns from a narrow set of question types, its performance may be limited to those specific patterns, struggling with questions that deviate from its training data. Thus, it is crucial to provide a comprehensive training dataset that covers different question categories to ensure the model can effectively answer a broader spectrum of queries.\"]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.a-2a) Return RAG system output for test question 3\n",
        "\n",
        "# Define question ID\n",
        "q_id_3 = 83\n",
        "\n",
        "# Return RAG results for question ID only\n",
        "RAG_output_marketing_tq3 = RAG_T_MARKETING_COHERE_OUTPUT_MBT[\n",
        "    RAG_T_MARKETING_COHERE_OUTPUT_MBT['q_id']==q_id_3\n",
        "]\n",
        "\n",
        "# View RAG results\n",
        "print('5.2.3.a-2a: RAG Results - Test Question 3 (Marketing):\\n')\n",
        "display(RAG_output_marketing_tq3)"
      ],
      "metadata": {
        "id": "fPhZUxxokAn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c4d02d22-fd47-4cce-c3e5-343c89e43981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.a-2a: RAG Results - Test Question 3 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   q_id                                           question  \\\n",
              "0    83  How does a model's ability to answer questions...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  A model's ability to answer questions is direc...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [A model is able to correctly memorize and res...   \n",
              "\n",
              "                                             sources  \n",
              "0  [https://lilianweng.github.io/posts/2020-10-29...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eacfb014-370b-4fdc-92a1-5356365271d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eacfb014-370b-4fdc-92a1-5356365271d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eacfb014-370b-4fdc-92a1-5356365271d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eacfb014-370b-4fdc-92a1-5356365271d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_a5f8fe19-4cae-4ce4-91ee-1183f6cb7c21\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('RAG_output_marketing_tq3')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a5f8fe19-4cae-4ce4-91ee-1183f6cb7c21 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('RAG_output_marketing_tq3');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "RAG_output_marketing_tq3",
              "summary": "{\n  \"name\": \"RAG_output_marketing_tq3\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 83,\n        \"max\": 83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A model's ability to answer questions is directly related to its exposure to specific question types during training. The more diverse and comprehensive the training data, the better equipped the model is to handle novel questions and provide accurate responses, even for unseen answers.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.a-2b) Return RAG system output for test question 3\n",
        "\n",
        "# Return RAG \"marketing\" answer only\n",
        "RAG_marketing_answer_tq3 = RAG_output_marketing_tq3['answer'].to_list()\n",
        "\n",
        "# View RAG answer\n",
        "print('5.2.3.a-2b: RAG Response - Test Question 3 (Marketing):\\n')\n",
        "display(RAG_marketing_answer_tq3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "AB1KJhaRCorl",
        "outputId": "51fc5d87-45cd-4dca-ed54-b20c2790bd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.a-2b: RAG Response - Test Question 3 (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\"A model's ability to answer questions is directly related to its exposure to specific question types during training. The more diverse and comprehensive the training data, the better equipped the model is to handle novel questions and provide accurate responses, even for unseen answers.\"]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.b) Return RAG system output for test question 3\n",
        "\n",
        "# Return RAG contexts for question ID only\n",
        "RAG_contexts_tq3 = RAG_output_research_tq3['contexts'].to_list()\n",
        "\n",
        "# View RAG contexts\n",
        "print('5.2.3.b: RAG Contexts - Test Question 3 (Research & Marketing):\\n')\n",
        "display(RAG_contexts_tq3)"
      ],
      "metadata": {
        "id": "DHoH-oe8kAn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c6e1a601-9d1c-4fc7-92aa-7268c87a62aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.b: RAG Contexts - Test Question 3 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['A model is able to correctly memorize and respond with the answer to a question that has been seen at training time.\\nA model is able to answer novel questions at test time and choose an answer from the set of answers it has seen during training.\\nA model is able to answer novel questions which have answers not contained in the training dataset.',\n",
              "        'model to answer questions based on “knowledge” that it internalized during pre-training.',\n",
              "        'knowledge, we provided additional training before\\nletting them work on the task.\\n3\\nQASPER Analysis\\nTable 1 provides representative examples from\\nQASPER categorized by question, answer, and ev-\\nidence types, which we describe here in greater\\ndetail.\\nQuestion types\\nWe ﬁrst analyze whether our an-\\nnotation setup results in questions that are anchored',\n",
              "        'servation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train'],\n",
              "       dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.2.3.c) Return RAG system output for test question 3\n",
        "\n",
        "# Return RAG document sources for question ID only\n",
        "RAG_sources_tq3 = RAG_output_research_tq3['sources'].to_list()\n",
        "\n",
        "# View RAG document sources\n",
        "print('5.2.3.c: RAG Document Sources - Test Question 3 (Research & Marketing):\\n')\n",
        "display(RAG_sources_tq3)"
      ],
      "metadata": {
        "id": "52aNpUDdkAn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0efefb79-96a8-4f22-ee57-38ba2229fb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2.3.c: RAG Document Sources - Test Question 3 (Research & Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[array(['https://lilianweng.github.io/posts/2020-10-29-odqa/',\n",
              "        'https://lilianweng.github.io/posts/2020-10-29-odqa/',\n",
              "        'https://arxiv.org/pdf/2105.03011.pdf',\n",
              "        'https://arxiv.org/pdf/2309.15217.pdf'], dtype=object)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44h038A7AUdn"
      },
      "source": [
        "###5.3 Model Reflections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflections on Chunk Sizes:**\n",
        "\n",
        "*Assuming a Small Chunk Size of 50:* With a chunk size of 50, I'd expect response quality to *decrease* overall. While smaller chunk sizes may improve relevance (i.e., by capturing more pertinent information within a given chunk) and model processing efficiency, they can also result in incomplete information retrieval (i.e., by not capturing enough pertinent information within a given chunk) and limited context. For this RAG system – especially for answers provided to the engineering (\"research\") team – longer context windows play a key role in providing the model with sufficient content on which to base its answers, especially given the technical nature of the questions most likely to be passed through the system. A chunk size of 50 would almost certainly miss out on important information from relevant documents that needs to inform the model's answers, and cause the model to focus on partially helpful yet fragmented contextual source details (e.g., authors names from relevant research papers, website URLs from relevant web articles, etc.). To confirm this suspicion, I actually tested a chunk size of 50 observed a marked quality decrease relative to larger chunk sizes (e.g., 300, 400, etc.), with a sizable proportion of RAG answers lacking coherence or overall helpfulness.\n",
        "\n",
        "*Assuming a Large Chunk Size of 5000:* With a chunk size of 5000, I'd also expect response quality to *decrease* overall, though perhaps to a larger degree than with a chunk size of 50. A larger chunk size would allow for the retrieval of more comprehensive and detailed information, allowing the model to better understand any long-range dependencies or relationships within texts. However, such a huge chunk size could also return too much irrelevant or tangentially-relevant context and decrease model response quality by making it more challenging for the model to identify the *most* relevant portions of contextual text. As a result, responses could lack precision and potentially wind up more \"over-generalized\" than desired. While I didn't physically test a chunk size of 5000 (which would require *more* modifications to parameters within this notebook than testing a chunk size of 50 – e.g., adjusting the max_length within the LLM), I suspect it could lead to even worse performance than a chunk size of 50 given the fact that our document store contains only a small number of documents (and very large chunks/splits would limit the number of possible rankings from which the RAG system could draw relevant contexts).\n",
        "\n",
        "**Reflections on LLM Fine-Tuning:**\n",
        "\n",
        "Fine-tuning the LLM used the POC RAG system would likely be beneficial. The ideal data for this task would be an extended version of the validation set of question and answer pairs – containing similar types of GenAI-/NLP-related questions – where each question has a corresponding research/engineering \"gold\" (ground truth) answer and a corresponding marketing \"gold\" (ground truth) answer that have already been judged or compiled by human reviewers. Even better would be a dataset including these question and answer pairs *along with relevant context documents*. As a starting point for fine-tuning, I would define a performance metric – like BERTScore-F1 – for the model to try to optimize/maximize by generating answers that closely match the provided ground truth responses in terms of semantic similarity. This would tune the model's parameters (e.g., embeddings) to better fit the domain of Generative AI, reduce the risk of hallucinations or inaccurate responses, and enhance the relevance of RAG-generated answers to any new/unseen questions posed in the future.\n",
        "\n",
        "**Reflections on Prompt Design and Engineering/Marketing Support Needs:**\n",
        "\n",
        "I chose to design separate prompts for the engineering (\"research\") and marketing support (\"marketing\") personas. For the engineering prompt, I specifically requested that the model (A) provide \"clear and detailed responses tailored to the engineering staff and technical researchers\" at the company (identifying the target audience), (B) aim for a response \"between four and six sentences in length, with each sentence averaging between 100 and 160 words\" (identifying the target length based on a brief analysis of \"gold\" answers), and (C) ensure the \"answer is technically accurate and aligns with the provided context\", supported with \"concise references to relevant examples\" where appropriate (identifying the target feel and level of detail). Meanwhile, for the marketing prompt, I specifically requested that the model (A) provide \"concise and high-level responses suited for the marketing team\" at the company (identifying a different target audience), (B) aim for a response that is \"one or two sentences with each sentence averaging between 120 and 180 words\" (identifying the target length based on a brief analysis of \"gold\" answers), and (C) strive to offer a \"quick and high-level snapshot of the most important insights from the context that relate to the question\" (identifying the target feel and level of detail). For both prompts, I also assigned the model a persona (e.g., \"you are a highly capable Q&A assistant\") and encouraged the model to answer questions based solely on the context provided. This helped the RAG-generated responses more closely resemble the overall verbosity/conciseness (or average length) and level of detail of the \"gold\" answers provided for the validation questions and better satisfy the unique needs of each group.\n",
        "\n",
        "**Reflections on Hypothetical Average and Peak Load Estimates for the RAG System (Exploring Practical Use and Feasibility):**\n",
        "\n",
        "Given the size of the tech company, I'd estimate a reasonable *average* system load of 4-8 queries per staff member per day – with about 5-7 expected for each engineer (x300) and 2-3 expected for each marketing team member (x40) – translating to a total of ~8,000-11,000 queries per 5-day workweek. Engineers would likely use the system more often for problem-solving, product development, testing, and research, while marketing staff would likely use the system less frequently for content creation, messaging to consumers, and A/B testing or experimentation. *Peak* load periods would likely occur during quarterly product releases, where engineers might be using the system more than usual for troubleshooting issues, supporting QC checks, or looking up technical details, and where marketing staff might be using it more than usual for drafting communications to go with product launches. During these periods, I'd estimate that system usage would roughly double to 8-16 queries per staff member per day (or to ~16,000-22,000 per workweek), with perhaps even additional volume (maybe another 100-200 queries) expected over the weekend. Based on this high (but reasonably predictable) variation in system load and the relatively low volume of potential users (340 staff members in total), I'd think a *pay-per-use deployment* would be best if this system were actually implemented. This would allow the company to scale up resources during peak load periods, scale them down during average load periods, and optimize costs by paying for only what is needed by the staff without incurring any unnecessary fixed expenses.\n",
        "\n",
        "**Reflections on Risks and Limitations of this POC:**\n",
        "\n",
        "1. Risk of the LLM potentially generating harmful or offensive content (given that neither the Mistral nor Cohere model used in testing has been trained for safety\n",
        "2. Risk of providing misleading or factually inaccurate answers (e.g., hallucinations) as a result of non-relevant or imprecise context information\n",
        "3. Risk of \"context drift\", where the contextual information retrieved from source documents and supplied to the system for answer generation becomes increasingly outdated over time\n",
        "4. Limited scope and inability to scale/generalize more broadly to additional uses, topic areas, etc. given the nature of documents loaded into the vectorstore (currently all GenAI-/NLP-related)\n",
        "5. Limited ability to apply abstract reasoning or creative problem-solving approaches to answer user queries (given that the system is restricted to answer based only the context provided)\n",
        "\n"
      ],
      "metadata": {
        "id": "_FtEaM44d3VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4 Full Validation & Test Question POC RAG Output\n",
        "\n",
        "Documented below are the RAG system's generated answers to all 75 \"validation\" questions and all 29 \"test\" questions. They are shown here as a final test of the system."
      ],
      "metadata": {
        "id": "mqBTl8Tr8p0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.a) Test RAG system on complete list of validation and test questions\n",
        "\n",
        "# REFERENCE: Count of validation questions and test questions\n",
        "# * Validation = 75\n",
        "# ** Question IDs:\n",
        "#   [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23, 24, 25, 27,\n",
        "#    28, 30, 33, 34, 35, 36, 38, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52,\n",
        "#    54, 55, 59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 73, 74, 75, 76, 78, 80,\n",
        "#    81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 101, 102, 103,\n",
        "#    104]\n",
        "# * Test = 29\n",
        "# ** Question IDs:\n",
        "#   [4, 5, 6, 10, 14, 15, 21, 26, 29, 31, 32, 37, 40, 42, 49, 56, 57, 58, 66,\n",
        "#    68, 71, 72, 77, 79, 83, 90, 98, 99, 100]\n",
        "\n",
        "\n",
        "# PART 1: Define validation question IDs to run through RAG system (use full list)\n",
        "v_q_ids_full = [0, 1, 2, 3, 7, 8, 9, 11, 12, 13, 16, 17, 18, 19, 20, 22, 23,\n",
        "                24, 25, 27, 28, 30, 33, 34, 35, 36, 38, 39, 41, 43, 44, 45,\n",
        "                46, 47, 48, 50, 51, 52, 54, 55, 59, 60, 61, 62, 63, 64, 65,\n",
        "                67, 69, 70, 73, 74, 75, 76, 78, 80, 81, 82, 84, 85, 86, 87,\n",
        "                88, 89, 91, 92, 93, 94, 95, 96, 97, 101, 102, 103, 104]\n",
        "\n",
        "# PART 2: Define test question IDs to run through RAG system (use full list)\n",
        "t_q_ids_full = [4, 5, 6, 10, 14, 15, 21, 26, 29, 31, 32, 37, 40, 42, 49,\n",
        "                56, 57, 58, 66, 68, 71, 72, 77, 79, 83, 90, 98, 99, 100]\n",
        "\n",
        "\n",
        "# PART 3: Generate RAG output for validation questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_v_research_mistral_full = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids_full,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_v_research_cohere_full = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids_full,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_v_marketing_mistral_full = generate_rag_data_dict_v(\n",
        "#     q_id_list=v_q_ids_full,\n",
        "#     qa_dict=validation_questions_answers,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_v_marketing_cohere_full = generate_rag_data_dict_v(\n",
        "    q_id_list=v_q_ids_full,\n",
        "    qa_dict=validation_questions_answers,\n",
        "    llm='cohere',\n",
        "    persona='marketing')\n",
        "\n",
        "# PART 4: Generate RAG output for test questions\n",
        "\n",
        "## Research Persona\n",
        "######################\n",
        "# rag_output_t_research_mistral_full = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids_full,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='research')\n",
        "rag_output_t_research_cohere_full = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids_full,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='research')\n",
        "\n",
        "## Marketing Persona\n",
        "######################\n",
        "# rag_output_t_marketing_mistral_full = generate_rag_data_dict_t(\n",
        "#     q_id_list=t_q_ids_full,\n",
        "#     q_dict=test_questions,\n",
        "#     llm='mistral',\n",
        "#     persona='marketing')\n",
        "rag_output_t_marketing_cohere_full = generate_rag_data_dict_t(\n",
        "    q_id_list=t_q_ids_full,\n",
        "    q_dict=test_questions,\n",
        "    llm='cohere',\n",
        "    persona='marketing')\n",
        "\n",
        "# PART 5: Convert RAG outputs to pandas dataframes for final answer review\n",
        "\n",
        "# Create \"research\" and \"marketing\" RAG output dataframes for validation question set\n",
        "# rag_output_v_research_mistral_df_full = Dataset.from_dict(rag_output_v_research_mistral_full).to_pandas()\n",
        "rag_output_v_research_cohere_df_full = Dataset.from_dict(rag_output_v_research_cohere_full).to_pandas()\n",
        "# rag_output_v_marketing_mistral_df_full = Dataset.from_dict(rag_output_v_marketing_mistral_full).to_pandas()\n",
        "rag_output_v_marketing_cohere_df_full = Dataset.from_dict(rag_output_v_marketing_cohere_full).to_pandas()\n",
        "\n",
        "# Create \"research\" and \"marketing\" RAG output dataframes for testing question set\n",
        "# rag_output_t_research_mistral_df_full = Dataset.from_dict(rag_output_t_research_mistral_full).to_pandas()\n",
        "rag_output_t_research_cohere_df_full = Dataset.from_dict(rag_output_t_research_cohere_full).to_pandas()\n",
        "# rag_output_t_marketing_mistral_df_full = Dataset.from_dict(rag_output_t_marketing_mistral_full).to_pandas()\n",
        "rag_output_t_marketing_cohere_df_full = Dataset.from_dict(rag_output_t_marketing_cohere_full).to_pandas()"
      ],
      "metadata": {
        "id": "vHmHLaw-8_zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.b) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_V_RESEARCH_MISTRAL_OUTPUT_FULL = rag_output_v_research_mistral_df_full\n",
        "RAG_V_RESEARCH_COHERE_OUTPUT_FULL = rag_output_v_research_cohere_df_full\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Full Validation List Final Review (Research):\\n')\n",
        "# display(RAG_V_RESEARCH_MISTRAL_OUTPUT_FULL)\n",
        "display(RAG_V_RESEARCH_COHERE_OUTPUT_FULL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "lOzgIdrFFsp2",
        "outputId": "e645db0a-0ce8-46a5-89ba-1d0e036beb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Full Validation List Final Review (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    q_id                                           question  \\\n",
              "0      0  What purpose do large language models serve in...   \n",
              "1      1  How does a large language model learn from tex...   \n",
              "2      2  What are some key architectures behind the dev...   \n",
              "3      3  Can you name some specific large language mode...   \n",
              "4      7  What licensing models have been adopted for th...   \n",
              "..   ...                                                ...   \n",
              "70    97  What can be inferred about the utilization of ...   \n",
              "71   101  Can the use of attention mechanisms in deep le...   \n",
              "72   102  What are the potential benefits of incorporati...   \n",
              "73   103  How does the transformer model variate from tr...   \n",
              "74   104  What implications does the concept of a Neural...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   Large language models (LLMs) have become integ...   \n",
              "1   Large language models (LLMs) learn by ingestin...   \n",
              "2   Large Language Models (LLMs) have become a pro...   \n",
              "3   Some prominent examples of Large Language Mode...   \n",
              "4   The provided context does not explicitly menti...   \n",
              "..                                                ...   \n",
              "70  Attention in neural networks is a powerful too...   \n",
              "71  Yes, attention mechanisms have proven effectiv...   \n",
              "72  The integration of self-attention mechanisms i...   \n",
              "73  The Transformer model differs from traditional...   \n",
              "74  The Neural Turing Machine (NTM) architecture p...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [A large language model (LLM) is a language mo...   \n",
              "1   [Fig. 13. The amount of computation used for t...   \n",
              "2   [S. Purohit, U. S. Prashanth, E. Raff, et al. ...   \n",
              "3   [A large language model (LLM) is a language mo...   \n",
              "4   [regulation. On the other hand, if large langu...   \n",
              "..                                                ...   \n",
              "70  [In a nutshell, attention in deep learning can...   \n",
              "71  [With the help of the attention, the dependenc...   \n",
              "72  [Self-Attention GAN#\\nSelf-Attention GAN (SAGA...   \n",
              "73  [SNAIL#\\nThe transformer has no recurrent or c...   \n",
              "74  [Neural Turing Machine (NTM, Graves, Wayne & D...   \n",
              "\n",
              "                                              sources  \\\n",
              "0   [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1   [https://lilianweng.github.io/posts/2020-10-29...   \n",
              "2   [https://arxiv.org/pdf/2305.14314.pdf, https:/...   \n",
              "3   [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "4   [https://arxiv.org/pdf/2203.02155.pdf, https:/...   \n",
              "..                                                ...   \n",
              "70  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "71  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "72  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "73  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "74  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "\n",
              "                                         ground_truth  \n",
              "0   Large language models (LLMs) serve the purpose...  \n",
              "1   A large language model learns from text during...  \n",
              "2   Key architectures behind the development of la...  \n",
              "3   Some specific large language models include GP...  \n",
              "4   Based on the provided context, it seems that l...  \n",
              "..                                                ...  \n",
              "70  Attention mechanisms in neural networks play a...  \n",
              "71  Yes, attention mechanisms in deep learning mod...  \n",
              "72  Incorporating self-attention mechanisms into G...  \n",
              "73  The transformer model differs from traditional...  \n",
              "74  The concept of a Neural Turing Machine (NTM) e...  \n",
              "\n",
              "[75 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52a2bb83-6988-491b-a139-7a8868f9afd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) have become integ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models (LLMs) serve the purpose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>How does a large language model learn from tex...</td>\n",
              "      <td>Large language models (LLMs) learn by ingestin...</td>\n",
              "      <td>[Fig. 13. The amount of computation used for t...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "      <td>A large language model learns from text during...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What are some key architectures behind the dev...</td>\n",
              "      <td>Large Language Models (LLMs) have become a pro...</td>\n",
              "      <td>[S. Purohit, U. S. Prashanth, E. Raff, et al. ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.14314.pdf, https:/...</td>\n",
              "      <td>Key architectures behind the development of la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Can you name some specific large language mode...</td>\n",
              "      <td>Some prominent examples of Large Language Mode...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Some specific large language models include GP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>What licensing models have been adopted for th...</td>\n",
              "      <td>The provided context does not explicitly menti...</td>\n",
              "      <td>[regulation. On the other hand, if large langu...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "      <td>Based on the provided context, it seems that l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>97</td>\n",
              "      <td>What can be inferred about the utilization of ...</td>\n",
              "      <td>Attention in neural networks is a powerful too...</td>\n",
              "      <td>[In a nutshell, attention in deep learning can...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Attention mechanisms in neural networks play a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>101</td>\n",
              "      <td>Can the use of attention mechanisms in deep le...</td>\n",
              "      <td>Yes, attention mechanisms have proven effectiv...</td>\n",
              "      <td>[With the help of the attention, the dependenc...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Yes, attention mechanisms in deep learning mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>102</td>\n",
              "      <td>What are the potential benefits of incorporati...</td>\n",
              "      <td>The integration of self-attention mechanisms i...</td>\n",
              "      <td>[Self-Attention GAN#\\nSelf-Attention GAN (SAGA...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Incorporating self-attention mechanisms into G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>103</td>\n",
              "      <td>How does the transformer model variate from tr...</td>\n",
              "      <td>The Transformer model differs from traditional...</td>\n",
              "      <td>[SNAIL#\\nThe transformer has no recurrent or c...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>The transformer model differs from traditional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>104</td>\n",
              "      <td>What implications does the concept of a Neural...</td>\n",
              "      <td>The Neural Turing Machine (NTM) architecture p...</td>\n",
              "      <td>[Neural Turing Machine (NTM, Graves, Wayne &amp; D...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>The concept of a Neural Turing Machine (NTM) e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52a2bb83-6988-491b-a139-7a8868f9afd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52a2bb83-6988-491b-a139-7a8868f9afd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52a2bb83-6988-491b-a139-7a8868f9afd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-388ada2a-c4eb-42a9-a2d8-48f6b9852c51\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-388ada2a-c4eb-42a9-a2d8-48f6b9852c51')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-388ada2a-c4eb-42a9-a2d8-48f6b9852c51 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c6a92ca2-2228-4b46-ab5f-5bb1c53f93e1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_v_research_cohere_df_full')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c6a92ca2-2228-4b46-ab5f-5bb1c53f93e1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_v_research_cohere_df_full');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_v_research_cohere_df_full",
              "summary": "{\n  \"name\": \"rag_output_v_research_cohere_df_full\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 104,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          7,\n          89,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"The provided context does not explicitly mention specific licensing models for source-available language models. However, it discusses a few strategies and considerations related to the deployment and distribution of language models. One approach mentioned is for an organization to own the end-to-end infrastructure of model deployment and provide access through an application programming interface (API). This allows the organization to implement safety protocols and control how the model is used. Another strategy is to release language models under a source-available license, as proposed by some researchers in the field. This approach aims to balance accessibility and innovation while mitigating potential risks associated with large language models. It is important to design licensing models that encourage responsible use and address ethical concerns, such as bias, privacy, and the potential for misuse. This could involve restricting access to those who agree to follow certain guidelines or implementing mechanisms to monitor and mitigate harmful applications of the technology.\",\n          \"The primary objective of prompt engineering in the context of autoregressive language models is to achieve alignment and steerability. It is a method of communicating with LLMs to guide their behavior and achieve desired outcomes, all without the need to adjust the model weights. Prompt engineering is an empirical science, and its effectiveness can vary significantly across different models. This variability necessitates extensive experimentation and the application of heuristics to identify the most suitable prompt engineering techniques for a specific model. The core idea is to design prompts that provide context and guidance to the LLM, influencing its responses or generated text to align with the desired objectives.\",\n          \"The development of generative language models by Anthropic was influenced by a range of factors, including academic research and a focus on societal impact. The company's language models are shaped by academic papers, such as those by Solaiman et al. and Strubell et al., which address the social impacts of language models and propose strategies for responsible development and deployment. Additionally, Anthropic's models are informed by techniques like fine-tuning from human preferences, as described by Christiano, Irving, and others. This approach allows models to learn directly from human feedback, aligning with Anthropic's goal of creating models that are beneficial and adaptable to society's needs. The company also considers processes like the one proposed by Solaiman and Dennison, which involves adapting language models to societal values using targeted datasets, further emphasizing the societal impact aspect.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.c) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Validation (V)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_V_MARKETING_MISTRAL_OUTPUT_FULL = rag_output_v_marketing_mistral_df_full\n",
        "RAG_V_MARKETING_COHERE_OUTPUT_FULL = rag_output_v_marketing_cohere_df_full\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Full Validation List Final Review (Marketing):\\n')\n",
        "# display(RAG_V_MARKETING_MISTRAL_OUTPUT_FULL)\n",
        "display(RAG_V_MARKETING_COHERE_OUTPUT_FULL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "_xiCvXLSFsxD",
        "outputId": "bbe80d9c-5af9-4730-bd0c-9f906344f075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Full Validation List Final Review (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    q_id                                           question  \\\n",
              "0      0  What purpose do large language models serve in...   \n",
              "1      1  How does a large language model learn from tex...   \n",
              "2      2  What are some key architectures behind the dev...   \n",
              "3      3  Can you name some specific large language mode...   \n",
              "4      7  What licensing models have been adopted for th...   \n",
              "..   ...                                                ...   \n",
              "70    97  What can be inferred about the utilization of ...   \n",
              "71   101  Can the use of attention mechanisms in deep le...   \n",
              "72   102  What are the potential benefits of incorporati...   \n",
              "73   103  How does the transformer model variate from tr...   \n",
              "74   104  What implications does the concept of a Neural...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   Large language models (LLMs) are an essential ...   \n",
              "1   Large language models (LLMs) learn by ingestin...   \n",
              "2   Large Language Models (LLMs) are developed usi...   \n",
              "3   Some prominent examples of Large Language Mode...   \n",
              "4   There are a few licensing models for source-av...   \n",
              "..                                                ...   \n",
              "70  Attention in neural networks is a powerful too...   \n",
              "71  Yes, attention mechanisms have proven effectiv...   \n",
              "72  The integration of self-attention mechanisms i...   \n",
              "73  The transformer model differs from traditional...   \n",
              "74  The Neural Turing Machine (NTM) model enhances...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [A large language model (LLM) is a language mo...   \n",
              "1   [Fig. 13. The amount of computation used for t...   \n",
              "2   [S. Purohit, U. S. Prashanth, E. Raff, et al. ...   \n",
              "3   [A large language model (LLM) is a language mo...   \n",
              "4   [regulation. On the other hand, if large langu...   \n",
              "..                                                ...   \n",
              "70  [In a nutshell, attention in deep learning can...   \n",
              "71  [With the help of the attention, the dependenc...   \n",
              "72  [Self-Attention GAN#\\nSelf-Attention GAN (SAGA...   \n",
              "73  [SNAIL#\\nThe transformer has no recurrent or c...   \n",
              "74  [Neural Turing Machine (NTM, Graves, Wayne & D...   \n",
              "\n",
              "                                              sources  \\\n",
              "0   [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "1   [https://lilianweng.github.io/posts/2020-10-29...   \n",
              "2   [https://arxiv.org/pdf/2305.14314.pdf, https:/...   \n",
              "3   [https://en.wikipedia.org/wiki/Large_language_...   \n",
              "4   [https://arxiv.org/pdf/2203.02155.pdf, https:/...   \n",
              "..                                                ...   \n",
              "70  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "71  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "72  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "73  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "74  [https://lilianweng.github.io/posts/2018-06-24...   \n",
              "\n",
              "                                         ground_truth  \n",
              "0   Large language models serve the purpose of imp...  \n",
              "1   A large language model learns from text during...  \n",
              "2   Key architectures behind the development of la...  \n",
              "3            Chinchilla by DeepMind, GPT-3 by OpenAI.  \n",
              "4   Answer: Some organizations choose open-sourcin...  \n",
              "..                                                ...  \n",
              "70  Attention in neural networks allows the model ...  \n",
              "71  Yes, attention mechanisms in deep learning mod...  \n",
              "72  Incorporating self-attention mechanisms into G...  \n",
              "73  The transformer model differs from traditional...  \n",
              "74  The concept of a Neural Turing Machine suggest...  \n",
              "\n",
              "[75 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b151a31a-5fde-46d3-a41a-9ab28b7334c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What purpose do large language models serve in...</td>\n",
              "      <td>Large language models (LLMs) are an essential ...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Large language models serve the purpose of imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>How does a large language model learn from tex...</td>\n",
              "      <td>Large language models (LLMs) learn by ingestin...</td>\n",
              "      <td>[Fig. 13. The amount of computation used for t...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "      <td>A large language model learns from text during...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What are some key architectures behind the dev...</td>\n",
              "      <td>Large Language Models (LLMs) are developed usi...</td>\n",
              "      <td>[S. Purohit, U. S. Prashanth, E. Raff, et al. ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.14314.pdf, https:/...</td>\n",
              "      <td>Key architectures behind the development of la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Can you name some specific large language mode...</td>\n",
              "      <td>Some prominent examples of Large Language Mode...</td>\n",
              "      <td>[A large language model (LLM) is a language mo...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Large_language_...</td>\n",
              "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>What licensing models have been adopted for th...</td>\n",
              "      <td>There are a few licensing models for source-av...</td>\n",
              "      <td>[regulation. On the other hand, if large langu...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>97</td>\n",
              "      <td>What can be inferred about the utilization of ...</td>\n",
              "      <td>Attention in neural networks is a powerful too...</td>\n",
              "      <td>[In a nutshell, attention in deep learning can...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Attention in neural networks allows the model ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>101</td>\n",
              "      <td>Can the use of attention mechanisms in deep le...</td>\n",
              "      <td>Yes, attention mechanisms have proven effectiv...</td>\n",
              "      <td>[With the help of the attention, the dependenc...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Yes, attention mechanisms in deep learning mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>102</td>\n",
              "      <td>What are the potential benefits of incorporati...</td>\n",
              "      <td>The integration of self-attention mechanisms i...</td>\n",
              "      <td>[Self-Attention GAN#\\nSelf-Attention GAN (SAGA...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>Incorporating self-attention mechanisms into G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>103</td>\n",
              "      <td>How does the transformer model variate from tr...</td>\n",
              "      <td>The transformer model differs from traditional...</td>\n",
              "      <td>[SNAIL#\\nThe transformer has no recurrent or c...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>The transformer model differs from traditional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>104</td>\n",
              "      <td>What implications does the concept of a Neural...</td>\n",
              "      <td>The Neural Turing Machine (NTM) model enhances...</td>\n",
              "      <td>[Neural Turing Machine (NTM, Graves, Wayne &amp; D...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "      <td>The concept of a Neural Turing Machine suggest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b151a31a-5fde-46d3-a41a-9ab28b7334c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b151a31a-5fde-46d3-a41a-9ab28b7334c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b151a31a-5fde-46d3-a41a-9ab28b7334c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1dcd4640-8881-441a-8e07-209bdf4cf173\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1dcd4640-8881-441a-8e07-209bdf4cf173')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1dcd4640-8881-441a-8e07-209bdf4cf173 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dcba576e-aa75-492a-afd4-a1cc3fca0dcc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_v_marketing_cohere_df_full')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dcba576e-aa75-492a-afd4-a1cc3fca0dcc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_v_marketing_cohere_df_full');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_v_marketing_cohere_df_full",
              "summary": "{\n  \"name\": \"rag_output_v_marketing_cohere_df_full\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 104,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          7,\n          89,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"There are a few licensing models for source-available language models, including open-source licenses like the Apache License used by Llama and proprietary licenses like the one used by Cohere.\",\n          \"The primary goal of prompt engineering in language models is to guide and align the model's behavior to achieve desired outcomes without modifying the underlying model weights. It is an experimental and heuristic-driven approach that involves effective communication with the LLM to steer its behavior in the desired direction.\",\n          \"Anthropic's development of generative language models was influenced by strategies for model release and the social impact of language models, as well as adapting these models to societal values through targeted datasets and fine-tuning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.d) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Research\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_RESEARCH_MISTRAL_OUTPUT_FULL = rag_output_t_research_mistral_df_full\n",
        "RAG_T_RESEARCH_COHERE_OUTPUT_FULL = rag_output_t_research_cohere_df_full\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Full Test List Final Review (Research):\\n')\n",
        "# display(RAG_T_RESEARCH_MISTRAL_OUTPUT_FULL)\n",
        "display(RAG_T_RESEARCH_COHERE_OUTPUT_FULL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TLfpiUje96uV",
        "outputId": "a5098bb1-5ce3-4106-f6a0-eef275976859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Full Test List Final Review (Research):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    q_id                                           question  \\\n",
              "0      4  When was the transformer architecture introduc...   \n",
              "1      5  How has the accessibility of powerful language...   \n",
              "2      6  What benchmarks or ratings are used to compare...   \n",
              "3     10  What are some of the primary applications for ...   \n",
              "4     14  How are language models typically evaluated an...   \n",
              "5     15  What datasets are available for evaluating lan...   \n",
              "6     21  What collaborations with other companies have ...   \n",
              "7     26  According to DeepMind, how should the number o...   \n",
              "8     29  How do the sizes of models in the Gopher famil...   \n",
              "9     31  What type of model architecture do the Gopher ...   \n",
              "10    32  Can you name the author who wrote the novels A...   \n",
              "11    37  What are the key advantages of InstructGPT mod...   \n",
              "12    40  What metrics are used to compare the performan...   \n",
              "13    42  What types of evaluation metrics are commonly ...   \n",
              "14    49  What factors contribute to the performance imp...   \n",
              "15    56  What are the benchmarks used to evaluate the p...   \n",
              "16    57  What methodologies have been evaluated for tra...   \n",
              "17    58  What methods have been discussed in the litera...   \n",
              "18    66  What are some of the evaluation metrics used f...   \n",
              "19    68  Consider a document related to research in nat...   \n",
              "20    71  What is the significance of using reflection t...   \n",
              "21    72  How does the inclusion of selected context as ...   \n",
              "22    77  What are the benefits of modeling human biases...   \n",
              "23    79  What are the modifications made to the traditi...   \n",
              "24    83  How does a model's ability to answer questions...   \n",
              "25    90  How can adding examples to a prompt affect the...   \n",
              "26    98  What are the main components of a Neural Turin...   \n",
              "27    99  How might a seq2seq model's limitations be add...   \n",
              "28   100  What differentiates hard attention from soft a...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The Transformer architecture was introduced in...   \n",
              "1   The developers of large language models like G...   \n",
              "2   Language models are typically evaluated using ...   \n",
              "3   Language models have a wide range of applicati...   \n",
              "4   Language models are typically evaluated by com...   \n",
              "5   There are several datasets available for evalu...   \n",
              "6   Anthropic's collaboration with Notion and Quor...   \n",
              "7   DeepMind's Chinchilla team recommends that the...   \n",
              "8   The Gopher family is a set of transformer mode...   \n",
              "9   The Gopher and Chinchilla families are both pa...   \n",
              "10  The author of the novels *A Farewell to Arms* ...   \n",
              "11  InstructGPT models offer several key advantage...   \n",
              "12  To evaluate model performance, the document de...   \n",
              "13  To assess the accuracy of responses in AI-driv...   \n",
              "14  The performance enhancement of retrieval-augme...   \n",
              "15  The document mentions that the performance of ...   \n",
              "16  Several methods for training language models t...   \n",
              "17  The literature suggests that learning from hum...   \n",
              "18  The study assesses text generation and evaluat...   \n",
              "19  Some recent topics and methods in the field of...   \n",
              "20  Reflection tokens are a key feature of the SEL...   \n",
              "21  In language model generation tasks, including ...   \n",
              "22  Human-Aware Loss Functions (HALOs) offer a uni...   \n",
              "23  The Kahneman-Tversky (KT) model is a seminal d...   \n",
              "24  A model's ability to answer questions is direc...   \n",
              "25  Adding examples to a prompt can significantly ...   \n",
              "26  The Neural Turing Machine (NTM) is a model tha...   \n",
              "27  The seq2seq model has some inherent limitation...   \n",
              "28  Soft attention mechanisms learn alignment weig...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [Full Architecture#\\nFinally here is the compl...   \n",
              "1   [regulation. On the other hand, if large langu...   \n",
              "2   [Evaluation of the quality of language models ...   \n",
              "3   [models are useful for a variety of tasks, inc...   \n",
              "4   [Evaluation of the quality of language models ...   \n",
              "5   [Holistic evaluation of language models.\\narXi...   \n",
              "6   [== Models ==\\n\\n\\n=== Claude ===\\nClaude was ...   \n",
              "7   [Fig. 13. The amount of computation used for t...   \n",
              "8   [The Gopher family contains six models of incr...   \n",
              "9   [== Architecture ==\\nBoth the Gopher family an...   \n",
              "10  [Document 1: his works are considered classics...   \n",
              "11  [In Figure 5, we\\nalso compare InstructGPT to ...   \n",
              "12  [tiple references. Hence we ensure that most o...   \n",
              "13  [Preprint.\\nChallenge; Clark et al. 2018). We ...   \n",
              "14  [Retrieval-Augmented Multimodal Language Model...   \n",
              "15  [4\\nDirect Preference Optimization\\nMotivated ...   \n",
              "16  [For additional details about the human study,...   \n",
              "17  [5.4\\nOpen questions\\nThis work is a ﬁrst step...   \n",
              "18  [text generation. To test RAG’s natural langua...   \n",
              "19  [In Proceedings of the 40th International Conf...   \n",
              "20  [Table 1: Four types of reflection tokens used...   \n",
              "21  [filtered by model Mctx for generation, denote...   \n",
              "22  [than gains, a property called loss aversion. ...   \n",
              "23  [Ziegler, D. M., Stiennon, N., Wu, J., Brown, ...   \n",
              "24  [A model is able to correctly memorize and res...   \n",
              "25  [arxiv.org/abs/1709.00103.\\nA\\nLARGE LANGUAGE ...   \n",
              "26  [Neural Turing Machine (NTM, Graves, Wayne & D...   \n",
              "27  [The seq2seq model was born in the field of la...   \n",
              "28  [Soft Attention: the alignment weights are lea...   \n",
              "\n",
              "                                              sources  \n",
              "0   [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "1   [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "2   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "3   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "4   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "5   [https://arxiv.org/pdf/2305.14314.pdf, https:/...  \n",
              "6   [https://en.wikipedia.org/wiki/Claude_(languag...  \n",
              "7   [https://lilianweng.github.io/posts/2020-10-29...  \n",
              "8   [https://en.wikipedia.org/wiki/Chinchilla_(lan...  \n",
              "9   [https://en.wikipedia.org/wiki/Chinchilla_(lan...  \n",
              "10  [https://arxiv.org/pdf/2005.11401.pdf, https:/...  \n",
              "11  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "12  [https://arxiv.org/pdf/2105.03011.pdf, https:/...  \n",
              "13  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "14  [https://arxiv.org/pdf/2211.12561.pdf, https:/...  \n",
              "15  [https://arxiv.org/pdf/2305.18290.pdf, https:/...  \n",
              "16  [https://arxiv.org/pdf/2305.18290.pdf, https:/...  \n",
              "17  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "18  [https://arxiv.org/pdf/2005.11401.pdf, https:/...  \n",
              "19  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "20  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "21  [https://arxiv.org/pdf/2311.08377.pdf, https:/...  \n",
              "22  [https://arxiv.org/pdf/2402.01306.pdf, https:/...  \n",
              "23  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "24  [https://lilianweng.github.io/posts/2020-10-29...  \n",
              "25  [https://arxiv.org/pdf/2106.09685.pdf, https:/...  \n",
              "26  [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "27  [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "28  [https://lilianweng.github.io/posts/2018-06-24...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9e87120-022b-4271-b048-9acd5053fa66\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>When was the transformer architecture introduc...</td>\n",
              "      <td>The Transformer architecture was introduced in...</td>\n",
              "      <td>[Full Architecture#\\nFinally here is the compl...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>How has the accessibility of powerful language...</td>\n",
              "      <td>The developers of large language models like G...</td>\n",
              "      <td>[regulation. On the other hand, if large langu...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>What benchmarks or ratings are used to compare...</td>\n",
              "      <td>Language models are typically evaluated using ...</td>\n",
              "      <td>[Evaluation of the quality of language models ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>What are some of the primary applications for ...</td>\n",
              "      <td>Language models have a wide range of applicati...</td>\n",
              "      <td>[models are useful for a variety of tasks, inc...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>How are language models typically evaluated an...</td>\n",
              "      <td>Language models are typically evaluated by com...</td>\n",
              "      <td>[Evaluation of the quality of language models ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15</td>\n",
              "      <td>What datasets are available for evaluating lan...</td>\n",
              "      <td>There are several datasets available for evalu...</td>\n",
              "      <td>[Holistic evaluation of language models.\\narXi...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.14314.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>What collaborations with other companies have ...</td>\n",
              "      <td>Anthropic's collaboration with Notion and Quor...</td>\n",
              "      <td>[== Models ==\\n\\n\\n=== Claude ===\\nClaude was ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Claude_(languag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26</td>\n",
              "      <td>According to DeepMind, how should the number o...</td>\n",
              "      <td>DeepMind's Chinchilla team recommends that the...</td>\n",
              "      <td>[Fig. 13. The amount of computation used for t...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>29</td>\n",
              "      <td>How do the sizes of models in the Gopher famil...</td>\n",
              "      <td>The Gopher family is a set of transformer mode...</td>\n",
              "      <td>[The Gopher family contains six models of incr...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Chinchilla_(lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>31</td>\n",
              "      <td>What type of model architecture do the Gopher ...</td>\n",
              "      <td>The Gopher and Chinchilla families are both pa...</td>\n",
              "      <td>[== Architecture ==\\nBoth the Gopher family an...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Chinchilla_(lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>Can you name the author who wrote the novels A...</td>\n",
              "      <td>The author of the novels *A Farewell to Arms* ...</td>\n",
              "      <td>[Document 1: his works are considered classics...</td>\n",
              "      <td>[https://arxiv.org/pdf/2005.11401.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>37</td>\n",
              "      <td>What are the key advantages of InstructGPT mod...</td>\n",
              "      <td>InstructGPT models offer several key advantage...</td>\n",
              "      <td>[In Figure 5, we\\nalso compare InstructGPT to ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>40</td>\n",
              "      <td>What metrics are used to compare the performan...</td>\n",
              "      <td>To evaluate model performance, the document de...</td>\n",
              "      <td>[tiple references. Hence we ensure that most o...</td>\n",
              "      <td>[https://arxiv.org/pdf/2105.03011.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>42</td>\n",
              "      <td>What types of evaluation metrics are commonly ...</td>\n",
              "      <td>To assess the accuracy of responses in AI-driv...</td>\n",
              "      <td>[Preprint.\\nChallenge; Clark et al. 2018). We ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>49</td>\n",
              "      <td>What factors contribute to the performance imp...</td>\n",
              "      <td>The performance enhancement of retrieval-augme...</td>\n",
              "      <td>[Retrieval-Augmented Multimodal Language Model...</td>\n",
              "      <td>[https://arxiv.org/pdf/2211.12561.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>56</td>\n",
              "      <td>What are the benchmarks used to evaluate the p...</td>\n",
              "      <td>The document mentions that the performance of ...</td>\n",
              "      <td>[4\\nDirect Preference Optimization\\nMotivated ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.18290.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>57</td>\n",
              "      <td>What methodologies have been evaluated for tra...</td>\n",
              "      <td>Several methods for training language models t...</td>\n",
              "      <td>[For additional details about the human study,...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.18290.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>58</td>\n",
              "      <td>What methods have been discussed in the litera...</td>\n",
              "      <td>The literature suggests that learning from hum...</td>\n",
              "      <td>[5.4\\nOpen questions\\nThis work is a ﬁrst step...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>66</td>\n",
              "      <td>What are some of the evaluation metrics used f...</td>\n",
              "      <td>The study assesses text generation and evaluat...</td>\n",
              "      <td>[text generation. To test RAG’s natural langua...</td>\n",
              "      <td>[https://arxiv.org/pdf/2005.11401.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>68</td>\n",
              "      <td>Consider a document related to research in nat...</td>\n",
              "      <td>Some recent topics and methods in the field of...</td>\n",
              "      <td>[In Proceedings of the 40th International Conf...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>71</td>\n",
              "      <td>What is the significance of using reflection t...</td>\n",
              "      <td>Reflection tokens are a key feature of the SEL...</td>\n",
              "      <td>[Table 1: Four types of reflection tokens used...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>72</td>\n",
              "      <td>How does the inclusion of selected context as ...</td>\n",
              "      <td>In language model generation tasks, including ...</td>\n",
              "      <td>[filtered by model Mctx for generation, denote...</td>\n",
              "      <td>[https://arxiv.org/pdf/2311.08377.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>77</td>\n",
              "      <td>What are the benefits of modeling human biases...</td>\n",
              "      <td>Human-Aware Loss Functions (HALOs) offer a uni...</td>\n",
              "      <td>[than gains, a property called loss aversion. ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2402.01306.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>79</td>\n",
              "      <td>What are the modifications made to the traditi...</td>\n",
              "      <td>The Kahneman-Tversky (KT) model is a seminal d...</td>\n",
              "      <td>[Ziegler, D. M., Stiennon, N., Wu, J., Brown, ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>90</td>\n",
              "      <td>How can adding examples to a prompt affect the...</td>\n",
              "      <td>Adding examples to a prompt can significantly ...</td>\n",
              "      <td>[arxiv.org/abs/1709.00103.\\nA\\nLARGE LANGUAGE ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2106.09685.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>98</td>\n",
              "      <td>What are the main components of a Neural Turin...</td>\n",
              "      <td>The Neural Turing Machine (NTM) is a model tha...</td>\n",
              "      <td>[Neural Turing Machine (NTM, Graves, Wayne &amp; D...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>99</td>\n",
              "      <td>How might a seq2seq model's limitations be add...</td>\n",
              "      <td>The seq2seq model has some inherent limitation...</td>\n",
              "      <td>[The seq2seq model was born in the field of la...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>100</td>\n",
              "      <td>What differentiates hard attention from soft a...</td>\n",
              "      <td>Soft attention mechanisms learn alignment weig...</td>\n",
              "      <td>[Soft Attention: the alignment weights are lea...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9e87120-022b-4271-b048-9acd5053fa66')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9e87120-022b-4271-b048-9acd5053fa66 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9e87120-022b-4271-b048-9acd5053fa66');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3ca41648-3af9-4ac8-af80-2beb9821e138\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ca41648-3af9-4ac8-af80-2beb9821e138')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3ca41648-3af9-4ac8-af80-2beb9821e138 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_37b9a9f3-03b0-4826-8954-0bf603b1f9bd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_research_cohere_df_full')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_37b9a9f3-03b0-4826-8954-0bf603b1f9bd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_research_cohere_df_full');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_research_cohere_df_full",
              "summary": "{\n  \"name\": \"rag_output_t_research_cohere_df_full\",\n  \"rows\": 29,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 4,\n        \"max\": 100,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          99,\n          57,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\",\n          \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\",\n          \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"The seq2seq model has some inherent limitations, including the tendency to generate generic responses and the challenge of handling long-range dependencies in the input sequence. To address these issues, researchers have proposed various modifications and extensions to the basic model. One approach is to incorporate attention mechanisms, which allow the decoder to focus on specific parts of the input sequence, improving the model's ability to capture long-range dependencies. Another strategy is to use reinforcement learning techniques to optimize non-differentiable metrics, such as the complexity and diversity of generated responses. Additionally, pre-training large-scale models with massive text corpora and then fine-tuning them on specific tasks has proven effective in enhancing the model's performance and generalization capabilities. Finally, combining the seq2seq model with external knowledge sources, such as knowledge graphs or databases, can improve its ability to generate factually accurate and contextually relevant responses, making it more suitable for knowledge-intensive NLP tasks.\",\n          \"Several methods for training language models to align with human preferences were assessed, including zero-shot and few-shot prompting, fine-tuning techniques, and the proposed DPO (Learning from Preferences) approach. Zero-shot prompting with GPT-J in the summarization task and 2-shot prompting with Pythia-2.8B in the dialogue task offered a simple evaluation of the models' ability to understand and generate human-preferred responses. Fine-tuning methods, such as SFT and Preferred-FT, further enhanced the models' performance by utilizing supervised learning with human feedback. The DPO method, on the other hand, introduces a novel training paradigm that learns from stated human preferences, offering a scalable and effective way to train language models that align with human values and desires. While each method has its merits, the DPO approach particularly stands out for its potential to train capable and aligned language models without requiring extensive fine-tuning or large amounts of labeled data.\",\n          \"To evaluate model performance, the document describes using a combination of automatic metrics and human evaluation metrics. For automatic evaluation, the models are trained on a dataset split into training, validation, and test sets, ensuring that each paper appears in only one of the sets. The models are then fine-tuned on a large number of examples (approximately 1 million), and the checkpoint with the highest reward model score on the validation set is chosen. This reward model score is one metric used to compare performance on the validation split. Additionally, the document mentions \\\"Knowledge F1\\\" and \\\"Rare F1\\\" as automatic metrics that are compared to the standard F1 metric. Human evaluation metrics are also employed to assess model performance, providing a nuanced understanding of the models' generation quality. The document recommends evaluating these human evaluation metrics in conjunction with the automatic metrics to gain a comprehensive understanding of model alignment and performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.e) Run evaluation of RAG system output\n",
        "\n",
        "#############################\n",
        "## RUN SPECS:\n",
        "## * Q_List = Test (T)\n",
        "## * Persona = Marketing\n",
        "## * LLM = Cohere\n",
        "#############################\n",
        "\n",
        "# Return RAG system output\n",
        "# RAG_T_MARKETING_MISTRAL_OUTPUT_FULL = rag_output_t_marketing_mistral_df_full\n",
        "RAG_T_MARKETING_COHERE_OUTPUT_FULL = rag_output_t_marketing_cohere_df_full\n",
        "\n",
        "# Show resulting non-scored output\n",
        "print('RAG System Non-Scored Output - Full Test List Final Review (Marketing):\\n')\n",
        "# display(RAG_T_MARKETING_MISTRAL_OUTPUT_FULL)\n",
        "display(RAG_T_MARKETING_COHERE_OUTPUT_FULL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aRCKYlUB-MvJ",
        "outputId": "c8d9ff83-f1d7-4c35-d115-8269042249f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System Non-Scored Output - Full Test List Final Review (Marketing):\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    q_id                                           question  \\\n",
              "0      4  When was the transformer architecture introduc...   \n",
              "1      5  How has the accessibility of powerful language...   \n",
              "2      6  What benchmarks or ratings are used to compare...   \n",
              "3     10  What are some of the primary applications for ...   \n",
              "4     14  How are language models typically evaluated an...   \n",
              "5     15  What datasets are available for evaluating lan...   \n",
              "6     21  What collaborations with other companies have ...   \n",
              "7     26  According to DeepMind, how should the number o...   \n",
              "8     29  How do the sizes of models in the Gopher famil...   \n",
              "9     31  What type of model architecture do the Gopher ...   \n",
              "10    32  Can you name the author who wrote the novels A...   \n",
              "11    37  What are the key advantages of InstructGPT mod...   \n",
              "12    40  What metrics are used to compare the performan...   \n",
              "13    42  What types of evaluation metrics are commonly ...   \n",
              "14    49  What factors contribute to the performance imp...   \n",
              "15    56  What are the benchmarks used to evaluate the p...   \n",
              "16    57  What methodologies have been evaluated for tra...   \n",
              "17    58  What methods have been discussed in the litera...   \n",
              "18    66  What are some of the evaluation metrics used f...   \n",
              "19    68  Consider a document related to research in nat...   \n",
              "20    71  What is the significance of using reflection t...   \n",
              "21    72  How does the inclusion of selected context as ...   \n",
              "22    77  What are the benefits of modeling human biases...   \n",
              "23    79  What are the modifications made to the traditi...   \n",
              "24    83  How does a model's ability to answer questions...   \n",
              "25    90  How can adding examples to a prompt affect the...   \n",
              "26    98  What are the main components of a Neural Turin...   \n",
              "27    99  How might a seq2seq model's limitations be add...   \n",
              "28   100  What differentiates hard attention from soft a...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The Transformer architecture was introduced in...   \n",
              "1   The developers of GPT-3 and GPT-4 have impleme...   \n",
              "2   Human-created samples and benchmarks are the p...   \n",
              "3   Language models have a wide range of applicati...   \n",
              "4   Language models are typically evaluated by com...   \n",
              "5   There are various datasets designed for evalua...   \n",
              "6   Anthropic's collaboration with Notion and Quor...   \n",
              "7   DeepMind's Chinchilla team suggests that the n...   \n",
              "8   The Gopher family offers a range of model size...   \n",
              "9   The Gopher and Chinchilla families are both pa...   \n",
              "10  The author of the novels *A Farewell to Arms* ...   \n",
              "11  InstructGPT models show significant improvemen...   \n",
              "12  The models' performance is evaluated using F1 ...   \n",
              "13  Accuracy, answer/span-level F1 score, and fait...   \n",
              "14  Performance improvements in retrieval-augmente...   \n",
              "15  The document mentions that the performance of ...   \n",
              "16  Several methods for training language models t...   \n",
              "17  The literature suggests that using human feedb...   \n",
              "18  For the MSMARCO NLG task, the model is evaluat...   \n",
              "19  Recent topics and methods in natural language ...   \n",
              "20  Reflection tokens are a key feature of the SEL...   \n",
              "21  Inclusion of selected context reduces computat...   \n",
              "22  Modeling human biases in HALOs offers a strate...   \n",
              "23  The Kahneman-Tversky model is modified by intr...   \n",
              "24  A model's ability to answer questions is direc...   \n",
              "25  Adding examples to prompts can significantly e...   \n",
              "26  The Neural Turing Machine (NTM) architecture c...   \n",
              "27  The limitations of the seq2seq model in natura...   \n",
              "28  Soft attention mechanisms use alignment weight...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [Full Architecture#\\nFinally here is the compl...   \n",
              "1   [regulation. On the other hand, if large langu...   \n",
              "2   [Evaluation of the quality of language models ...   \n",
              "3   [models are useful for a variety of tasks, inc...   \n",
              "4   [Evaluation of the quality of language models ...   \n",
              "5   [Holistic evaluation of language models.\\narXi...   \n",
              "6   [== Models ==\\n\\n\\n=== Claude ===\\nClaude was ...   \n",
              "7   [Fig. 13. The amount of computation used for t...   \n",
              "8   [The Gopher family contains six models of incr...   \n",
              "9   [== Architecture ==\\nBoth the Gopher family an...   \n",
              "10  [Document 1: his works are considered classics...   \n",
              "11  [In Figure 5, we\\nalso compare InstructGPT to ...   \n",
              "12  [tiple references. Hence we ensure that most o...   \n",
              "13  [Preprint.\\nChallenge; Clark et al. 2018). We ...   \n",
              "14  [Retrieval-Augmented Multimodal Language Model...   \n",
              "15  [4\\nDirect Preference Optimization\\nMotivated ...   \n",
              "16  [For additional details about the human study,...   \n",
              "17  [5.4\\nOpen questions\\nThis work is a ﬁrst step...   \n",
              "18  [text generation. To test RAG’s natural langua...   \n",
              "19  [In Proceedings of the 40th International Conf...   \n",
              "20  [Table 1: Four types of reflection tokens used...   \n",
              "21  [filtered by model Mctx for generation, denote...   \n",
              "22  [than gains, a property called loss aversion. ...   \n",
              "23  [Ziegler, D. M., Stiennon, N., Wu, J., Brown, ...   \n",
              "24  [A model is able to correctly memorize and res...   \n",
              "25  [arxiv.org/abs/1709.00103.\\nA\\nLARGE LANGUAGE ...   \n",
              "26  [Neural Turing Machine (NTM, Graves, Wayne & D...   \n",
              "27  [The seq2seq model was born in the field of la...   \n",
              "28  [Soft Attention: the alignment weights are lea...   \n",
              "\n",
              "                                              sources  \n",
              "0   [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "1   [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "2   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "3   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "4   [https://en.wikipedia.org/wiki/Language_model,...  \n",
              "5   [https://arxiv.org/pdf/2305.14314.pdf, https:/...  \n",
              "6   [https://en.wikipedia.org/wiki/Claude_(languag...  \n",
              "7   [https://lilianweng.github.io/posts/2020-10-29...  \n",
              "8   [https://en.wikipedia.org/wiki/Chinchilla_(lan...  \n",
              "9   [https://en.wikipedia.org/wiki/Chinchilla_(lan...  \n",
              "10  [https://arxiv.org/pdf/2005.11401.pdf, https:/...  \n",
              "11  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "12  [https://arxiv.org/pdf/2105.03011.pdf, https:/...  \n",
              "13  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "14  [https://arxiv.org/pdf/2211.12561.pdf, https:/...  \n",
              "15  [https://arxiv.org/pdf/2305.18290.pdf, https:/...  \n",
              "16  [https://arxiv.org/pdf/2305.18290.pdf, https:/...  \n",
              "17  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "18  [https://arxiv.org/pdf/2005.11401.pdf, https:/...  \n",
              "19  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "20  [https://arxiv.org/pdf/2310.11511.pdf, https:/...  \n",
              "21  [https://arxiv.org/pdf/2311.08377.pdf, https:/...  \n",
              "22  [https://arxiv.org/pdf/2402.01306.pdf, https:/...  \n",
              "23  [https://arxiv.org/pdf/2203.02155.pdf, https:/...  \n",
              "24  [https://lilianweng.github.io/posts/2020-10-29...  \n",
              "25  [https://arxiv.org/pdf/2106.09685.pdf, https:/...  \n",
              "26  [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "27  [https://lilianweng.github.io/posts/2018-06-24...  \n",
              "28  [https://lilianweng.github.io/posts/2018-06-24...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a325596-e466-4c95-b4ca-b80e494f0f57\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>sources</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>When was the transformer architecture introduc...</td>\n",
              "      <td>The Transformer architecture was introduced in...</td>\n",
              "      <td>[Full Architecture#\\nFinally here is the compl...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>How has the accessibility of powerful language...</td>\n",
              "      <td>The developers of GPT-3 and GPT-4 have impleme...</td>\n",
              "      <td>[regulation. On the other hand, if large langu...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>What benchmarks or ratings are used to compare...</td>\n",
              "      <td>Human-created samples and benchmarks are the p...</td>\n",
              "      <td>[Evaluation of the quality of language models ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>What are some of the primary applications for ...</td>\n",
              "      <td>Language models have a wide range of applicati...</td>\n",
              "      <td>[models are useful for a variety of tasks, inc...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>How are language models typically evaluated an...</td>\n",
              "      <td>Language models are typically evaluated by com...</td>\n",
              "      <td>[Evaluation of the quality of language models ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Language_model,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15</td>\n",
              "      <td>What datasets are available for evaluating lan...</td>\n",
              "      <td>There are various datasets designed for evalua...</td>\n",
              "      <td>[Holistic evaluation of language models.\\narXi...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.14314.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>21</td>\n",
              "      <td>What collaborations with other companies have ...</td>\n",
              "      <td>Anthropic's collaboration with Notion and Quor...</td>\n",
              "      <td>[== Models ==\\n\\n\\n=== Claude ===\\nClaude was ...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Claude_(languag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26</td>\n",
              "      <td>According to DeepMind, how should the number o...</td>\n",
              "      <td>DeepMind's Chinchilla team suggests that the n...</td>\n",
              "      <td>[Fig. 13. The amount of computation used for t...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>29</td>\n",
              "      <td>How do the sizes of models in the Gopher famil...</td>\n",
              "      <td>The Gopher family offers a range of model size...</td>\n",
              "      <td>[The Gopher family contains six models of incr...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Chinchilla_(lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>31</td>\n",
              "      <td>What type of model architecture do the Gopher ...</td>\n",
              "      <td>The Gopher and Chinchilla families are both pa...</td>\n",
              "      <td>[== Architecture ==\\nBoth the Gopher family an...</td>\n",
              "      <td>[https://en.wikipedia.org/wiki/Chinchilla_(lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>Can you name the author who wrote the novels A...</td>\n",
              "      <td>The author of the novels *A Farewell to Arms* ...</td>\n",
              "      <td>[Document 1: his works are considered classics...</td>\n",
              "      <td>[https://arxiv.org/pdf/2005.11401.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>37</td>\n",
              "      <td>What are the key advantages of InstructGPT mod...</td>\n",
              "      <td>InstructGPT models show significant improvemen...</td>\n",
              "      <td>[In Figure 5, we\\nalso compare InstructGPT to ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>40</td>\n",
              "      <td>What metrics are used to compare the performan...</td>\n",
              "      <td>The models' performance is evaluated using F1 ...</td>\n",
              "      <td>[tiple references. Hence we ensure that most o...</td>\n",
              "      <td>[https://arxiv.org/pdf/2105.03011.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>42</td>\n",
              "      <td>What types of evaluation metrics are commonly ...</td>\n",
              "      <td>Accuracy, answer/span-level F1 score, and fait...</td>\n",
              "      <td>[Preprint.\\nChallenge; Clark et al. 2018). We ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>49</td>\n",
              "      <td>What factors contribute to the performance imp...</td>\n",
              "      <td>Performance improvements in retrieval-augmente...</td>\n",
              "      <td>[Retrieval-Augmented Multimodal Language Model...</td>\n",
              "      <td>[https://arxiv.org/pdf/2211.12561.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>56</td>\n",
              "      <td>What are the benchmarks used to evaluate the p...</td>\n",
              "      <td>The document mentions that the performance of ...</td>\n",
              "      <td>[4\\nDirect Preference Optimization\\nMotivated ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.18290.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>57</td>\n",
              "      <td>What methodologies have been evaluated for tra...</td>\n",
              "      <td>Several methods for training language models t...</td>\n",
              "      <td>[For additional details about the human study,...</td>\n",
              "      <td>[https://arxiv.org/pdf/2305.18290.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>58</td>\n",
              "      <td>What methods have been discussed in the litera...</td>\n",
              "      <td>The literature suggests that using human feedb...</td>\n",
              "      <td>[5.4\\nOpen questions\\nThis work is a ﬁrst step...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>66</td>\n",
              "      <td>What are some of the evaluation metrics used f...</td>\n",
              "      <td>For the MSMARCO NLG task, the model is evaluat...</td>\n",
              "      <td>[text generation. To test RAG’s natural langua...</td>\n",
              "      <td>[https://arxiv.org/pdf/2005.11401.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>68</td>\n",
              "      <td>Consider a document related to research in nat...</td>\n",
              "      <td>Recent topics and methods in natural language ...</td>\n",
              "      <td>[In Proceedings of the 40th International Conf...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>71</td>\n",
              "      <td>What is the significance of using reflection t...</td>\n",
              "      <td>Reflection tokens are a key feature of the SEL...</td>\n",
              "      <td>[Table 1: Four types of reflection tokens used...</td>\n",
              "      <td>[https://arxiv.org/pdf/2310.11511.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>72</td>\n",
              "      <td>How does the inclusion of selected context as ...</td>\n",
              "      <td>Inclusion of selected context reduces computat...</td>\n",
              "      <td>[filtered by model Mctx for generation, denote...</td>\n",
              "      <td>[https://arxiv.org/pdf/2311.08377.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>77</td>\n",
              "      <td>What are the benefits of modeling human biases...</td>\n",
              "      <td>Modeling human biases in HALOs offers a strate...</td>\n",
              "      <td>[than gains, a property called loss aversion. ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2402.01306.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>79</td>\n",
              "      <td>What are the modifications made to the traditi...</td>\n",
              "      <td>The Kahneman-Tversky model is modified by intr...</td>\n",
              "      <td>[Ziegler, D. M., Stiennon, N., Wu, J., Brown, ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2203.02155.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>83</td>\n",
              "      <td>How does a model's ability to answer questions...</td>\n",
              "      <td>A model's ability to answer questions is direc...</td>\n",
              "      <td>[A model is able to correctly memorize and res...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2020-10-29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>90</td>\n",
              "      <td>How can adding examples to a prompt affect the...</td>\n",
              "      <td>Adding examples to prompts can significantly e...</td>\n",
              "      <td>[arxiv.org/abs/1709.00103.\\nA\\nLARGE LANGUAGE ...</td>\n",
              "      <td>[https://arxiv.org/pdf/2106.09685.pdf, https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>98</td>\n",
              "      <td>What are the main components of a Neural Turin...</td>\n",
              "      <td>The Neural Turing Machine (NTM) architecture c...</td>\n",
              "      <td>[Neural Turing Machine (NTM, Graves, Wayne &amp; D...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>99</td>\n",
              "      <td>How might a seq2seq model's limitations be add...</td>\n",
              "      <td>The limitations of the seq2seq model in natura...</td>\n",
              "      <td>[The seq2seq model was born in the field of la...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>100</td>\n",
              "      <td>What differentiates hard attention from soft a...</td>\n",
              "      <td>Soft attention mechanisms use alignment weight...</td>\n",
              "      <td>[Soft Attention: the alignment weights are lea...</td>\n",
              "      <td>[https://lilianweng.github.io/posts/2018-06-24...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a325596-e466-4c95-b4ca-b80e494f0f57')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5a325596-e466-4c95-b4ca-b80e494f0f57 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5a325596-e466-4c95-b4ca-b80e494f0f57');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4ce1b172-9a00-4fa7-8977-f91a95761ed2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ce1b172-9a00-4fa7-8977-f91a95761ed2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4ce1b172-9a00-4fa7-8977-f91a95761ed2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_bc6f0e02-4244-49e9-8a11-31f5ae0db9a2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rag_output_t_marketing_cohere_df_full')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bc6f0e02-4244-49e9-8a11-31f5ae0db9a2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rag_output_t_marketing_cohere_df_full');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rag_output_t_marketing_cohere_df_full",
              "summary": "{\n  \"name\": \"rag_output_t_marketing_cohere_df_full\",\n  \"rows\": 29,\n  \"fields\": [\n    {\n      \"column\": \"q_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 4,\n        \"max\": 100,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          99,\n          57,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\",\n          \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\",\n          \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"The limitations of the seq2seq model in natural language processing tasks can be addressed by utilizing an encoder-decoder architecture with attention mechanisms. This enhancement enables the model to focus on relevant parts of the input sequence dynamically, improving performance in tasks such as machine translation, dialogue generation, and question-answering.\",\n          \"Several methods for training language models to align with human preferences were assessed, including zero-shot and 2-shot prompting with different language models, as well as the SFT and Preferred-FT models. The results indicate that the DPO training paradigm outperforms these existing approaches, offering a simple yet effective way to train language models that better adhere to human preferences.\",\n          \"The models' performance is evaluated using F1 metrics, specifically Knowledge F1 and Rare F1, which are recommended over standard F1. These metrics are calculated on the training and validation splits, with a focus on questions with multiple references.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5.4.f) Export RAG system output data for full validation and test sets\n",
        "\n",
        "# PART 1: Set RAG system outputs to export objects and convert RAG mean score dictionaries to dataframes\n",
        "# RAG_V_RESEARCH_MISTRAL_OUTPUT_EXPORT_FULL = RAG_V_RESEARCH_MISTRAL_OUTPUT_FULL\n",
        "# RAG_V_MARKETING_MISTRAL_OUTPUT_EXPORT_FULL = RAG_V_MARKETING_MISTRAL_OUTPUT_FULL\n",
        "# RAG_T_RESEARCH_MISTRAL_OUTPUT_EXPORT_FULL = RAG_T_RESEARCH_MISTRAL_OUTPUT_FULL\n",
        "# RAG_T_MARKETING_MISTRAL_OUTPUT_EXPORT_FULL = RAG_T_MARKETING_MISTRAL_OUTPUT_FULL\n",
        "RAG_V_RESEARCH_COHERE_OUTPUT_EXPORT_FULL = RAG_V_RESEARCH_COHERE_OUTPUT_FULL\n",
        "RAG_V_MARKETING_COHERE_OUTPUT_EXPORT_FULL = RAG_V_MARKETING_COHERE_OUTPUT_FULL\n",
        "RAG_T_RESEARCH_COHERE_OUTPUT_EXPORT_FULL = RAG_T_RESEARCH_COHERE_OUTPUT_FULL\n",
        "RAG_T_MARKETING_COHERE_OUTPUT_EXPORT_FULL = RAG_T_MARKETING_COHERE_OUTPUT_FULL\n",
        "\n",
        "# PART 2: Write each dataframe to a different worksheet in an Excel (.xlsx) file\n",
        "# Note - The 'Specs' codes below are defined as follows:\n",
        "# * Questions (V = Validation, T = Test)\n",
        "# * LLM (M = Mistral, C = Cohere)\n",
        "# * Embedding Models (1 = all-MiniLM-L6-v2, 2 = multi-qa-mpnet-base-dot-v1, 3 = GIST-Embedding-v0)\n",
        "# * Chunk Size (Number - e.g. 160)\n",
        "# * Chunk Overlap (Number - e.g., 40)\n",
        "# * Comments (Space for additional notes - e.g., LLM temperature = 0.6, Prompt Changes, etc.)\n",
        "\n",
        "with pd.ExcelWriter(\n",
        "    'RAG_V_nonscored_full_list.xlsx',    # Specs: V | C | 3 | 400 | 20 | Final iteration\n",
        "    ) as writer:\n",
        "    # RAG_V_RESEARCH_MISTRAL_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='V_R_Output_Full', index=False)\n",
        "    # RAG_V_MARKETING_MISTRAL_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='V_M_Output_Full', index=False)\n",
        "    RAG_V_RESEARCH_COHERE_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='V_R_Output_Full', index=False)\n",
        "    RAG_V_MARKETING_COHERE_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='V_M_Output_Full', index=False)\n",
        "print('DataFrames written to Excel file successfully.')\n",
        "with pd.ExcelWriter(\n",
        "    'RAG_T_nonscored_full_list.xlsx',    # Specs: T | C | 3 | 400 | 20 | Final iteration\n",
        "    ) as writer:\n",
        "    # RAG_T_RESEARCH_MISTRAL_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='T_R_Output_Full', index=False)\n",
        "    # RAG_T_MARKETING_MISTRAL_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='T_M_Output_Full', index=False)\n",
        "    RAG_T_RESEARCH_COHERE_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='T_R_Output_Full', index=False)\n",
        "    RAG_T_MARKETING_COHERE_OUTPUT_EXPORT_FULL.to_excel(writer, sheet_name='T_M_Output_Full', index=False)\n",
        "print('DataFrames written to Excel file successfully.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWHBtIDr-jQI",
        "outputId": "3b76ea27-6b52-43ba-a4ba-048103ef6c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames written to Excel file successfully.\n",
            "DataFrames written to Excel file successfully.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d94b8911b2e4f3b891db37156573473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a54d472f2f948a681dd7b32c9362957",
              "IPY_MODEL_e7a1897292d047c2aa155785401b609f",
              "IPY_MODEL_9f29ad4ef9074c57a683df349460b722"
            ],
            "layout": "IPY_MODEL_98b300b5fbb746cca35009e8f45dec32"
          }
        },
        "4a54d472f2f948a681dd7b32c9362957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19de87c0fbe7429f867ae8e0341d774c",
            "placeholder": "​",
            "style": "IPY_MODEL_65fa7772833442c3b64063508eba0999",
            "value": "Downloading builder script: 100%"
          }
        },
        "e7a1897292d047c2aa155785401b609f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f42dbc6e8af94927849690314986447b",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abe69d8bef0c4da884b9b194dd719d3f",
            "value": 6270
          }
        },
        "9f29ad4ef9074c57a683df349460b722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14329d4468a64f90bc095456108853d8",
            "placeholder": "​",
            "style": "IPY_MODEL_2ef5414753f048d8886f766ffdf6e375",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 567kB/s]"
          }
        },
        "98b300b5fbb746cca35009e8f45dec32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19de87c0fbe7429f867ae8e0341d774c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65fa7772833442c3b64063508eba0999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f42dbc6e8af94927849690314986447b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe69d8bef0c4da884b9b194dd719d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14329d4468a64f90bc095456108853d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ef5414753f048d8886f766ffdf6e375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37dd310d794f4a76927bcb1bf2cc910a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de4ebe4452cd495082c3bae5feef2991",
              "IPY_MODEL_bbd5e59b13c04a3f9b27e3ba5ea08829",
              "IPY_MODEL_8f05dea707ae42a999ea7bdcdd006a60"
            ],
            "layout": "IPY_MODEL_75477af511f94bf59069ab6d956c07f3"
          }
        },
        "de4ebe4452cd495082c3bae5feef2991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80546491a0e47979edb7157486c01c0",
            "placeholder": "​",
            "style": "IPY_MODEL_f9074995b028487fb338f25d92cca0df",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bbd5e59b13c04a3f9b27e3ba5ea08829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52164085387548199943e0c3dc8bd9be",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95a769b339c44921b642176442e82aa8",
            "value": 48
          }
        },
        "8f05dea707ae42a999ea7bdcdd006a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9cf6ea82c849018c989e7f765a8791",
            "placeholder": "​",
            "style": "IPY_MODEL_41eb39f02b0d41afbd74f7177b6fe5be",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.58kB/s]"
          }
        },
        "75477af511f94bf59069ab6d956c07f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80546491a0e47979edb7157486c01c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9074995b028487fb338f25d92cca0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52164085387548199943e0c3dc8bd9be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a769b339c44921b642176442e82aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a9cf6ea82c849018c989e7f765a8791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41eb39f02b0d41afbd74f7177b6fe5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35dfe6a5bdca4df2a90f47fbb0f7b8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67429625f914562a9542dd841063fa6",
              "IPY_MODEL_1cb607c160224975a525f71c4effd4cb",
              "IPY_MODEL_dd736a5fc15c48799f1f4e12d979b5af"
            ],
            "layout": "IPY_MODEL_d86b6b6158114844a512afcd20e05684"
          }
        },
        "a67429625f914562a9542dd841063fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88fc71e8cf484dceb140d497402e8476",
            "placeholder": "​",
            "style": "IPY_MODEL_34a0afa569a041bc936792c9f67ab187",
            "value": "config.json: 100%"
          }
        },
        "1cb607c160224975a525f71c4effd4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a286a451acba4548be2b8ec51c13f17e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdd0b9ec6e29492082bcb39b3114fe65",
            "value": 570
          }
        },
        "dd736a5fc15c48799f1f4e12d979b5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2658d1bd134fdb9ac8245cd4a29853",
            "placeholder": "​",
            "style": "IPY_MODEL_a218bb8b77b945a8bef60c83d65d85a4",
            "value": " 570/570 [00:00&lt;00:00, 46.6kB/s]"
          }
        },
        "d86b6b6158114844a512afcd20e05684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88fc71e8cf484dceb140d497402e8476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a0afa569a041bc936792c9f67ab187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a286a451acba4548be2b8ec51c13f17e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd0b9ec6e29492082bcb39b3114fe65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff2658d1bd134fdb9ac8245cd4a29853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a218bb8b77b945a8bef60c83d65d85a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8627c3f5e0b4c9287913fa88d284f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43d4264c79444d528902a36e49f756b5",
              "IPY_MODEL_b577383603df4e36ad6dfca39765766c",
              "IPY_MODEL_dd2be183ad8c475b9db7a3e9d1de7196"
            ],
            "layout": "IPY_MODEL_74ac000804874e92ae36e54e2b0d3eff"
          }
        },
        "43d4264c79444d528902a36e49f756b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6687d793a7fc4bcb893dce81c60b82d7",
            "placeholder": "​",
            "style": "IPY_MODEL_b0692f68e9c7411889d6e4b6e6b3d370",
            "value": "vocab.txt: 100%"
          }
        },
        "b577383603df4e36ad6dfca39765766c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fc39ab5a42b4fbd94ef96956507f000",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d8ef0b7c26f4c77a133da93dd5846b0",
            "value": 231508
          }
        },
        "dd2be183ad8c475b9db7a3e9d1de7196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdaf422342f94d3cb3c15be8b8f2a9e8",
            "placeholder": "​",
            "style": "IPY_MODEL_2c67f454438a4c52894324684d833601",
            "value": " 232k/232k [00:00&lt;00:00, 1.97MB/s]"
          }
        },
        "74ac000804874e92ae36e54e2b0d3eff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6687d793a7fc4bcb893dce81c60b82d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0692f68e9c7411889d6e4b6e6b3d370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fc39ab5a42b4fbd94ef96956507f000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8ef0b7c26f4c77a133da93dd5846b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdaf422342f94d3cb3c15be8b8f2a9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c67f454438a4c52894324684d833601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5786f812fb74f3ea70fb2c409e9136f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95c88f1c67f94b97be4d14d3aef1c9c6",
              "IPY_MODEL_a1bdc5b5e6584073bd8f9add20a18ff5",
              "IPY_MODEL_731aefe487674ceda8211858c1d86a6f"
            ],
            "layout": "IPY_MODEL_7c056b4998b945e4a2dddf027876bc1f"
          }
        },
        "95c88f1c67f94b97be4d14d3aef1c9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b777e3e786a4401eaa49b94e2a2b598b",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6983554fa8402d91ea5cb550884141",
            "value": "tokenizer.json: 100%"
          }
        },
        "a1bdc5b5e6584073bd8f9add20a18ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b20cc45042499aa81390d9d53aab88",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e971edc49194241954025fcb50fe2b8",
            "value": 466062
          }
        },
        "731aefe487674ceda8211858c1d86a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31faa9c980443e3a91f50b2e4f7dea8",
            "placeholder": "​",
            "style": "IPY_MODEL_f747fc5ad53a44aebe3d21c3f6cbcb22",
            "value": " 466k/466k [00:00&lt;00:00, 7.60MB/s]"
          }
        },
        "7c056b4998b945e4a2dddf027876bc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b777e3e786a4401eaa49b94e2a2b598b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6983554fa8402d91ea5cb550884141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b20cc45042499aa81390d9d53aab88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e971edc49194241954025fcb50fe2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b31faa9c980443e3a91f50b2e4f7dea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f747fc5ad53a44aebe3d21c3f6cbcb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "168ff720a5214612849f93b9dd08cb71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46c47397366948de960c6c0730a4fa80",
              "IPY_MODEL_a537e75bf1a44353857da086cd49ab5d",
              "IPY_MODEL_d396f8bedd9e4c93ae8616c34e43d013"
            ],
            "layout": "IPY_MODEL_b81e3d4c9ac04261b6f41879eefdf648"
          }
        },
        "46c47397366948de960c6c0730a4fa80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae857bd9e4fa4502960be177e025b514",
            "placeholder": "​",
            "style": "IPY_MODEL_fda0fcc6d86e4fef8ba354184926a0f2",
            "value": "model.safetensors: 100%"
          }
        },
        "a537e75bf1a44353857da086cd49ab5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e45ec7c251245158984d317f7c46098",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e419d38fd9b84d0fb822822328d5249f",
            "value": 440449768
          }
        },
        "d396f8bedd9e4c93ae8616c34e43d013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac20e673a8640e484df598c0a5b86da",
            "placeholder": "​",
            "style": "IPY_MODEL_ae48b99ab2b040a69063aeb200235473",
            "value": " 440M/440M [00:01&lt;00:00, 334MB/s]"
          }
        },
        "b81e3d4c9ac04261b6f41879eefdf648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae857bd9e4fa4502960be177e025b514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda0fcc6d86e4fef8ba354184926a0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e45ec7c251245158984d317f7c46098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e419d38fd9b84d0fb822822328d5249f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ac20e673a8640e484df598c0a5b86da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae48b99ab2b040a69063aeb200235473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74337a6316864621a500b9ab26d6d8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af46ace5a79a480aa39a934e2603b24e",
              "IPY_MODEL_23b812957476458ca2b6f8f9106a5dcd",
              "IPY_MODEL_95db2722ff71474f8503fe955144b7a0"
            ],
            "layout": "IPY_MODEL_0d9840872a5045e9bebb86cd6c1cd1ff"
          }
        },
        "af46ace5a79a480aa39a934e2603b24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6073362a88a464a97a04fd3c4d6bb82",
            "placeholder": "​",
            "style": "IPY_MODEL_8ad119222faf430798ce5eda0828d05f",
            "value": "Evaluating: 100%"
          }
        },
        "23b812957476458ca2b6f8f9106a5dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f756c939f28942f787fe917471693aee",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a58dd7f79a6b4a54a9842065d963040d",
            "value": 10
          }
        },
        "95db2722ff71474f8503fe955144b7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a99140a00a2047558cff420176fcb41f",
            "placeholder": "​",
            "style": "IPY_MODEL_4bcf47607b38493d8c0eff4760cf9644",
            "value": " 10/10 [00:09&lt;00:00,  1.02it/s]"
          }
        },
        "0d9840872a5045e9bebb86cd6c1cd1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6073362a88a464a97a04fd3c4d6bb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad119222faf430798ce5eda0828d05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f756c939f28942f787fe917471693aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58dd7f79a6b4a54a9842065d963040d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a99140a00a2047558cff420176fcb41f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bcf47607b38493d8c0eff4760cf9644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f8c3479c54f43218a60f2775fdf2fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8140201ee714214be6f1778793c42e9",
              "IPY_MODEL_143005359db245399d95dcab5053698c",
              "IPY_MODEL_a26c9f23fd9b42c4a7881543ae8cd9e4"
            ],
            "layout": "IPY_MODEL_00ee819db4a84a9eaeea71439b04f892"
          }
        },
        "b8140201ee714214be6f1778793c42e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd529d3deb614f56a3b6a2043b4b4299",
            "placeholder": "​",
            "style": "IPY_MODEL_5e15639388f744d2a9d2cd066dd50e22",
            "value": "Evaluating: 100%"
          }
        },
        "143005359db245399d95dcab5053698c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52a82c9cef9a497693ae57e3679b9a0c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72231c8bec47467b939c1e744b496ea7",
            "value": 10
          }
        },
        "a26c9f23fd9b42c4a7881543ae8cd9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_603e30b730744102adf29fc4acaeb3a2",
            "placeholder": "​",
            "style": "IPY_MODEL_989add017ba8483b918aa0e9c63c0ba7",
            "value": " 10/10 [00:05&lt;00:00,  1.74it/s]"
          }
        },
        "00ee819db4a84a9eaeea71439b04f892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd529d3deb614f56a3b6a2043b4b4299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e15639388f744d2a9d2cd066dd50e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52a82c9cef9a497693ae57e3679b9a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72231c8bec47467b939c1e744b496ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "603e30b730744102adf29fc4acaeb3a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989add017ba8483b918aa0e9c63c0ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c15db3fd3e4910a5c068857405890f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47ec7878b44749238f1f548f6a9465d4",
              "IPY_MODEL_3ad92e9328e54636829d778674cff266",
              "IPY_MODEL_a5d42065b8314841ba7550afba846fa8"
            ],
            "layout": "IPY_MODEL_83389b9430f845e2b330a4b5dc7decc9"
          }
        },
        "47ec7878b44749238f1f548f6a9465d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3790f799ce6941a780d465fac4830d80",
            "placeholder": "​",
            "style": "IPY_MODEL_05b03ad91ea54455b66d7d66a58aa1a3",
            "value": "Evaluating: 100%"
          }
        },
        "3ad92e9328e54636829d778674cff266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4303d4145f4b549275a1914ea8e299",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bd9dfcb42264397aff233abf9ccc543",
            "value": 10
          }
        },
        "a5d42065b8314841ba7550afba846fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b28021973a354e308dcab500c5c83e47",
            "placeholder": "​",
            "style": "IPY_MODEL_8d54feb2b1274c14b62b0888c5ec1d6b",
            "value": " 10/10 [00:07&lt;00:00,  1.17it/s]"
          }
        },
        "83389b9430f845e2b330a4b5dc7decc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3790f799ce6941a780d465fac4830d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b03ad91ea54455b66d7d66a58aa1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea4303d4145f4b549275a1914ea8e299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd9dfcb42264397aff233abf9ccc543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b28021973a354e308dcab500c5c83e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d54feb2b1274c14b62b0888c5ec1d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "311e05851b4749bf9d8d85dcc6395677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52ebc5c991f24794ad68e564b4c5be33",
              "IPY_MODEL_82af016063464f5bbfa961463766faa4",
              "IPY_MODEL_afda63a9ede0426a97c1a593c649757e"
            ],
            "layout": "IPY_MODEL_ea2d6a4f876b439dbd6d2968c5f952bd"
          }
        },
        "52ebc5c991f24794ad68e564b4c5be33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b9835349994459a9e5dac9eef9a241",
            "placeholder": "​",
            "style": "IPY_MODEL_4d51196b306c4c709c7c6ba20690cbc1",
            "value": "Evaluating: 100%"
          }
        },
        "82af016063464f5bbfa961463766faa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2443ca718942098b504b120c80ebed",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79ed2a2489da47b3ab485fa6bd4c7f41",
            "value": 10
          }
        },
        "afda63a9ede0426a97c1a593c649757e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a8e577c39e43f0bc26bbfed984df31",
            "placeholder": "​",
            "style": "IPY_MODEL_074a3352e0c1427fbf412a7e6d406183",
            "value": " 10/10 [00:05&lt;00:00,  1.75it/s]"
          }
        },
        "ea2d6a4f876b439dbd6d2968c5f952bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b9835349994459a9e5dac9eef9a241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d51196b306c4c709c7c6ba20690cbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2443ca718942098b504b120c80ebed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ed2a2489da47b3ab485fa6bd4c7f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43a8e577c39e43f0bc26bbfed984df31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "074a3352e0c1427fbf412a7e6d406183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}